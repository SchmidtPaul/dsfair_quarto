{
  "hash": "f7bc08cd81ce0fb0fd2cf61e2e3ef700",
  "result": {
    "markdown": "---\ntitle: \"Correlation & Regression\"\nabstract: \"Correlation and simple linear regression (with and without intercept).\"\n---\n\n\n\n\n\nThis chapter is trying to give you a feeling for what correlation and (simple linear) regression is. I am aware that the example data doesn't have anything to do with agriculture or related fields, but I decided to keep it because it allows for an intuitive conclusion at the end.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# (install &) load packages\npacman::p_load(\n  broom,\n  conflicted,\n  modelbased,\n  tidyverse)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)\n```\n:::\n\n\n\n# Data\n\nThis is data I made up: Peter and Max went out multiple evenings and at the end of every evening wrote down how many drinks they had and what the alcohol content in their blood was.\n\n## Import\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# data is available online:\npath <- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/DrinksPeterMax.csv\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- read_csv(path) # use path from above\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 20 x 3\n   Person drinks blood_alc\n   <chr>   <dbl>     <dbl>\n 1 Max         1       0.2\n 2 Max         2       0.3\n 3 Max         3       0.5\n 4 Max         3       0.6\n 5 Max         4       0.6\n 6 Max         4       0.5\n 7 Max         4       0.7\n 8 Max         5       0.6\n 9 Max         7       0.8\n10 Max         8       1  \n11 Peter       1       0.1\n12 Peter       1       0.1\n13 Peter       1       0.2\n14 Peter       1       0.2\n15 Peter       1       0.1\n16 Peter       3       0.3\n17 Peter       5       0.5\n18 Peter       6       0.8\n19 Peter       8       0.9\n20 Peter       9       1.3\n```\n:::\n:::\n\n\n\n## Goal\n\nThe goal of this analysis is to answer the question how the number of drinks relates to the blood alcohol level. Note that we can ignore the column `Person`, since we do not care whether data came from Peter or Max. Thus, we only focus on the two *numeric* columns `drinks` and `blood_alc`. For them, we will do a correlation and a regression analysis.\n\n## Exploring\n\nTo quickly get a first feeling for this dataset, we can use `summary()` and draw a plot via `plot()` or `ggplot()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n    Person              drinks       blood_alc    \n Length:20          Min.   :1.00   Min.   :0.100  \n Class :character   1st Qu.:1.00   1st Qu.:0.200  \n Mode  :character   Median :3.50   Median :0.500  \n                    Mean   :3.85   Mean   :0.515  \n                    3rd Qu.:5.25   3rd Qu.:0.725  \n                    Max.   :9.00   Max.   :1.300  \n```\n:::\n:::\n\n\n\n::: columns\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(y = dat$blood_alc, x = dat$drinks)\n```\n\n::: {.cell-output-display}\n![](correlation_regression_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = dat) +\n  aes(x = drinks, y = blood_alc) +\n  geom_point(size = 2) +\n  scale_x_continuous(\n    name = \"Number of drinks\",\n    limits = c(0, 9),\n    breaks = seq(0, 9),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_y_continuous(\n    name = \"Blood alcohol content\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](correlation_regression_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n:::\n\nApparently, the number of drinks ranges from 1 to 9 with a mean of 3.85, while the measured blood alcohol levels range from 0.1 to 1.3 with a mean of 0.515. The plots show a clear trend of increasing blood alcohol levels with a higher number of drinks - which is what we would expect.\n\n# Correlation\n\nOne way of actually putting a number on this relationship is to estimate the correlation. When people talk about correlation ($\\rho$ or $r$) in statistics, they usually refer to the [Pearson correlation coefficient](https://www.wikiwand.com/en/Pearson_correlation_coefficient), which is a measure of linear correlation between two numeric variables. Correlation can only have values between -1 and 1, where 0 means *no correlation*, while all other possible values are either negative or positive correlations. The farther away from 0, the stronger is the correlation.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](../../img/correlation.PNG){fig-align='center' width=60%}\n:::\n:::\n\n\n\nSimply put, a positive correlation means *\"if one variable gets bigger, the other also gets bigger\"* and a negative correlation means *\"if one variable gets bigger, the other gets smaller\"*. Therefore, it does not matter which of the two variables is the first (\"x\") or the second (\"y\") variable. Thus, a correlation estimate is not like a model and it cannot make predictions. Finally, [*\"correlation does not imply causation\"*](https://www.wikiwand.com/en/Correlation_does_not_imply_causation) means that just because you found a (strong) correlation between two things, you cannot conclude that there is a cause-and-effect relationship between the two, which becomes clear when looking at [these examples](https://www.tylervigen.com/spurious-correlations).\n\n## Get it\n\nIf you only want to get the actual correlation estimate, you can use the function `cor()` and provide the two numeric variables (as vectors):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(dat$drinks, dat$blood_alc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n[1] 0.9559151\n```\n:::\n:::\n\n\n\nSo the correlation between number of drinks and blood alcohol content in our sample is ca. 0.96 and thus very strong, since it is almost 1.\n\n## Test it\n\nIf you would like additional information, such as a confidence interval and a test resulting in a p-value, you can use `cor.test()` instead of `cor()`. We may also use the [{broom}](../misc/usefulthings.qmd#broom) package to get the results in a more convenient format.\n\n::: columns\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmycor <- cor.test(dat$drinks, \n                  dat$blood_alc)\nmycor\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n\n\tPearson's product-moment correlation\n\ndata:  dat$drinks and dat$blood_alc\nt = 13.811, df = 18, p-value = 5.089e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8897837 0.9827293\nsample estimates:\n      cor \n0.9559151 \n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(mycor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 1 x 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>     <chr>      \n1    0.956      13.8 5.09e-11        18    0.890     0.983 Pearson'~ two.sided  \n```\n:::\n:::\n\n\n:::\n:::\n\nLooking at this longer output, you can see the sample estimate at the bottom, a confidence interval above it and a p-value with the corresponding test hypothesis above that. Run `?cor.test()` and look at the \"Details\" section for more info. Here, our correlation estimate of 0.96 is significantly different from 0, since the p-value is 0.0000000000509 and therefore $< 0.05$. Furthermore, the confidence interval is 0.890 - 0.983 meaning that we are 95% sure that the true correlation is somewhere in that range.\n\n::: {.callout-tip collapse=\"true\"}\n## Additional Resources\n\n-   [{correlation}](https://easystats.github.io/correlation/)\n-   [{corrr}](https://corrr.tidymodels.org/)\n-   [{ggcorrplot}](http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2)\n:::\n\n# Simple linear regression\n\nWhen people talk about regression in statistics, they usually refer to [simple linear regression](https://www.wikiwand.com/en/Simple_linear_regression), which - simply put - finds the best straight line that goes through dots in a scatter plot of two numeric variables:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](../../img/regressionexamples.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\nThe linear model behind such a straight line is simply:\n\n$$ y = \\alpha + \\beta x$$\n\nwhere $\\alpha$ or $a$ is the intercept and $\\beta$ or $b$ is the slope, while $y$ and $x$ are our data points. Fitting such a regression is really just finding the optimal estimates for $\\alpha$ and $\\beta$.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](../../img/regressiontheory.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\nIn contrast to correlation, a simple linear regression is a model and it therefore matters which variable is $y$ (dependent variable) and which is $x$ (independent), because after fitting the regression, the latter can be used to predict the former.\n\n## Get it\n\nIn R, we can use the `lm()` function for fitting linear models so that it fits the simple linear regression equation shown above easily:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg <- lm(formula = blood_alc ~ drinks,\n          data = dat)\n```\n:::\n\n\n\nAs you can see, we refer to our data object `dat` in the `data =` argument so that in the `formula =` argument we only need to write the names of the respective columns in `dat`. Furthermore, we store the results in the `reg` object. When looking at this object, we get the following results:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n\nCall:\nlm(formula = blood_alc ~ drinks, data = dat)\n\nCoefficients:\n(Intercept)       drinks  \n    0.04896      0.12105  \n```\n:::\n:::\n\n\n\nFirst, our command is repeated and then the \"Coefficients\" are shown, which are indeed the estimates for $a$ and $b$. So the best straight line is:\n\n$$ bloodalc = 0.049 + 0.121 * drinks $$\n\nwhich looks like this:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = dat) +\n  aes(x = drinks, y = blood_alc) +\n  geom_point(size = 2) +\n  geom_abline(\n    intercept = reg$coefficients[1],\n    slope = reg$coefficients[2],\n    color = \"#00923f\", \n    linewidth = 1\n  ) +\n  scale_x_continuous(\n    name = \"Number of drinks\",\n    limits = c(0, 9),\n    breaks = seq(0, 9),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_y_continuous(\n    name = \"Blood alcohol content\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](correlation_regression_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\nHere is a little more info why `formula = blood_alc ~ drinks` leads to R estimating the $a$ and $b$ we want: What makes sense is that `blood_alc` is $y$, `drinks` is $x$ and `~` would therefore be the $=$ in our equation. However, why is it we never had to write anything about $a$ or $b$? The answer is that (i) when fitting a linear model, there is usually always an intercept (=$a$) by default and (ii) when writing a numeric variable (=`drinks`) as on the right side of the equation, it will automatically be assumed to have a slope (=$b$) multiplied with it. Accordingly, `blood_alc ~ drinks` automatically translates to `blood_alc = a + b*drinks` so to speak.\n\n## Is this right?\n\nAfter fitting a model, you may use it to make predictions. Here is one way of obtaining the expected blood alcohol content for having 0 to 9 drinks according to our simple linear regression via [{modelbased}](../misc/usefulthings.qmd#modelbased):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreddat <- tibble(drinks = seq(0, 9)) %>% \n  estimate_expectation(model = reg) %>% \n  as_tibble()\n\npreddat\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 10 x 5\n   drinks Predicted     SE  CI_low CI_high\n    <int>     <dbl>  <dbl>   <dbl>   <dbl>\n 1      0    0.0490 0.0406 -0.0363   0.134\n 2      1    0.170  0.0337  0.0993   0.241\n 3      2    0.291  0.0278  0.233    0.349\n 4      3    0.412  0.0238  0.362    0.462\n 5      4    0.533  0.0226  0.486    0.581\n 6      5    0.654  0.0247  0.602    0.706\n 7      6    0.775  0.0294  0.713    0.837\n 8      7    0.896  0.0357  0.821    0.971\n 9      8    1.02   0.0428  0.927    1.11 \n10      9    1.14   0.0505  1.03     1.24 \n```\n:::\n:::\n\n\n\nYou may notice that according to our model, the expected alcohol content in your blood when having 0 drinks is actually 0.049 and thus larger than 0. This is obviously not true in real life. Instead, the true intercept should actually be exactly 0, so what went wrong?\n\nFirst of all, data will never be perfect in the sense that the when a parameter really is e.g. 42, its estimate based on measured data is also exactly 42.000000... . Instead, there are e.g. measurement errors: Peter and Max may have forgotten a drink or two or their device to measure the alcohol content is not precise enough. In fact, this would most likely be the underlying reason here - but remember that I made the data up.\n\nSo I would like you to think about the issue from two other angles:\n\n1.  Are the results really saying the intercept is \\> 0?\n2.  Did we even ask the right question or should we have fitted a different model?\n\n### Are the results really saying the intercept is \\> 0?\n\nNo, they are not. Yes, the sample estimate for the intercept is 0.049, but when looking at more detailed information via e.g. `summary()`. We may also use the [{broom}](../misc/usefulthings.qmd#broom) package to get the results in a more convenient format.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(reg, conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 2 x 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   0.0490   0.0406       1.21 2.43e- 1  -0.0363     0.134\n2 drinks        0.121    0.00876     13.8  5.09e-11   0.103      0.139\n```\n:::\n:::\n\n\n\nyou can see that the p-value for the intercept is 0.243, which is larger than 0.05 and thus saying that we could not find the intercept to be significantly different from 0. A second indication can be found when looking at the confidence interval of the expected value for having 0 drinks in the table above: `[-0.0363, 0.1340]`. This interval actually includes 0 which suggests that the true expected blood alcohol content for having 0 drinks may indeed be 0.\n\n### Should we have fitted a different model?\n\nWe certainly **could** have and we will actually do it now. It must be clear that statistically speaking there was nothing wrong with our analysis. However, from a biological standpoint or in other words - because of our background knowledge and expertise as scientists - we could have indeed actively decided for a regression analysis that does **not** have an intercept and is thus forced to start 0 in terms of blood alcohol content. After all, statistics is just a tool to help us make conclusions. It is a powerful tool, but it will always be our responsibility to \"ask the right questions\" i.e. apply expedient methods.\n\nA simple linear regression without an intercept is strictly speaking no longer \"simple\", since it no longer has the typical equation, but instead this one:\n\n$$ y = \\beta x$$\n\nTo tell `lm()` that it should not estimate the default intercept, we simply add `0 +` right after the `~`. As expected, we only get one estimate for the slope:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreg_noint <- lm(formula = blood_alc ~ 0 + drinks, data = dat)\nreg_noint\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n\nCall:\nlm(formula = blood_alc ~ 0 + drinks, data = dat)\n\nCoefficients:\ndrinks  \n0.1298  \n```\n:::\n:::\n\n\n\nmeaning that this regression with no intercept is estimated as\n\n$$ bloodalc = 0.1298 * drinks $$\n\nand must definitely predict 0 `blood_alc` when having 0 `drinks`. As a final result, we can compare both regression lines visually in a ggplot:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = dat) + \n  aes(x = drinks, y = blood_alc) +\n  geom_point(size = 2) +\n  geom_abline(\n    intercept = reg$coefficients[1],\n    slope = reg$coefficients[2],\n    color = \"#00923f\", \n    linewidth = 1\n  ) +\n  geom_abline(\n    intercept = reg_noint$coefficients[1],\n    slope = reg_noint$coefficients[2],\n    color = \"#e4572e\", \n    linewidth = 1\n  ) +\n  scale_x_continuous(\n    name = \"Number of drinks\",\n    limits = c(0, 9),\n    breaks = seq(0, 9),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_y_continuous(\n    name = \"Blood alcohol content\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](correlation_regression_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n",
    "supporting": [
      "correlation_regression_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}