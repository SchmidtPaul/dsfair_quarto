{
  "hash": "a06dd1b0fa19f2b8a5598c6709dc9c15",
  "result": {
    "markdown": "---\ntitle: \"Bad data & Outliers\"\nabstract: \"Cleaning data, dealing with missing data and comparing results for correlation and regression before vs. after removing an outlier from the data.\"\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# (install &) load packages\npacman::p_load(\n  broom,\n  conflicted,\n  here,\n  janitor,\n  naniar,\n  readxl,\n  tidyverse\n)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)\n```\n:::\n\n\n\nThere are two download links:\n\n-   Download the **original** excel file [here](https://drive.google.com/uc?export=download&id=1HuzLyyIt0Ihh8crV2qV1qahC_lB3Zj2G).\n\n-   Download the **formatted** excel file [here](https://drive.google.com/uc?export=download&id=1xUDsQPI9kIRHYpNOvaOS4kYgOiXA15LD).\n\n# Data\n\nImagine that this dataset was obtained by you. You spent an entire day walking around the campus of a university and asked a total of 29 people for things like how old they are and you also tested how well they could see on a scale of 1-10.\n\n## Import\n\nAssuming you are working in a [R-project](https://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects), save the formatted file somewhere within the project directory. I have saved it within a sub folder called `data` so that the relative path to my file is `data/vision_fixed.xls`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath <- here(\"data\", \"vision_fixed.xls\")\ndat <- read_excel(path)\n\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 29 x 9\n   Person   Ages Gender `Civil state` Height Profession Vision Distance PercDist\n   <chr>   <dbl> <chr>  <chr>          <dbl> <chr>       <dbl>    <dbl>    <dbl>\n 1 Andrés     25 M      S                180 Student        10      1.5     15  \n 2 Anja       29 F      S                168 Professio~     10      4.5     45  \n 3 Armando    31 M      S                169 Professio~      9      4.5     50  \n 4 Carlos     25 M      M                185 Professio~      8      6       75  \n 5 Cristi~    23 F      <NA>             170 Student        10      3       30  \n 6 Delfa      39 F      M                158 Professio~      6      4.5     75  \n 7 Eduardo    28 M      S                166 Professio~      8      4.5     56.2\n 8 Enrique    NA <NA>   <NA>              NA Professio~     NA      6       NA  \n 9 Fanny      25 F      M                164 Student         9      3       33.3\n10 Franci~    46 M      M                168 Professio~      8      4.5     56.2\n# i 19 more rows\n```\n:::\n:::\n\n\n\nThis is optional, but we could argue that our column names are not in a desirable format. To deal with this, we can use the `clean_names()` functions of [{janitor}](https://sfirke.github.io/janitor/). This package has several more handy functions for cleaning data that are worth checking out.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- dat %>% clean_names()\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 29 x 9\n   person    ages gender civil_state height profession vision distance perc_dist\n   <chr>    <dbl> <chr>  <chr>        <dbl> <chr>       <dbl>    <dbl>     <dbl>\n 1 Andrés      25 M      S              180 Student        10      1.5      15  \n 2 Anja        29 F      S              168 Professio~     10      4.5      45  \n 3 Armando     31 M      S              169 Professio~      9      4.5      50  \n 4 Carlos      25 M      M              185 Professio~      8      6        75  \n 5 Cristina    23 F      <NA>           170 Student        10      3        30  \n 6 Delfa       39 F      M              158 Professio~      6      4.5      75  \n 7 Eduardo     28 M      S              166 Professio~      8      4.5      56.2\n 8 Enrique     NA <NA>   <NA>            NA Professio~     NA      6        NA  \n 9 Fanny       25 F      M              164 Student         9      3        33.3\n10 Francis~    46 M      M              168 Professio~      8      4.5      56.2\n# i 19 more rows\n```\n:::\n:::\n\n\n\n## Goal\n\nVery much like in the previous chapter, our goal is to look at the relationship of two numeric variables: `ages` and `vision`. What is new about this data is, that it (i) has missing values and (ii) has a potential outlier.\n\n## Exploring\n\nTo quickly get a first feeling for this dataset, we can use `summary()` and draw a plot via `plot()` or `ggplot()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n    person               ages          gender          civil_state       \n Length:29          Min.   :22.00   Length:29          Length:29         \n Class :character   1st Qu.:25.00   Class :character   Class :character  \n Mode  :character   Median :26.00   Mode  :character   Mode  :character  \n                    Mean   :30.61                                        \n                    3rd Qu.:29.50                                        \n                    Max.   :55.00                                        \n                    NA's   :1                                            \n     height       profession            vision          distance    \n Min.   :145.0   Length:29          Min.   : 3.000   Min.   :1.500  \n 1st Qu.:164.8   Class :character   1st Qu.: 7.000   1st Qu.:1.500  \n Median :168.0   Mode  :character   Median : 9.000   Median :3.000  \n Mean   :168.2                      Mean   : 8.357   Mean   :3.466  \n 3rd Qu.:172.8                      3rd Qu.:10.000   3rd Qu.:4.500  \n Max.   :190.0                      Max.   :10.000   Max.   :6.000  \n NA's   :1                          NA's   :1                       \n   perc_dist     \n Min.   : 15.00  \n 1st Qu.: 20.24  \n Median : 40.18  \n Mean   : 45.45  \n 3rd Qu.: 57.19  \n Max.   :150.00  \n NA's   :1       \n```\n:::\n:::\n\n\n\n::: columns\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(y = dat$vision, x = dat$ages)\n```\n\n::: {.cell-output-display}\n![](baddata_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = dat) +\n  aes(x = ages, y = vision) +\n  geom_point(size = 2) +\n  scale_x_continuous(\n    name = \"Person's age\",\n    limits = c(20, 60),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_y_continuous(\n    name = \"Person's vision\",\n    limits = c(0, NA),\n    breaks = seq(0, 10, 2),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](baddata_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n:::\n\nApparently, most people are in their 20s and can see quite well, however some people are older and they tend to have a vision that's a little worse.\n\n# Missing Data?\n\nWhile the data has 29 rows, it actually only holds `vision` and `ages` information for 28 people. This is because instead of values, there are `NA` (Not Available) for one person. Note that `NA` as missing values are treated somewhat special in R. As an example: If you want to filter for missing values, you cannot write `value == NA`, but must instead write `is.na(value)`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat %>% \n  filter(is.na(vision))\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 1 x 9\n  person   ages gender civil_state height profession   vision distance perc_dist\n  <chr>   <dbl> <chr>  <chr>        <dbl> <chr>         <dbl>    <dbl>     <dbl>\n1 Enrique    NA <NA>   <NA>            NA Professional     NA        6        NA\n```\n:::\n:::\n\n\n\nMoreover, if you want to count the missing observations (per group) in a dataset, the most basic way of doing it is `sum(is.na(values))` (or for not-missing: `sum(!is.na(values))`). However, if you are dealing with missing values a lot, you may also want to check out [{naniar}](http://naniar.njtierney.com/), which *provides principled, tidy ways to summarise, visualise, and manipulate missing data*.\n\n::: columns\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# standard functions\ndat %>% \n  group_by(profession) %>% \n  summarise(\n    n_rows = n(),\n    n_NA = sum(is.na(vision)),\n    n_notNA = sum(!is.na(vision))\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 2 x 4\n  profession   n_rows  n_NA n_notNA\n  <chr>         <int> <int>   <int>\n1 Professional     18     1      17\n2 Student          11     0      11\n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# naniar functions\ndat %>% \n  group_by(profession) %>% \n  summarise(\n    n_rows = n(),\n    n_NA = n_miss(vision),\n    n_notNA = n_complete(vision)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 2 x 4\n  profession   n_rows  n_NA n_notNA\n  <chr>         <int> <int>   <int>\n1 Professional     18     1      17\n2 Student          11     0      11\n```\n:::\n:::\n\n\n:::\n:::\n\n# Corr. & Reg.\n\nLet's estimate the correlation and simple linear regression and look at the results in a tidy format:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor <- cor.test(dat$vision, dat$ages)\ntidy(cor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 1 x 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     <dbl>     <dbl>   <dbl>     <int>    <dbl>     <dbl> <chr>      <chr>      \n1   -0.497     -2.92 0.00709        26   -0.734    -0.153 Pearson's~ two.sided  \n```\n:::\n\n```{.r .cell-code}\nreg <- lm(vision ~ ages, data = dat)\ntidy(reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 2 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  11.1       0.996      11.2  1.97e-11\n2 ages         -0.0910    0.0311     -2.92 7.09e- 3\n```\n:::\n:::\n\n\n\nThus, we have a moderate, negative correlation of -0.497 and for the regression we have *vision = 11.14 + -0.09 ages*. We can plot the regression line, too:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = dat) +\n  aes(x = ages, y = vision) +\n  geom_point(size = 2) +\n    geom_smooth(\n    method = \"lm\",\n    formula = \"y ~ x\",\n    se = FALSE,\n    color = \"#00923f\"\n  ) +\n  scale_x_continuous(\n    name = \"Person's age\",\n    limits = c(20, 55),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_y_continuous(\n    name = \"Person's vision\",\n    limits = c(0, NA),\n    breaks = seq(0, 10, 2),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](baddata_files/figure-pdf/unnamed-chunk-12-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n# Outlier?\n\nLooking at the plot, you may find one data point to oddly stick out from all others: Apparently there was one person in their mid-20s who had a vision score of only 3, which is the lowest by far.\n\n::: callout-note\nHere, we manually and thus subjectively identified a data point as a potential outlier. We do not discuss automatic and thus objective approaches for outlier detection, but see e.g. [here](https://statsandr.com/blog/outliers-detection-in-r/) or [here](https://datascienceplus.com/outlier-detection-and-treatment-with-r/).\n:::\n\n## Step 1: Investigate\n\nIn such a scenario, the first thing you should do is find out more about this suspicious data point. In our case, we would start by finding out the person's name. One way of doing this is by simply filtering the data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat %>% \n  filter(vision == 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 1 x 9\n  person   ages gender civil_state height profession   vision distance perc_dist\n  <chr>   <dbl> <chr>  <chr>        <dbl> <chr>         <dbl>    <dbl>     <dbl>\n1 Rolando    26 M      M              180 Professional      3      4.5       150\n```\n:::\n:::\n\n\n\nWe find that it was 26 year old Rolando who supposedly had a vision score of only 3.\n\n## Step 2: Act\n\nSince we pretend it is you who collected the data, you should now\n\n-   think back if you can actually remember Rolando and if he had poor vision and/or\n\n-   find other documents such as your handwritten sheets to verify this number and make sure you did not make any typos transferring the data to your computer.\n\nThis may reaffirm or correct the suspicious data point and thus end the discussion on whether it is an outlier that should be removed from the data. However, you may also decide to delete this value. Yet, it must be realized, that deleting one or multiple values from a dataset almost always affects the results from subsequent statistical analyses - especially if the values stick out from the rest.\n\n# Corr. & Reg. - again\n\nLet us estimate correlation and regression again, but this time excluding Rolando from the dataset. Note that there are multiple ways of obtaining such a subset - two are shown here:\n\n::: columns\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_noRo <- dat %>% \n  filter(person != \"Rolando\")\n```\n:::\n\n\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_noRo <- dat %>% \n  filter(vision > 3)\n```\n:::\n\n\n:::\n:::\n\nWe now apply the same functions to this new dataset:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_noRo <- cor.test(dat_noRo$vision, dat_noRo$ages)\nreg_noRo <- lm(vision ~ ages, data = dat_noRo)\n```\n:::\n\n\n\nand directly compare these results to those from above:\n\n::: columns\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(cor) %>%\n  select(1, 3, 5, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 1 x 4\n  estimate p.value conf.low conf.high\n     <dbl>   <dbl>    <dbl>     <dbl>\n1   -0.497 0.00709   -0.734    -0.153\n```\n:::\n\n```{.r .cell-code}\ntidy(reg) %>% \n  select(1, 2, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 2 x 3\n  term        estimate std.error\n  <chr>          <dbl>     <dbl>\n1 (Intercept)  11.1       0.996 \n2 ages         -0.0910    0.0311\n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(cor_noRo) %>%\n  select(1, 3, 5, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 1 x 4\n  estimate   p.value conf.low conf.high\n     <dbl>     <dbl>    <dbl>     <dbl>\n1   -0.696 0.0000548   -0.851    -0.430\n```\n:::\n\n```{.r .cell-code}\ntidy(reg_noRo) %>% \n  select(1, 2, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 2 x 3\n  term        estimate std.error\n  <chr>          <dbl>     <dbl>\n1 (Intercept)   11.7      0.679 \n2 ages          -0.102    0.0211\n```\n:::\n:::\n\n\n:::\n:::\n\n::: columns\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = dat) +\n  aes(x = ages, y = vision) +\n  geom_point(size = 2) +\n    geom_smooth(\n    method = \"lm\",\n    formula = \"y ~ x\",\n    se = FALSE,\n    color = \"#00923f\"\n  ) +\n  scale_x_continuous(\n    name = \"Person's age\",\n    limits = c(20, 55),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_y_continuous(\n    name = \"Person's vision\",\n    limits = c(0, NA),\n    breaks = seq(0, 10, 2),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](baddata_files/figure-pdf/unnamed-chunk-19-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = dat_noRo) +\n  aes(x = ages, y = vision) +\n  geom_point(size = 2) +\n    geom_smooth(\n    method = \"lm\",\n    formula = \"y ~ x\",\n    se = FALSE,\n    color = \"#e4572e\"\n  ) +\n  scale_x_continuous(\n    name = \"Person's age\",\n    limits = c(20, 55),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_y_continuous(\n    name = \"Person's vision\",\n    limits = c(0, NA),\n    breaks = seq(0, 10, 2),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](baddata_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n:::\n:::\n\nAs we can see, removing Rolando from the dataset changed the correlation quite a bit from -0.5 to -0.7. Furthermore, it's p-value became notably smaller. While it was already \\< 0.05 and thus statistically significant in this case, it must be realized that in other cases removing a single data point can indeed make the difference between a p-value larger and smaller 0.05.\n\nYet, regarding the parameter estimates - intercept ($a$) and slope ($b$) - of the simple linear regression, the changes are not as striking. Even with a visual comparison of the two regression lines, one must look closely to spot the differences.\n\n## R² - Coeff. of det.\n\nNevertheless, it is clear that the red line has a much better fit to the remaining data points, than the green line has - simply because Rolando's data point sticks out so much. One way of measuring how well a regression fits the data is by calculating the coefficient of determination $R^2$, which measures the proportion of total variation in the data explained by the model and can thus range from 0 (=bad) to 1 (=good). It can easily obtained via `glance()`, which is another function from [{broom}](../misc/usefulthings.qmd#broom):\n\n::: columns\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.247         0.218  1.54      8.54 0.00709     1  -50.9  108.  112.\n# i 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"2%\"}\n:::\n\n::: {.column width=\"49%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(reg_noRo)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.custom-output}\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.485         0.464  1.04      23.5 0.0000548     1  -38.4  82.7  86.6\n# i 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n:::\n\n\n:::\n:::\n\nFinally, we find that removing Rolando from the dataset increased the $R^2$ for the simple linear regression from 25% to 49%.\n",
    "supporting": [
      "baddata_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}