[
  {
    "objectID": "exan/simple/crd_mead1993.html",
    "href": "exan/simple/crd_mead1993.html",
    "title": "One-way completely randomized design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflict_prefer(\"filter\", \"dplyr\") \nconflict_prefer(\"select\", \"dplyr\")"
  },
  {
    "objectID": "exan/simple/crd_mead1993.html#import",
    "href": "exan/simple/crd_mead1993.html#import",
    "title": "One-way completely randomized design",
    "section": "Import",
    "text": "Import\n\n\n# data is available online:\npath <- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/Mead1993.csv\"\n\n\n\ndat <- read_csv(path) # use path from above\ndat\n\n# A tibble: 24 × 4\n   variety yield   row   col\n   <chr>   <dbl> <dbl> <dbl>\n 1 v1       25.1     4     2\n 2 v1       17.2     1     6\n 3 v1       26.4     4     1\n 4 v1       16.1     1     4\n 5 v1       22.2     1     2\n 6 v1       15.9     2     4\n 7 v2       40.2     4     4\n 8 v2       35.2     3     1\n 9 v2       32.0     4     6\n10 v2       36.5     2     1\n# … with 14 more rows"
  },
  {
    "objectID": "exan/simple/crd_mead1993.html#format",
    "href": "exan/simple/crd_mead1993.html#format",
    "title": "One-way completely randomized design",
    "section": "Format",
    "text": "Format\nBefore anything, the column variety should be encoded as a factor, since R by default encoded it as a character variable. There are multiple ways to do this - here are two:\n\n\n\ndat <- dat %>% \n  mutate(variety = as.factor(variety))\n\n\n\n\n\ndat <- dat %>% \n  mutate(across(variety, ~ as.factor(.x)))"
  },
  {
    "objectID": "exan/simple/crd_mead1993.html#explore",
    "href": "exan/simple/crd_mead1993.html#explore",
    "title": "One-way completely randomized design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can summarize per variety.\n\ndat %>% \n  group_by(variety) %>% \n  dlookr::describe(yield) %>% \n  select(2:sd, p00, p100) %>%\n  arrange(desc(mean))\n\n# A tibble: 4 × 7\n  variety     n    na  mean    sd   p00  p100\n  <fct>   <int> <int> <dbl> <dbl> <dbl> <dbl>\n1 v2          6     0  37.4  3.95  32.0  43.3\n2 v4          6     0  29.9  2.23  27.6  33.2\n3 v1          6     0  20.5  4.69  15.9  26.4\n4 v3          6     0  19.5  5.56  11.4  25.9\n\n\nAdditionally, we can decide to plot our data:\n\nClick to show/hide codeggplot(data = dat) +\n  aes(y = yield, x = variety) +\n  geom_point() +\n  scale_x_discrete(\n    name = \"Variety\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  theme_classic()\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a completely randomized design; CRD) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}:\n\nClick to show/hide codedesplot(\n  data = dat, \n  flip = TRUE, # row 1 on top, not on bottom\n  form = variety ~ col + row, # fill color per variety\n  text = variety, # variety names per plot\n  cex = 1, # variety names: font size\n  shorten = \"no\", # variety names: don't abbreviate\n  main = \"Field layout\", # plot title\n  show.key = FALSE # hide legend \n  )"
  },
  {
    "objectID": "exan/simple/latsq_bridges1989.html",
    "href": "exan/simple/latsq_bridges1989.html",
    "title": "One-way latin square design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  agridat,\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflict_prefer(\"filter\", \"dplyr\") \nconflict_prefer(\"select\", \"dplyr\")"
  },
  {
    "objectID": "exan/simple/latsq_bridges1989.html#import",
    "href": "exan/simple/latsq_bridges1989.html#import",
    "title": "One-way latin square design",
    "section": "Import",
    "text": "Import\n\ndat <- agridat::bridges.cucumber %>% \n  as_tibble() %>% \n  filter(loc == \"Clemson\") %>% # filter data from only one location\n  select(-loc) # remove loc column which is now unnecessary\n\ndat\n\n# A tibble: 16 × 4\n   gen        row   col yield\n   <fct>    <int> <int> <dbl>\n 1 Dasher       1     3  44.2\n 2 Dasher       2     4  54.1\n 3 Dasher       3     2  47.2\n 4 Dasher       4     1  36.7\n 5 Guardian     1     4  33  \n 6 Guardian     2     2  13.6\n 7 Guardian     3     1  44.1\n 8 Guardian     4     3  35.8\n 9 Poinsett     1     1  11.5\n10 Poinsett     2     3  22.4\n11 Poinsett     3     4  30.3\n12 Poinsett     4     2  21.5\n13 Sprint       1     2  15.1\n14 Sprint       2     1  20.3\n15 Sprint       3     3  41.3\n16 Sprint       4     4  27.1"
  },
  {
    "objectID": "exan/simple/latsq_bridges1989.html#format",
    "href": "exan/simple/latsq_bridges1989.html#format",
    "title": "One-way latin square design",
    "section": "Format",
    "text": "Format\nFor our analysis, gen, row and col should be encoded as factors. However, the desplot() function needs row and col as formatted as integers. Therefore we create copies of these columns encoded as factors and named rowF and colF. Below are two ways how to achieve this:\n\n\n\ndat <- dat %>%\n  mutate(\n    colF = as.factor(col),\n    rowF = as.factor(row)\n  )\n\n\n\n\n\ndat <- dat %>%\n  mutate(across(\n    .cols = c(row, col), \n    .fns = ~ as.factor(.x), \n    .names = (\"{.col}F\")\n  ))"
  },
  {
    "objectID": "exan/simple/latsq_bridges1989.html#explore",
    "href": "exan/simple/latsq_bridges1989.html#explore",
    "title": "One-way latin square design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can summarize per genotype, per row and per column.\n\ndat %>% \n  group_by(gen) %>% \n  dlookr::describe(yield) %>% \n  select(2:sd) %>%\n  arrange(desc(mean))\n\n# A tibble: 4 × 5\n  gen          n    na  mean    sd\n  <fct>    <int> <int> <dbl> <dbl>\n1 Dasher       4     0  45.6  7.21\n2 Guardian     4     0  31.6 12.9 \n3 Sprint       4     0  26.0 11.4 \n4 Poinsett     4     0  21.4  7.71\n\n\n\n\n\ndat %>% \n  group_by(rowF) %>% \n  dlookr::describe(yield) %>% \n  select(2:sd) %>%\n  arrange(desc(mean))\n\n# A tibble: 4 × 5\n  rowF      n    na  mean    sd\n  <fct> <int> <int> <dbl> <dbl>\n1 3         4     0  40.7  7.36\n2 4         4     0  30.3  7.28\n3 2         4     0  27.6 18.1 \n4 1         4     0  26.0 15.4 \n\n\n\n\n\n\ndat %>% \n  group_by(colF) %>% \n  dlookr::describe(yield) %>% \n  select(2:sd) %>%\n  arrange(desc(mean))\n\n# A tibble: 4 × 5\n  colF      n    na  mean    sd\n  <fct> <int> <int> <dbl> <dbl>\n1 4         4     0  36.1 12.2 \n2 3         4     0  35.9  9.67\n3 1         4     0  28.2 14.9 \n4 2         4     0  24.4 15.6 \n\n\n\n\nAdditionally, we can decide to plot our data:\n\nClick to show/hide codeggplot(data = dat) +\n  aes(y = yield, x = gen, color = colF, shape = rowF) +\n  geom_point() +\n    scale_x_discrete(\n    name = \"Genotype\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_color_discrete(\n    name = \"Column\"\n  ) +\n  scale_shape_discrete(\n    name = \"Row\"\n  ) +\n  theme_classic()\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a Latin square design) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}. We can even create a second field plan that gives us a feeling for the yields per plot.\n\n\n\nClick to show/hide codedesplot(\n  data = dat,\n  flip = TRUE, # row 1 on top, not on bottom\n  form = gen ~ col + row, # fill color per genotype       \n  out1 = rowF, # line between rows\n  out2 = colF, # line between columns\n  out1.gpar = list(col = \"black\", lwd = 2), # out1 line style\n  out2.gpar = list(col = \"black\", lwd = 2), # out2 line style\n  text = gen, # gen names per plot\n  cex = 1, # gen names: font size\n  shorten = FALSE, # gen names: don't abbreviate\n  main = \"Field layout\", # plot title\n  show.key = FALSE # hide legend\n  ) \n\n\n\n\n\n\n\n\nClick to show/hide codedesplot(\n  data = dat,\n  flip = TRUE, # row 1 on top, not on bottom\n  form = yield ~ col + row, # fill color according to yield     \n  out1 = rowF, # line between rows\n  out2 = colF, # line between columns\n  out1.gpar = list(col = \"black\", lwd = 2), # out1 line style\n  out2.gpar = list(col = \"black\", lwd = 2), # out2 line style\n  text = gen, # gen names per plot\n  cex = 1, # gen names: font size\n  shorten = FALSE, # gen names: don't abbreviate\n  main = \"Yield per plot\", # plot title\n  show.key = FALSE # hide legend\n  ) \n\n\n\n\n\n\nThus, Dasher seems to be the most promising genotype in terms of yield. Moreover, it can be seen that yields were generally higher in column 4 and row 3."
  },
  {
    "objectID": "exan/simple/rcbd_clewerscarisbrick2001.html",
    "href": "exan/simple/rcbd_clewerscarisbrick2001.html",
    "title": "One-way randomized complete block design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflict_prefer(\"filter\", \"dplyr\") \nconflict_prefer(\"select\", \"dplyr\")"
  },
  {
    "objectID": "exan/simple/rcbd_clewerscarisbrick2001.html#import",
    "href": "exan/simple/rcbd_clewerscarisbrick2001.html#import",
    "title": "One-way randomized complete block design",
    "section": "Import",
    "text": "Import\n\n\n# data is available online:\npath <- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/Clewer&Scarisbrick2001.csv\"\n\n\n\ndat <- read_csv(path) # use path from above\ndat\n\n# A tibble: 12 × 5\n   block cultivar yield   row   col\n   <chr> <chr>    <dbl> <dbl> <dbl>\n 1 B1    C1         7.4     2     1\n 2 B1    C2         9.8     3     1\n 3 B1    C3         7.3     1     1\n 4 B1    C4         9.5     4     1\n 5 B2    C1         6.5     1     2\n 6 B2    C2         6.8     4     2\n 7 B2    C3         6.1     3     2\n 8 B2    C4         8       2     2\n 9 B3    C1         5.6     2     3\n10 B3    C2         6.2     1     3\n11 B3    C3         6.4     3     3\n12 B3    C4         7.4     4     3"
  },
  {
    "objectID": "exan/simple/rcbd_clewerscarisbrick2001.html#format",
    "href": "exan/simple/rcbd_clewerscarisbrick2001.html#format",
    "title": "One-way randomized complete block design",
    "section": "Format",
    "text": "Format\nBefore anything, the columns block and cultivar should be encoded as factors, since R by default encoded them as character.\n\ndat <- dat %>%\n  mutate(across(c(block, cultivar), ~ as.factor(.x)))"
  },
  {
    "objectID": "exan/simple/rcbd_clewerscarisbrick2001.html#explore",
    "href": "exan/simple/rcbd_clewerscarisbrick2001.html#explore",
    "title": "One-way randomized complete block design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can summarize per block and per cultivar.\n\n\n\ndat %>% \n  group_by(cultivar) %>% \n  dlookr::describe(yield) %>% \n  select(2:sd) %>%\n  arrange(desc(mean))\n\n# A tibble: 4 × 5\n  cultivar     n    na  mean    sd\n  <fct>    <int> <int> <dbl> <dbl>\n1 C4           3     0   8.3 1.08 \n2 C2           3     0   7.6 1.93 \n3 C3           3     0   6.6 0.624\n4 C1           3     0   6.5 0.9  \n\n\n\n\n\n\ndat %>% \n  group_by(block) %>% \n  dlookr::describe(yield) %>% \n  select(2:sd) %>%\n  arrange(desc(mean))\n\n# A tibble: 3 × 5\n  block     n    na  mean    sd\n  <fct> <int> <int> <dbl> <dbl>\n1 B1        4     0  8.5  1.33 \n2 B2        4     0  6.85 0.819\n3 B3        4     0  6.4  0.748\n\n\n\n\nAdditionally, we can decide to plot our data:\n\nClick to show/hide codeggplot(data = dat) +\n  aes(y = yield, x = cultivar, color = block) +\n  geom_point() +\n    scale_x_discrete(\n    name = \"Cultivar\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_color_discrete(\n    name = \"Block\"\n  ) +\n  theme_classic()\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a randomized complete block design; RCBD) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}. We can even create a second field plan that gives us a feeling for the yields per plot.\n\n\n\nClick to show/hide codedesplot(\n  data = dat,\n  flip = TRUE, # row 1 on top, not on bottom\n  form = cultivar ~ col + row, # fill color per cultivar       \n  out1 = block, # line between blocks                     \n  text = cultivar, # cultivar names per plot\n  cex = 1, # cultviar names: font size\n  shorten = FALSE, # cultivar names: don't abbreviate\n  main = \"Field layout\", # plot title\n  show.key = FALSE # hide legend\n  ) \n\n\n\n\n\n\n\n\nClick to show/hide codedesplot(\n  data = dat,\n  flip = TRUE, # row 1 on top, not on bottom\n  form = yield ~ col + row, # fill color according to yield      \n  out1 = block, # line between blocks                     \n  text = cultivar, # cultivar names per plot\n  cex = 1, # cultviar names: font size\n  shorten = FALSE, # cultivar names: don't abbreviate\n  main = \"Yield per plot\", # plot title\n  show.key = FALSE # hide legend\n  ) \n\n\n\n\n\n\nThus, C4 seems to be the most promising cultivar in terms of yield. Moreover, it can be seen that yields were generally higher in block B1 (left), compared to the other blocks."
  },
  {
    "objectID": "exan/simple/rcbd_gomezgomez1984.html",
    "href": "exan/simple/rcbd_gomezgomez1984.html",
    "title": "Two-way randomized complete block design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  MetBrewer,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflict_prefer(\"filter\", \"dplyr\") \nconflict_prefer(\"select\", \"dplyr\")"
  },
  {
    "objectID": "exan/simple/rcbd_gomezgomez1984.html#import",
    "href": "exan/simple/rcbd_gomezgomez1984.html#import",
    "title": "Two-way randomized complete block design",
    "section": "Import",
    "text": "Import\n\n\n# data is available online:\npath <- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/riceRCBD.csv\"\n\n\n\ndat <- read_csv(path) # use path from above\ndat\n\n# A tibble: 72 × 6\n     row   col rep   N     G     yield\n   <dbl> <dbl> <chr> <chr> <chr> <dbl>\n 1     2     6 rep1  N1    A      4520\n 2     3     4 rep1  N2    A      5598\n 3     2     3 rep1  N4    A      6192\n 4     1     1 rep1  N6    A      8542\n 5     2     1 rep1  N3    A      5806\n 6     3     1 rep1  N5    A      7470\n 7     4     5 rep1  N1    B      4034\n 8     4     1 rep1  N2    B      6682\n 9     3     2 rep1  N4    B      6869\n10     1     2 rep1  N6    B      6318\n# … with 62 more rows"
  },
  {
    "objectID": "exan/simple/rcbd_gomezgomez1984.html#format",
    "href": "exan/simple/rcbd_gomezgomez1984.html#format",
    "title": "Two-way randomized complete block design",
    "section": "Format",
    "text": "Format\nBefore anything, the columns rep, N and G should be encoded as factors, since R by default encoded them as character.\n\ndat <- dat %>%\n  mutate(across(c(rep, N, G), ~ as.factor(.x)))"
  },
  {
    "objectID": "exan/simple/rcbd_gomezgomez1984.html#explore",
    "href": "exan/simple/rcbd_gomezgomez1984.html#explore",
    "title": "Two-way randomized complete block design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can summarize per nitrogen level, per genotype and also per nitrogen-genotype-combination.\n\n\n\ndat %>% \n  group_by(N) %>% \n  dlookr::describe(yield) %>% \n  select(2:sd) %>%\n  arrange(desc(mean))\n\n# A tibble: 6 × 5\n  N         n    na  mean    sd\n  <fct> <int> <int> <dbl> <dbl>\n1 N3       12     0 5866.  832.\n2 N4       12     0 5864. 1434.\n3 N5       12     0 5812  2349.\n4 N6       12     0 5797. 2660.\n5 N2       12     0 5478.  657.\n6 N1       12     0 4054.  672.\n\ndat %>% \n  group_by(G) %>% \n  dlookr::describe(yield) %>% \n  select(2:sd) %>%\n  arrange(desc(mean))\n\n# A tibble: 4 × 5\n  G         n    na  mean    sd\n  <fct> <int> <int> <dbl> <dbl>\n1 A        18     0 6554. 1475.\n2 B        18     0 6156. 1078.\n3 C        18     0 5563. 1269.\n4 D        18     0 3642. 1434.\n\n\n\n\n\n\ndat %>% \n  group_by(N, G) %>% \n  dlookr::describe(yield) %>% \n  select(2:sd) %>%\n  arrange(desc(mean)) %>% \n  print(n=Inf)\n\n# A tibble: 24 × 6\n   N     G         n    na  mean     sd\n   <fct> <fct> <int> <int> <dbl>  <dbl>\n 1 N6    A         3     0 8701.  270. \n 2 N5    A         3     0 7563.   86.9\n 3 N5    B         3     0 6951.  808. \n 4 N4    B         3     0 6895   166. \n 5 N4    A         3     0 6733.  490. \n 6 N5    C         3     0 6687.  496. \n 7 N6    B         3     0 6540.  936. \n 8 N3    A         3     0 6400   523. \n 9 N3    B         3     0 6259   499. \n10 N6    C         3     0 6065. 1097. \n11 N4    C         3     0 6014   515. \n12 N3    C         3     0 5994   101. \n13 N2    B         3     0 5982   684. \n14 N2    A         3     0 5672   458. \n15 N2    C         3     0 5443.  589. \n16 N2    D         3     0 4816   506. \n17 N3    D         3     0 4812   963. \n18 N1    D         3     0 4481.  463. \n19 N1    B         3     0 4306   646. \n20 N1    A         3     0 4253.  248. \n21 N4    D         3     0 3816  1311. \n22 N1    C         3     0 3177.  453. \n23 N5    D         3     0 2047.  703. \n24 N6    D         3     0 1881.  407. \n\n\n\n\nAdditionally, we can decide to plot our data. One way to deal with the combination of two factors would be to use panels/facets in ggplot2.\nNote that we here define a custom set of colors for the Nitrogen levels that will be used throughout this chapter.\n\nNcolors <- met.brewer(\"VanGogh2\", 6) %>% \n  as.vector() %>% \n  set_names(levels(dat$N))\n\n\nClick to show/hide codeggplot(data = dat) +\n  aes(y = yield, x = N, color = N) +\n  facet_wrap(~G, labeller = label_both) +\n  stat_summary(\n    fun = mean,\n    colour = \"grey\",\n    geom = \"line\",\n    linetype = \"dotted\",\n    group = 1\n  ) +\n  geom_point() +\n  scale_x_discrete(\n    name = \"Nitrogen\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_color_manual(\n    values = Ncolors, \n    guide = \"none\"\n  ) +\n  theme_bw()\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a randomized complete block design; RCBD) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}.\n\nClick to show/hide codedesplot(data = dat,\n        form = rep ~ col + row | rep, # fill color per rep, headers per rep\n        col.regions = c(\"white\", \"grey95\", \"grey90\"),\n        text = G, # genotype names per plot\n        cex = 1, # genotype names: font size\n        shorten = FALSE, # genotype names: don't abbreviate\n        col  = N, # color of genotype names for each N-level\n        col.text = Ncolors, # use custom colors from above\n        out1 = col, out1.gpar = list(col = \"darkgrey\"), # lines between columns\n        out2 = row, out2.gpar = list(col = \"darkgrey\"), # lines between rows\n        main = \"Field layout\", # plot title\n        show.key = TRUE, # show legend\n        key.cex = 0.7 # legend font size\n        )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Agriculture in R",
    "section": "",
    "text": "DSFAIR - Data Science for Agriculture in R provides a cookbook with statistical analyses of typical examples in life sciences with focus on experimental agriculture, biology, ecology and other related fields."
  },
  {
    "objectID": "index.html#workshops",
    "href": "index.html#workshops",
    "title": "Data Science for Agriculture in R",
    "section": "Workshops",
    "text": "Workshops\nMoreover, the articles published here serve as the basis for any of my R‑Workshops. Check out\n\n“Prepare for an upcoming workshop”\nand my list of workshops"
  },
  {
    "objectID": "rbasics/baddata.html",
    "href": "rbasics/baddata.html",
    "title": "Bad data & Outliers",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  broom,\n  conflicted,\n  here,\n  janitor,\n  naniar,\n  readxl,\n  tidyverse\n)\n\n# handle function conflicts\nconflict_prefer(\"filter\", \"dplyr\") \nconflict_prefer(\"select\", \"dplyr\")\nThere are two download links:"
  },
  {
    "objectID": "rbasics/baddata.html#import",
    "href": "rbasics/baddata.html#import",
    "title": "Bad data & Outliers",
    "section": "Import",
    "text": "Import\nAssuming you are working in a R-project, save the formatted file somewhere within the project directory. I have saved it within a sub folder called data so that the relative path to my file is data/vision_fixed.xls.\n\npath <- here(\"data\", \"vision_fixed.xls\")\ndat <- read_excel(path)\n\ndat\n\n# A tibble: 29 × 9\n   Person     Ages Gender `Civil state` Height Profession Vision Dista…¹ PercD…²\n   <chr>     <dbl> <chr>  <chr>          <dbl> <chr>       <dbl>   <dbl>   <dbl>\n 1 Andrés       25 M      S                180 Student        10     1.5    15  \n 2 Anja         29 F      S                168 Professio…     10     4.5    45  \n 3 Armando      31 M      S                169 Professio…      9     4.5    50  \n 4 Carlos       25 M      M                185 Professio…      8     6      75  \n 5 Cristina     23 F      <NA>             170 Student        10     3      30  \n 6 Delfa        39 F      M                158 Professio…      6     4.5    75  \n 7 Eduardo      28 M      S                166 Professio…      8     4.5    56.2\n 8 Enrique      NA <NA>   <NA>              NA Professio…     NA     6      NA  \n 9 Fanny        25 F      M                164 Student         9     3      33.3\n10 Francisco    46 M      M                168 Professio…      8     4.5    56.2\n# … with 19 more rows, and abbreviated variable names ¹​Distance, ²​PercDist\n\n\nThis is optional, but we could argue that our column names are not in a desirable format. To deal with this, we can use the clean_names() functions of {janitor}. This package has several more handy functions for cleaning data that are worth checking out.\n\ndat <- dat %>% clean_names()\ndat\n\n# A tibble: 29 × 9\n   person     ages gender civil_state height profession   vision dista…¹ perc_…²\n   <chr>     <dbl> <chr>  <chr>        <dbl> <chr>         <dbl>   <dbl>   <dbl>\n 1 Andrés       25 M      S              180 Student          10     1.5    15  \n 2 Anja         29 F      S              168 Professional     10     4.5    45  \n 3 Armando      31 M      S              169 Professional      9     4.5    50  \n 4 Carlos       25 M      M              185 Professional      8     6      75  \n 5 Cristina     23 F      <NA>           170 Student          10     3      30  \n 6 Delfa        39 F      M              158 Professional      6     4.5    75  \n 7 Eduardo      28 M      S              166 Professional      8     4.5    56.2\n 8 Enrique      NA <NA>   <NA>            NA Professional     NA     6      NA  \n 9 Fanny        25 F      M              164 Student           9     3      33.3\n10 Francisco    46 M      M              168 Professional      8     4.5    56.2\n# … with 19 more rows, and abbreviated variable names ¹​distance, ²​perc_dist"
  },
  {
    "objectID": "rbasics/baddata.html#goal",
    "href": "rbasics/baddata.html#goal",
    "title": "Bad data & Outliers",
    "section": "Goal",
    "text": "Goal\nVery much like in the previous chapter, our goal is to look at the relationship of two numeric variables: ages and vision. What is new about this data is, that it (i) has missing values and (ii) has a potential outlier."
  },
  {
    "objectID": "rbasics/baddata.html#exploring",
    "href": "rbasics/baddata.html#exploring",
    "title": "Bad data & Outliers",
    "section": "Exploring",
    "text": "Exploring\nTo quickly get a first feeling for this dataset, we can use summary() and draw a plot via plot() or ggplot().\n\nsummary(dat)\n\n    person               ages          gender          civil_state       \n Length:29          Min.   :22.00   Length:29          Length:29         \n Class :character   1st Qu.:25.00   Class :character   Class :character  \n Mode  :character   Median :26.00   Mode  :character   Mode  :character  \n                    Mean   :30.61                                        \n                    3rd Qu.:29.50                                        \n                    Max.   :55.00                                        \n                    NA's   :1                                            \n     height       profession            vision          distance    \n Min.   :145.0   Length:29          Min.   : 3.000   Min.   :1.500  \n 1st Qu.:164.8   Class :character   1st Qu.: 7.000   1st Qu.:1.500  \n Median :168.0   Mode  :character   Median : 9.000   Median :3.000  \n Mean   :168.2                      Mean   : 8.357   Mean   :3.466  \n 3rd Qu.:172.8                      3rd Qu.:10.000   3rd Qu.:4.500  \n Max.   :190.0                      Max.   :10.000   Max.   :6.000  \n NA's   :1                          NA's   :1                       \n   perc_dist     \n Min.   : 15.00  \n 1st Qu.: 20.24  \n Median : 40.18  \n Mean   : 45.45  \n 3rd Qu.: 57.19  \n Max.   :150.00  \n NA's   :1       \n\n\n\n\n\nClick to show/hide codeplot(y = dat$vision, x = dat$ages)\n\n\n\n\n\n\n\n\nClick to show/hide codeggplot(data = dat) +\n  aes(x = ages, y = vision) +\n  geom_point(size = 2) +\n  scale_x_continuous(\n    name = \"Person's age\",\n    limits = c(20, 60),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_y_continuous(\n    name = \"Person's vision\",\n    limits = c(0, NA),\n    breaks = seq(0, 10, 2),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n    theme_bw()\n\n\n\n\n\n\nApparently, most people are in their 20s and can see quite well, however some people are older and they tend to have a vision that’s a little worse."
  },
  {
    "objectID": "rbasics/baddata.html#step-1-investigate",
    "href": "rbasics/baddata.html#step-1-investigate",
    "title": "Bad data & Outliers",
    "section": "Step 1: Investigate",
    "text": "Step 1: Investigate\nIn such a scenario, the first thing you should do is find out more about this suspicious data point. In our case, we would start by finding out the person’s name. One way of doing this is by simply filtering the data:\n\ndat %>% \n  filter(vision == 3)\n\n# A tibble: 1 × 9\n  person   ages gender civil_state height profession   vision distance perc_dist\n  <chr>   <dbl> <chr>  <chr>        <dbl> <chr>         <dbl>    <dbl>     <dbl>\n1 Rolando    26 M      M              180 Professional      3      4.5       150\n\n\nWe find that it was 26 year old Rolando who supposedly had a vision score of only 3."
  },
  {
    "objectID": "rbasics/baddata.html#step-2-act",
    "href": "rbasics/baddata.html#step-2-act",
    "title": "Bad data & Outliers",
    "section": "Step 2: Act",
    "text": "Step 2: Act\nSince we pretend it is you who collected the data, you should now\n\nthink back if you can actually remember Rolando and if he had poor vision and/or\nfind other documents such as your handwritten sheets to verify this number and make sure you did not make any typos transferring the data to your computer.\n\nThis may reaffirm or correct the suspicious data point and thus end the discussion on whether it is an outlier that should be removed from the data. However, you may also decide to delete this value. Yet, it must be realized, that deleting one or multiple values from a dataset almost always affects the results from subsequent statistical analyses - especially if the values stick out from the rest."
  },
  {
    "objectID": "rbasics/baddata.html#r²---coeff.-of-det.",
    "href": "rbasics/baddata.html#r²---coeff.-of-det.",
    "title": "Bad data & Outliers",
    "section": "R² - Coeff. of det.",
    "text": "R² - Coeff. of det.\nNevertheless, it is clear that the red line has a much better fit to the remaining data points, than the green line has - simply because Rolando’s data point sticks out so much. One way of measuring how well a regression fits the data is by calculating the coefficient of determination \\(R^2\\), which measures the proportion of total variation in the data explained by the model and can thus range from 0 (=bad) to 1 (=good). It can easily obtained via glance(), which is another function from {broom}:\n\n\n\nglance(reg)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1   0.247   0.218  1.54    8.54 0.00709     1  -50.9  108.  112.    62.0      26\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\n\n\n\n\nglance(reg_noRo)\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1   0.485   0.464  1.04    23.5 5.48e-5     1  -38.4  82.7  86.6    27.1      25\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\n\n\nFinally, we find that removing Rolando from the dataset increased the \\(R^2\\) for the simple linear regression from 25% to 49%."
  },
  {
    "objectID": "rbasics/correlation_regression.html",
    "href": "rbasics/correlation_regression.html",
    "title": "Correlation & Regression",
    "section": "",
    "text": "This chapter is trying to give you a feeling for what correlation and (simple linear) regression is. I am aware that the example data doesn’t have anything to do with agriculture or related fields, but I decided to keep it because it allows for an intuitive conclusion at the end."
  },
  {
    "objectID": "rbasics/correlation_regression.html#import",
    "href": "rbasics/correlation_regression.html#import",
    "title": "Correlation & Regression",
    "section": "Import",
    "text": "Import\n\n\n# data is available online:\npath <- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/DrinksPeterMax.csv\"\n\n\n\ndat <- read_csv(path) # use path from above\ndat\n\n# A tibble: 20 × 3\n   Person drinks blood_alc\n   <chr>   <dbl>     <dbl>\n 1 Max         1       0.2\n 2 Max         2       0.3\n 3 Max         3       0.5\n 4 Max         3       0.6\n 5 Max         4       0.6\n 6 Max         4       0.5\n 7 Max         4       0.7\n 8 Max         5       0.6\n 9 Max         7       0.8\n10 Max         8       1  \n11 Peter       1       0.1\n12 Peter       1       0.1\n13 Peter       1       0.2\n14 Peter       1       0.2\n15 Peter       1       0.1\n16 Peter       3       0.3\n17 Peter       5       0.5\n18 Peter       6       0.8\n19 Peter       8       0.9\n20 Peter       9       1.3"
  },
  {
    "objectID": "rbasics/correlation_regression.html#goal",
    "href": "rbasics/correlation_regression.html#goal",
    "title": "Correlation & Regression",
    "section": "Goal",
    "text": "Goal\nThe goal of this analysis is to answer the question how the number of drinks relates to the blood alcohol level. Note that we can ignore the column Person, since we do not care whether data came from Peter or Max. Thus, we only focus on the two numeric columns drinks and blood_alc. For them, we will do a correlation and a regression analysis."
  },
  {
    "objectID": "rbasics/correlation_regression.html#exploring",
    "href": "rbasics/correlation_regression.html#exploring",
    "title": "Correlation & Regression",
    "section": "Exploring",
    "text": "Exploring\nTo quickly get a first feeling for this dataset, we can use summary() and draw a plot via plot() or ggplot().\n\nsummary(dat)\n\n    Person              drinks       blood_alc    \n Length:20          Min.   :1.00   Min.   :0.100  \n Class :character   1st Qu.:1.00   1st Qu.:0.200  \n Mode  :character   Median :3.50   Median :0.500  \n                    Mean   :3.85   Mean   :0.515  \n                    3rd Qu.:5.25   3rd Qu.:0.725  \n                    Max.   :9.00   Max.   :1.300  \n\n\n\n\n\nClick to show/hide codeplot(y = dat$blood_alc, x = dat$drinks)\n\n\n\n\n\n\n\n\nClick to show/hide codeggplot(data = dat) +\n  aes(x = drinks, y = blood_alc) +\n  geom_point(size = 2) +\n  scale_x_continuous(\n    name = \"Number of drinks\",\n    limits = c(0, 9),\n    breaks = seq(0, 9),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_y_continuous(\n    name = \"Blood alcohol content\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n    theme_classic()\n\n\n\n\n\n\nApparently, the number of drinks ranges from 1 to 9 with a mean of 3.85, while the measured blood alcohol levels range from 0.1 to 1.3 with a mean of 0.515. The plots show a clear trend of increasing blood alcohol levels with a higher number of drinks - which is what we would expect."
  },
  {
    "objectID": "rbasics/correlation_regression.html#get-it",
    "href": "rbasics/correlation_regression.html#get-it",
    "title": "Correlation & Regression",
    "section": "Get it",
    "text": "Get it\nIf you only want to get the actual correlation estimate, you can use the function cor() and provide the two numeric variables (as vectors):\n\ncor(dat$drinks, dat$blood_alc)\n\n[1] 0.9559151\n\n\nSo the correlation between number of drinks and blood alcohol content in our sample is ca. 0.96 and thus very strong, since it is almost 1."
  },
  {
    "objectID": "rbasics/correlation_regression.html#test-it",
    "href": "rbasics/correlation_regression.html#test-it",
    "title": "Correlation & Regression",
    "section": "Test it",
    "text": "Test it\nIf you would like additional information, such as a confidence interval and a test resulting in a p-value, you can use cor.test() instead of cor(). We may also use the {broom} package to get the results in a more convenient format.\n\n\n\nmycor <- cor.test(dat$drinks, \n                  dat$blood_alc)\nmycor\n\n\n    Pearson's product-moment correlation\n\ndata:  dat$drinks and dat$blood_alc\nt = 13.811, df = 18, p-value = 5.089e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8897837 0.9827293\nsample estimates:\n      cor \n0.9559151 \n\n\n\n\n\n\ntidy(mycor)\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method        alter…¹\n     <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>         <chr>  \n1    0.956      13.8 5.09e-11        18    0.890     0.983 Pearson's pr… two.si…\n# … with abbreviated variable name ¹​alternative\n\n\n\n\nLooking at this longer output, you can see the sample estimate at the bottom, a confidence interval above it and a p-value with the corresponding test hypothesis above that. Run ?cor.test() and look at the “Details” section for more info. Here, our correlation estimate of 0.96 is significantly different from 0, since the p-value is 0.0000000000509 and therefore \\(< 0.05\\). Furthermore, the confidence interval is 0.890 - 0.983 meaning that we are 95% sure that the true correlation is somewhere in that range.\n\n\n\n\n\n\nAdditional Resources (click to show)\n\n\n\n\n\n\n{correlation}\n{corrr}\n{ggcorrplot}"
  },
  {
    "objectID": "rbasics/correlation_regression.html#get-it-1",
    "href": "rbasics/correlation_regression.html#get-it-1",
    "title": "Correlation & Regression",
    "section": "Get it",
    "text": "Get it\nIn R, we can use the lm() function for fitting linear models so that it fits the simple linear regression equation shown above easily:\n\nreg <- lm(formula = blood_alc ~ drinks,\n          data = dat)\n\nAs you can see, we refer to our data object dat in the data = argument so that in the formula = argument we only need to write the names of the respective columns in dat. Furthermore, we store the results in the reg object. When looking at this object, we get the following results:\n\nreg\n\n\nCall:\nlm(formula = blood_alc ~ drinks, data = dat)\n\nCoefficients:\n(Intercept)       drinks  \n    0.04896      0.12105  \n\n\nFirst, our command is repeated and then the “Coefficients” are shown, which are indeed the estimates for \\(a\\) and \\(b\\). So the best straight line is:\n\\[ bloodalc = 0.049 + 0.121 * drinks \\]\nwhich looks like this:\n\nClick to show/hide codeggplot(data = dat) +\n  aes(x = drinks, y = blood_alc) +\n  geom_point(size = 2) +\n  geom_smooth(\n    method = \"lm\",\n    formula = \"y ~ x\",\n    se = FALSE,\n    fullrange = TRUE,\n    color = \"#00923f\"\n  ) +\n  scale_x_continuous(\n    name = \"Number of drinks\",\n    limits = c(0, 9),\n    breaks = seq(0, 9),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_y_continuous(\n    name = \"Blood alcohol content\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\nHere is a little more info why formula = blood_alc ~ drinks leads to R estimating the \\(a\\) and \\(b\\) we want: What makes sense is that blood_alc is \\(y\\), drinks is \\(x\\) and ~ would therefore be the \\(=\\) in our equation. However, why is it we never had to write anything about \\(a\\) or \\(b\\)? The answer is that (i) when fitting a linear model, there is usually always an intercept (=\\(a\\)) by default and (ii) when writing a numeric variable (=drinks) as on the right side of the equation, it will automatically be assumed to have a slope (=\\(b\\)) multiplied with it. Accordingly, blood_alc ~ drinks automatically translates to blood_alc = a + b*drinks so to speak."
  },
  {
    "objectID": "rbasics/correlation_regression.html#is-this-right",
    "href": "rbasics/correlation_regression.html#is-this-right",
    "title": "Correlation & Regression",
    "section": "Is this right?",
    "text": "Is this right?\nAfter fitting a model, you may use it to make predictions. Here is one way of obtaining the expected blood alcohol content for having 0 to 9 drinks according to our simple linear regression via {modelbased}:\n\npreddat <- tibble(drinks = seq(0, 9)) %>% \n  estimate_expectation(model = reg) %>% \n  as_tibble()\n\npreddat\n\n# A tibble: 10 × 5\n   drinks Predicted     SE  CI_low CI_high\n    <int>     <dbl>  <dbl>   <dbl>   <dbl>\n 1      0    0.0490 0.0406 -0.0363   0.134\n 2      1    0.170  0.0337  0.0993   0.241\n 3      2    0.291  0.0278  0.233    0.349\n 4      3    0.412  0.0238  0.362    0.462\n 5      4    0.533  0.0226  0.486    0.581\n 6      5    0.654  0.0247  0.602    0.706\n 7      6    0.775  0.0294  0.713    0.837\n 8      7    0.896  0.0357  0.821    0.971\n 9      8    1.02   0.0428  0.927    1.11 \n10      9    1.14   0.0505  1.03     1.24 \n\n\nYou may notice that according to our model, the expected alcohol content in your blood when having 0 drinks is actually 0.049 and thus larger than 0. This is obviously not true in real life. Instead, the true intercept should actually be exactly 0, so what went wrong?\nFirst of all, data will never be perfect in the sense that the when a parameter really is e.g. 42, its estimate based on measured data is also exactly 42.000000… . Instead, there are e.g. measurement errors: Peter and Max may have forgotten a drink or two or their device to measure the alcohol content is not precise enough. In fact, this would most likely be the underlying reason here - but remember that I made the data up.\nSo I would like you to think about the issue from two other angles:\n\nAre the results really saying the intercept is > 0?\nDid we even ask the right question or should we have fitted a different model?\n\nAre the results really saying the intercept is > 0?\nNo, they are not. Yes, the sample estimate for the intercept is 0.049, but when looking at more detailed information via e.g. summary(). We may also use the {broom} package to get the results in a more convenient format.\n\ntidy(reg, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   0.0490   0.0406       1.21 2.43e- 1  -0.0363     0.134\n2 drinks        0.121    0.00876     13.8  5.09e-11   0.103      0.139\n\n\nyou can see that the p-value for the intercept is 0.243, which is larger than 0.05 and thus saying that we could not find the intercept to be significantly different from 0. A second indication can be found when looking at the confidence interval of the expected value for having 0 drinks in the table above: [-0.0363, 0.1340]. This interval actually includes 0 which suggests that the true expected blood alcohol content for having 0 drinks may indeed be 0.\nShould we have fitted a different model?\nWe certainly could have and we will actually do it now. It must be clear that statistically speaking there was nothing wrong with our analysis. However, from a biological standpoint or in other words - because of our background knowledge and expertise as scientists - we could have indeed actively decided for a regression analysis that does not have an intercept and is thus forced to start 0 in terms of blood alcohol content. After all, statistics is just a tool to help us make conclusions. It is a powerful tool, but it will always be our responsibility to “ask the right questions” i.e. apply expedient methods.\nA simple linear regression without an intercept is strictly speaking no longer “simple”, since it no longer has the typical equation, but instead this one:\n\\[ y = \\beta x\\]\nTo tell lm() that it should not estimate the default intercept, we simply add 0 + right after the ~. As expected, we only get one estimate for the slope:\n\nreg_noint <- lm(formula = blood_alc ~ 0 + drinks, data = dat)\nreg_noint\n\n\nCall:\nlm(formula = blood_alc ~ 0 + drinks, data = dat)\n\nCoefficients:\ndrinks  \n0.1298  \n\n\nmeaning that this regression with no intercept is estimated as\n\\[ bloodalc = 0.1298 * drinks \\]\nand must definitely predict 0 blood_alc when having 0 drinks. As a final result, we can compare both regression lines visually in a ggplot:\n\nClick to show/hide codeggplot(data = dat) + \n  aes(x = drinks, y = blood_alc) +\n  geom_point(size = 2) +\n  geom_smooth(\n    method = \"lm\",\n    formula = \"y ~ x\",\n    se = FALSE,\n    fullrange = TRUE,\n    color = \"#00923f\"\n  ) +\n    geom_smooth(\n    method = \"lm\",\n    formula = \"y ~ 0 + x\",\n    se = FALSE,\n    fullrange = TRUE,\n    color = \"#e4572e\"\n  ) +\n  scale_x_continuous(\n    name = \"Number of drinks\",\n    limits = c(0, 9),\n    breaks = seq(0, 9),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_y_continuous(\n    name = \"Blood alcohol content\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  theme_classic()"
  },
  {
    "objectID": "rbasics/firststeps.html",
    "href": "rbasics/firststeps.html",
    "title": "First steps in R",
    "section": "",
    "text": "This chapter is mostly aimed at people who are very new to R. However, people who do know R may still find useful insights from the sections where I emphasize how I use R.\nFurthermore, this tutorial teaches R the way I use it, which means you can do (and may have done) almost everything I do here with other code/functions/approaches. “The way I use it” mostly refers to me using the {tidyverse}, which “is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures” and has become quite popular over the last years. Here is a direct comparison of how to do things in R with base R and via the tidyverse."
  },
  {
    "objectID": "rbasics/firststeps.html#base-r",
    "href": "rbasics/firststeps.html#base-r",
    "title": "First steps in R",
    "section": "base R",
    "text": "base R\nAfter installing R there are many functions etc. you can use right away - which is what we did above. For example, when running ?mean, the help page will tell you that this function is part of the {base} package. As the name suggests, this package is built-in and its functions are ready to use the moment you have installed R. You can verify this by going to the “Packages” tab in RStudio - you will find the base package and it will have a checked box next to it."
  },
  {
    "objectID": "rbasics/firststeps.html#loading-packages",
    "href": "rbasics/firststeps.html#loading-packages",
    "title": "First steps in R",
    "section": "loading packages",
    "text": "loading packages\nWhen looking at the “Packages” tab in RStudio you may notice that some packages are listed, but do not have a check mark in the box next to them. These are packages that are installed, but not loaded. When a package is not loaded, its functions cannot be used. In order to load a package, the default command is library(package_name). This command must be run once every time you open a new R session."
  },
  {
    "objectID": "rbasics/firststeps.html#installing-additional-packages",
    "href": "rbasics/firststeps.html#installing-additional-packages",
    "title": "First steps in R",
    "section": "installing additional packages",
    "text": "installing additional packages\nR really shines because of the ability to install additional packages from external sources. Basically, anyone can create a function, put it in a package and make it available online. Some packages are very sophisticated and popular - e.g. the package ggplot2, which is not built-in, has been downloaded 75 million times. In order to install a package, the default command is install.packages(package_name). Alternatively, you can also click on the “Install” button in the top left of the “Packages” tab and type in the package_name there.\n\n\n\n\n\n\nNote\n\n\n\nA package only needs to be installed once, but\nA package must be loaded every time you open a new R session!\n\n\nHere is a curated list of R packages and tools for different areas."
  },
  {
    "objectID": "rbasics/tidyverse.html",
    "href": "rbasics/tidyverse.html",
    "title": "The tidyverse",
    "section": "",
    "text": "When using R, you will sooner or later hear about the {tidyverse}. The tidyverse is a collection of R packages that “share an underlying design philosophy, grammar, and data structures” of tidy data. The individual tidyverse packages comprise some of the most downloaded R packages.\nInstall the complete tidyverse with:\nI did not use the tidyverse packages in my first years using R, but I wish I did. While you can often reach your goal with or without using the tidyverse packages, I personally prefer using them. Thus, they are used extensively throughout the chapters of this website.\nDuring the next sections I will try to explain how to use some of these packages and sometimes compare them to the Base R (= non-tidyverse) alternative."
  },
  {
    "objectID": "rbasics/tidyverse.html#data.frame",
    "href": "rbasics/tidyverse.html#data.frame",
    "title": "The tidyverse",
    "section": "data.frame",
    "text": "data.frame\nBase R has a standard format for data tables called data.frame. Here is an example table that is an R built-in, just like pi is - it is called PlantGrowth:\n\nPlantGrowth\n\n   weight group\n1    4.17  ctrl\n2    5.58  ctrl\n3    5.18  ctrl\n4    6.11  ctrl\n5    4.50  ctrl\n6    4.61  ctrl\n7    5.17  ctrl\n8    4.53  ctrl\n9    5.33  ctrl\n10   5.14  ctrl\n11   4.81  trt1\n12   4.17  trt1\n13   4.41  trt1\n14   3.59  trt1\n15   5.87  trt1\n16   3.83  trt1\n17   6.03  trt1\n18   4.89  trt1\n19   4.32  trt1\n20   4.69  trt1\n21   6.31  trt2\n22   5.12  trt2\n23   5.54  trt2\n24   5.50  trt2\n25   5.37  trt2\n26   5.29  trt2\n27   4.92  trt2\n28   6.15  trt2\n29   5.80  trt2\n30   5.26  trt2\n\n\nLet us create a copy of this table called df (dataframe) and then use some helpful functions to get a first impression of this data:\n\ndf <- PlantGrowth\nstr(df)\n\n'data.frame':   30 obs. of  2 variables:\n $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(df)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\n\nWe can see that this dataset has 30 observations (=rows) and 2 variables (=columns) and is of the type “data.frame”. Furthermore, the first variable is called weight and contains numeric values for which we get some measures of central tendency like the minimum, maximum, mean and median. The second variable is called group and is of the type factor containing a total of three different levels, which each appear 10 times.\nIf you want to extract/use values of only one column of such a data.frame, you write the name of the data.frame, then a $ and finally the name of the respective column. It returns the values of that column as a vector:\n\ndf$weight\n\n [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n[16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\n\ndf$group\n\n [1] ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl trt1 trt1 trt1 trt1 trt1\n[16] trt1 trt1 trt1 trt1 trt1 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2\nLevels: ctrl trt1 trt2"
  },
  {
    "objectID": "rbasics/tidyverse.html#tibble",
    "href": "rbasics/tidyverse.html#tibble",
    "title": "The tidyverse",
    "section": "tibble",
    "text": "tibble\nOne major aspect of the tidyverse is formatting tables as tibble instead of data.frame. A tibble “is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not.” It is super simple to convert a data.frame into a tibble, but you must have the tidyverse R package {tibble} installed and loaded - which it is if you are loading the entire {tidyverse}. Let us convert our df into a tibble and call it tbl:\n\npacman::p_load(tidyverse)\ntbl <- as_tibble(df)\ntbl\n\n# A tibble: 30 × 2\n   weight group\n    <dbl> <fct>\n 1   4.17 ctrl \n 2   5.58 ctrl \n 3   5.18 ctrl \n 4   6.11 ctrl \n 5   4.5  ctrl \n 6   4.61 ctrl \n 7   5.17 ctrl \n 8   4.53 ctrl \n 9   5.33 ctrl \n10   5.14 ctrl \n# … with 20 more rows\n\n\nOf course, the data itself does not change - only its format and the way it is displayed to us in R. If you compare the output we get from printing tbl here to that of printing df above, I would like to point out some things I find extremely convenient for tibbles:\n\nThere is an extra first line telling us about the number of rows and columns.\nThere is an extra line below the column names telling us about the data type of each column.\nOnly the first ten rows of data are printed and a “… with 20 more rows” is added below.\nIt can’t be seen here, but this would analogously happen if there were too many columns.\nIt can’t be seen here, but missing values NA and negative numbers are printed in red.\n\nFinally, note that in its heart, a tibble is still a data.frame and in most cases you can do everything with a tibble that you can do with a data.frame:\n\n\n\nclass(tbl)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nstr(tbl)\n\ntibble [30 × 2] (S3: tbl_df/tbl/data.frame)\n $ weight: num [1:30] 4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(tbl)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\ntbl$weight\n\n [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n[16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\n\ntbl$group\n\n [1] ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl trt1 trt1 trt1 trt1 trt1\n[16] trt1 trt1 trt1 trt1 trt1 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2\nLevels: ctrl trt1 trt2\n\n\n\n\n\n\nclass(df)\n\n[1] \"data.frame\"\n\nstr(df)\n\n'data.frame':   30 obs. of  2 variables:\n $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(df)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\ndf$weight\n\n [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n[16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\n\ndf$group\n\n [1] ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl trt1 trt1 trt1 trt1 trt1\n[16] trt1 trt1 trt1 trt1 trt1 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2\nLevels: ctrl trt1 trt2\n\n\n\n\nTherefore, I almost always format my datasets as tibbles."
  },
  {
    "objectID": "rbasics/tidyverse.html#no-pipe---intermediate-steps",
    "href": "rbasics/tidyverse.html#no-pipe---intermediate-steps",
    "title": "The tidyverse",
    "section": "No pipe - intermediate steps",
    "text": "No pipe - intermediate steps\nUsing one function at a time and saving the output in the variables a - d, we can do this:\n\na <- filter(PlantGrowth, group == \"ctrl\")\nb <- pull(a, weight) # same as: b <- a$weight\nc <- sqrt(b)\nd <- round(c, digits = 1)\nsort(d)\n\n [1] 2.0 2.1 2.1 2.1 2.3 2.3 2.3 2.3 2.4 2.5"
  },
  {
    "objectID": "rbasics/tidyverse.html#no-pipe---nesting-functions",
    "href": "rbasics/tidyverse.html#no-pipe---nesting-functions",
    "title": "The tidyverse",
    "section": "No pipe - nesting functions",
    "text": "No pipe - nesting functions\nJust like in MS Excel, it is possible to write functions inside of functions so that we can do this:\n\nsort(round(sqrt(pull(filter(PlantGrowth, group == \"ctrl\"), weight)), digits = 1))\n\n [1] 2.0 2.1 2.1 2.1 2.3 2.3 2.3 2.3 2.4 2.5"
  },
  {
    "objectID": "rbasics/tidyverse.html#pipe",
    "href": "rbasics/tidyverse.html#pipe",
    "title": "The tidyverse",
    "section": "Pipe!",
    "text": "Pipe!\nThis approach (i) allows you to write functions from left to right / top to bottom and thus in the order they are executed and the way you think about them and (ii) does not create extra variables for intermediate steps:\n\nPlantGrowth %>% \n  filter(group == \"ctrl\") %>% \n  pull(weight) %>% \n  sqrt() %>% \n  round(digits = 1) %>% \n  sort()\n\n [1] 2.0 2.1 2.1 2.1 2.3 2.3 2.3 2.3 2.4 2.5\n\n\nYou can think about it like this: Something (in this case the PlantGrowth data.frame) goes into the pipe and is directed to the next function filter(). By default, this function takes what came out of the previous pipe and puts it as its first argument. This happens with every pipe. You’ll notice that all the functions who required two arguments above, now only need one argument, i.e. the additional argument, because the main argument stating which data is to be used is by default simply what came out of the previous pipe. Accordingly, the functions sqrt() and sort() appear empty here, because they only need one piece of information and that is which data they should work with. Finally note that you can easily highlight only some of the lines up until one of the pipes to see the intermediate results.\n\n\n\n\n\n\nNote\n\n\n\nThe keyboard shortcut for writing %>% in RStudio is CTRL+SHIFT+M. Keyboard shortcuts can be customized in RStudio as described here."
  },
  {
    "objectID": "rbasics/tidyverse.html#mutate",
    "href": "rbasics/tidyverse.html#mutate",
    "title": "The tidyverse",
    "section": "mutate()",
    "text": "mutate()\nThis function is useful whenever you want to change existing columns or add new columns to your table. To keep the following examples short and simple, let’s create tbl2 as only the first six rows of tbl via the head() function:\n\ntbl2 <- head(tbl)\ntbl2\n\n# A tibble: 6 × 2\n  weight group\n   <dbl> <fct>\n1   4.17 ctrl \n2   5.58 ctrl \n3   5.18 ctrl \n4   6.11 ctrl \n5   4.5  ctrl \n6   4.61 ctrl \n\n\nLet’s start by adding 2 to the weight in our data. Below, we do this two different ways: by adding a column named new to the dataset (left) and by replacing/overwriting the original weight column (right):\n\n\n\ntbl2 %>% \n  mutate(new = weight + 2)\n\n# A tibble: 6 × 3\n  weight group   new\n   <dbl> <fct> <dbl>\n1   4.17 ctrl   6.17\n2   5.58 ctrl   7.58\n3   5.18 ctrl   7.18\n4   6.11 ctrl   8.11\n5   4.5  ctrl   6.5 \n6   4.61 ctrl   6.61\n\n\n\n\n\n\ntbl2 %>% \n  mutate(weight = weight + 2)\n\n# A tibble: 6 × 2\n  weight group\n   <dbl> <fct>\n1   6.17 ctrl \n2   7.58 ctrl \n3   7.18 ctrl \n4   8.11 ctrl \n5   6.5  ctrl \n6   6.61 ctrl \n\n\n\n\nWe can also create multiple columns at once (left) and make the values of the new column dynamically depend on the other columns via case_when() (right):\n\n\n\ntbl2 %>%\n  mutate(\n    `Name with Space` = \"Hello!\",\n    number10 = 10\n  )\n\n# A tibble: 6 × 4\n  weight group `Name with Space` number10\n   <dbl> <fct> <chr>                <dbl>\n1   4.17 ctrl  Hello!                  10\n2   5.58 ctrl  Hello!                  10\n3   5.18 ctrl  Hello!                  10\n4   6.11 ctrl  Hello!                  10\n5   4.5  ctrl  Hello!                  10\n6   4.61 ctrl  Hello!                  10\n\n\n\n\n\n\ntbl2 %>% \n  mutate(size = case_when(\n    weight > 5.5 ~ \"large\",\n    weight < 4.5 ~ \"small\",\n    TRUE ~ \"normal\" # everything else\n  ))\n\n# A tibble: 6 × 3\n  weight group size  \n   <dbl> <fct> <chr> \n1   4.17 ctrl  small \n2   5.58 ctrl  large \n3   5.18 ctrl  normal\n4   6.11 ctrl  large \n5   4.5  ctrl  normal\n6   4.61 ctrl  normal\n\n\n\n\nFinally, we can efficiently apply the same function to multiple columns at once via across(). We can select the columns e.g. manually via their names in a vector (left) or via a function such as is.numeric (right):\n\n\n\ntbl2 %>%\n  mutate(v1 = 1, v2 = 2, v3 = 3) %>%\n  mutate(\n    across(c(v1, v2), ~ .x + 20)\n    )\n\n# A tibble: 6 × 5\n  weight group    v1    v2    v3\n   <dbl> <fct> <dbl> <dbl> <dbl>\n1   4.17 ctrl     21    22     3\n2   5.58 ctrl     21    22     3\n3   5.18 ctrl     21    22     3\n4   6.11 ctrl     21    22     3\n5   4.5  ctrl     21    22     3\n6   4.61 ctrl     21    22     3\n\n\n\n\n\n\ntbl2 %>%\n  mutate(v1 = 1, v2 = 2, v3 = 3) %>%\n  mutate(\n    across(where(is.numeric), ~ .x + 20)\n    )\n\n# A tibble: 6 × 5\n  weight group    v1    v2    v3\n   <dbl> <fct> <dbl> <dbl> <dbl>\n1   24.2 ctrl     21    22    23\n2   25.6 ctrl     21    22    23\n3   25.2 ctrl     21    22    23\n4   26.1 ctrl     21    22    23\n5   24.5 ctrl     21    22    23\n6   24.6 ctrl     21    22    23\n\n\n\n\n\n\n\n\n\n\nAdditional Resources (click to show)\n\n\n\n\n\n\n\n5.5 Add new variables with mutate() in R for data science (Wickham and Grolemund 2017)\n\nCreate, modify, and delete columns with mutate()\nA general vectorised if with case_when()\nApply a function (or functions) across multiple columns with across()"
  },
  {
    "objectID": "rbasics/tidyverse.html#select",
    "href": "rbasics/tidyverse.html#select",
    "title": "The tidyverse",
    "section": "select()",
    "text": "select()\nThis function is useful whenever you want to select a subset of columns or change the order of columns. To provide better examples, let’s first create a table tbl3 with a few more columns:\n\ntbl3 <- tbl2 %>% \n  mutate(var1 = 1, var2 = 2, var3 = \"text\", var4 = \"word\")\n\ntbl3\n\n# A tibble: 6 × 6\n  weight group  var1  var2 var3  var4 \n   <dbl> <fct> <dbl> <dbl> <chr> <chr>\n1   4.17 ctrl      1     2 text  word \n2   5.58 ctrl      1     2 text  word \n3   5.18 ctrl      1     2 text  word \n4   6.11 ctrl      1     2 text  word \n5   4.5  ctrl      1     2 text  word \n6   4.61 ctrl      1     2 text  word \n\n\nWe can now select individual columns manually by giving all names (left) and even select all columns from:to by writing a colon between them (right):\n\n\n\ntbl3 %>% \n  select(group, var1, var4)\n\n# A tibble: 6 × 3\n  group  var1 var4 \n  <fct> <dbl> <chr>\n1 ctrl      1 word \n2 ctrl      1 word \n3 ctrl      1 word \n4 ctrl      1 word \n5 ctrl      1 word \n6 ctrl      1 word \n\n\n\n\n\n\ntbl3 %>% \n  select(group, var1:var4)\n\n# A tibble: 6 × 5\n  group  var1  var2 var3  var4 \n  <fct> <dbl> <dbl> <chr> <chr>\n1 ctrl      1     2 text  word \n2 ctrl      1     2 text  word \n3 ctrl      1     2 text  word \n4 ctrl      1     2 text  word \n5 ctrl      1     2 text  word \n6 ctrl      1     2 text  word \n\n\n\n\nWe can also delete specific columns by putting a - in fornt of their name or use functions like starts_with(), ends_with(), contains(), matches() and num_range() to select all columns based on (parts of) their name:\n\n\n\ntbl3 %>% \n  select(-group)\n\n# A tibble: 6 × 5\n  weight  var1  var2 var3  var4 \n   <dbl> <dbl> <dbl> <chr> <chr>\n1   4.17     1     2 text  word \n2   5.58     1     2 text  word \n3   5.18     1     2 text  word \n4   6.11     1     2 text  word \n5   4.5      1     2 text  word \n6   4.61     1     2 text  word \n\n\n\n\n\n\ntbl3 %>% \n  select(contains(\"r\"))\n\n# A tibble: 6 × 5\n  group  var1  var2 var3  var4 \n  <fct> <dbl> <dbl> <chr> <chr>\n1 ctrl      1     2 text  word \n2 ctrl      1     2 text  word \n3 ctrl      1     2 text  word \n4 ctrl      1     2 text  word \n5 ctrl      1     2 text  word \n6 ctrl      1     2 text  word \n\n\n\n\nFinally, we can select based on a function like is.numeric via where() (left) or simply rearrange while keeping all columns by using everything() (right)\n\n\n\ntbl3 %>% \n  select(where(is.numeric))\n\n# A tibble: 6 × 3\n  weight  var1  var2\n   <dbl> <dbl> <dbl>\n1   4.17     1     2\n2   5.58     1     2\n3   5.18     1     2\n4   6.11     1     2\n5   4.5      1     2\n6   4.61     1     2\n\n\n\n\n\n\ntbl3 %>% \n  select(var1, everything())\n\n# A tibble: 6 × 6\n   var1 weight group  var2 var3  var4 \n  <dbl>  <dbl> <fct> <dbl> <chr> <chr>\n1     1   4.17 ctrl      2 text  word \n2     1   5.58 ctrl      2 text  word \n3     1   5.18 ctrl      2 text  word \n4     1   6.11 ctrl      2 text  word \n5     1   4.5  ctrl      2 text  word \n6     1   4.61 ctrl      2 text  word \n\n\n\n\n\n\n\n\n\n\nAdditional Resources (click to show)\n\n\n\n\n\n\n\n5.4 Select columns with select() in R for data science (Wickham and Grolemund 2017)\n\nSubset columns using their names and types with select()\nSelect variables that match a pattern with starts_with() etc.\nSelect variables with a function with where()"
  },
  {
    "objectID": "rbasics/tidyverse.html#filter",
    "href": "rbasics/tidyverse.html#filter",
    "title": "The tidyverse",
    "section": "filter()",
    "text": "filter()\nThis function is useful whenever you want to subset rows based on their values. Note that for the examples here, we use the original tbl with 30 observations.\nLet’s immediately filter for two conditions: Observations that belong to group trt2 and (&) are larger than 6 (left); Observations that are larger than 6 or (|) smaller than 4 (right):\n\n\n\ntbl %>% \n  filter(weight > 6 & group == \"trt2\")\n\n# A tibble: 2 × 2\n  weight group\n   <dbl> <fct>\n1   6.31 trt2 \n2   6.15 trt2 \n\n\n\n\n\n\ntbl %>% \n  filter(weight > 6 | weight < 4)\n\n# A tibble: 6 × 2\n  weight group\n   <dbl> <fct>\n1   6.11 ctrl \n2   3.59 trt1 \n3   3.83 trt1 \n4   6.03 trt1 \n5   6.31 trt2 \n6   6.15 trt2 \n\n\n\n\nInstead of writing a lot of conditions separated by |, it is often more efficient to use %in%:\n\n\n\ntbl %>% \n  filter(group == \"trt1\" | group == \"trt2\")\n\n# A tibble: 20 × 2\n   weight group\n    <dbl> <fct>\n 1   4.81 trt1 \n 2   4.17 trt1 \n 3   4.41 trt1 \n 4   3.59 trt1 \n 5   5.87 trt1 \n 6   3.83 trt1 \n 7   6.03 trt1 \n 8   4.89 trt1 \n 9   4.32 trt1 \n10   4.69 trt1 \n11   6.31 trt2 \n12   5.12 trt2 \n13   5.54 trt2 \n14   5.5  trt2 \n15   5.37 trt2 \n16   5.29 trt2 \n17   4.92 trt2 \n18   6.15 trt2 \n19   5.8  trt2 \n20   5.26 trt2 \n\n\n\n\n\n\ntbl %>% \n  filter(group %in% c(\"trt1\", \"trt2\"))\n\n# A tibble: 20 × 2\n   weight group\n    <dbl> <fct>\n 1   4.81 trt1 \n 2   4.17 trt1 \n 3   4.41 trt1 \n 4   3.59 trt1 \n 5   5.87 trt1 \n 6   3.83 trt1 \n 7   6.03 trt1 \n 8   4.89 trt1 \n 9   4.32 trt1 \n10   4.69 trt1 \n11   6.31 trt2 \n12   5.12 trt2 \n13   5.54 trt2 \n14   5.5  trt2 \n15   5.37 trt2 \n16   5.29 trt2 \n17   4.92 trt2 \n18   6.15 trt2 \n19   5.8  trt2 \n20   5.26 trt2 \n\n\n\n\nWe can also filter for values that are not of the ctrl group (left) or that are larger than the mean weight (right):\n\n\n\ntbl %>% \n  filter(group != \"ctrl\")\n\n# A tibble: 20 × 2\n   weight group\n    <dbl> <fct>\n 1   4.81 trt1 \n 2   4.17 trt1 \n 3   4.41 trt1 \n 4   3.59 trt1 \n 5   5.87 trt1 \n 6   3.83 trt1 \n 7   6.03 trt1 \n 8   4.89 trt1 \n 9   4.32 trt1 \n10   4.69 trt1 \n11   6.31 trt2 \n12   5.12 trt2 \n13   5.54 trt2 \n14   5.5  trt2 \n15   5.37 trt2 \n16   5.29 trt2 \n17   4.92 trt2 \n18   6.15 trt2 \n19   5.8  trt2 \n20   5.26 trt2 \n\n\n\n\n\n\ntbl %>% \n  filter(weight > mean(weight))\n\n# A tibble: 17 × 2\n   weight group\n    <dbl> <fct>\n 1   5.58 ctrl \n 2   5.18 ctrl \n 3   6.11 ctrl \n 4   5.17 ctrl \n 5   5.33 ctrl \n 6   5.14 ctrl \n 7   5.87 trt1 \n 8   6.03 trt1 \n 9   6.31 trt2 \n10   5.12 trt2 \n11   5.54 trt2 \n12   5.5  trt2 \n13   5.37 trt2 \n14   5.29 trt2 \n15   6.15 trt2 \n16   5.8  trt2 \n17   5.26 trt2 \n\n\n\n\n\n\n\n\n\n\nAdditional Resources (click to show)\n\n\n\n\n\n\n\n5.2 Filter rows with filter() in R for data science (Wickham and Grolemund 2017)\n\nSubset rows using column values with filter()"
  },
  {
    "objectID": "rbasics/tidyverse.html#arrange",
    "href": "rbasics/tidyverse.html#arrange",
    "title": "The tidyverse",
    "section": "arrange()",
    "text": "arrange()\nThis function is useful whenever you want to sort rows based on their values. We’ll once more create a new version of our original dataset to best show what this function can do:\n\ntbl4 <- PlantGrowth %>%\n  slice(1:3, 11:13, 21:23) \n# this keep only rows 1,2,3,11,12,13,21,22,23\n\nWe can arrange rows via writing the column name (or column index/number). Note that by default values are sorted in ascending order and strings are sorted alphabetically, but this can be reversed by using desc():\n\n\n\ntbl4 %>% \n  arrange(weight)\n\n  weight group\n1   4.17  ctrl\n2   4.17  trt1\n3   4.41  trt1\n4   4.81  trt1\n5   5.12  trt2\n6   5.18  ctrl\n7   5.54  trt2\n8   5.58  ctrl\n9   6.31  trt2\n\n\n\n\n\n\ntbl4 %>% \n  arrange(desc(weight))\n\n  weight group\n1   6.31  trt2\n2   5.58  ctrl\n3   5.54  trt2\n4   5.18  ctrl\n5   5.12  trt2\n6   4.81  trt1\n7   4.41  trt1\n8   4.17  ctrl\n9   4.17  trt1\n\n\n\n\nYou can also sort via multiple columns and you can provide a custom sorting order in a vector:\n\n\n\ntbl4 %>% \n  arrange(group, weight)\n\n  weight group\n1   4.17  ctrl\n2   5.18  ctrl\n3   5.58  ctrl\n4   4.17  trt1\n5   4.41  trt1\n6   4.81  trt1\n7   5.12  trt2\n8   5.54  trt2\n9   6.31  trt2\n\n\n\n\n\n\nmyorder <- c(\"trt1\", \"ctrl\", \"trt2\")\n\ntbl4 %>% \n  arrange(\n    match(group, myorder), \n    weight\n  )\n\n  weight group\n1   4.17  trt1\n2   4.41  trt1\n3   4.81  trt1\n4   4.17  ctrl\n5   5.18  ctrl\n6   5.58  ctrl\n7   5.12  trt2\n8   5.54  trt2\n9   6.31  trt2\n\n\n\n\nNote that NA (= missing values) are always sorted to the end3, even when using desc().\n\n\n\n\n\n\nAdditional Resources (click to show)\n\n\n\n\n\n\n\n5.3 Arrange rows with arrange() in R for data science (Wickham and Grolemund 2017)\n\nArrange rows by column values with arrange()\nHow to have NA’s displayed first using arrange()"
  },
  {
    "objectID": "rbasics/tidyverse.html#summarise",
    "href": "rbasics/tidyverse.html#summarise",
    "title": "The tidyverse",
    "section": "summarise()",
    "text": "summarise()\nThis function can be useful whenever you want to summarise data. Yet, it is not very useful (left) unless it is paired with group_by() (right).\n\n\n\ntbl %>% \n  # no group_by \n  summarise(my_mean = mean(weight))\n\n# A tibble: 1 × 1\n  my_mean\n    <dbl>\n1    5.07\n\n\n\n\n\n\ntbl %>% \n  group_by(group) %>% \n  summarise(my_mean = mean(weight))\n\n# A tibble: 3 × 2\n  group my_mean\n  <fct>   <dbl>\n1 ctrl     5.03\n2 trt1     4.66\n3 trt2     5.53\n\n\n\n\nYou can create multiple summary output columns (left) and have multiple grouping columns (right):\n\n\n\ntbl %>% \n  group_by(group) %>% \n  summarise(\n    Mean = mean(weight),\n    StdDev = sd(weight),\n    Min = min(weight),\n    Median = median(weight),\n    Max = max(weight),\n    n_Obs = n(),\n  )\n\n# A tibble: 3 × 7\n  group  Mean StdDev   Min Median   Max n_Obs\n  <fct> <dbl>  <dbl> <dbl>  <dbl> <dbl> <int>\n1 ctrl   5.03  0.583  4.17   5.15  6.11    10\n2 trt1   4.66  0.794  3.59   4.55  6.03    10\n3 trt2   5.53  0.443  4.92   5.44  6.31    10\n\n\n\n\n\n\ntbl %>% \n  mutate(larger5 = case_when(\n    weight > 5 ~ \"yes\",\n    weight < 5 ~ \"no\"\n  )) %>% \n  group_by(group, larger5) %>% \n  summarise(\n    n_Obs = n(),\n    Mean = mean(weight)\n  )\n\n# A tibble: 6 × 4\n# Groups:   group [3]\n  group larger5 n_Obs  Mean\n  <fct> <chr>   <int> <dbl>\n1 ctrl  no          4  4.45\n2 ctrl  yes         6  5.42\n3 trt1  no          8  4.34\n4 trt1  yes         2  5.95\n5 trt2  no          1  4.92\n6 trt2  yes         9  5.59\n\n\n\n\nJust like with mutate(), we can make use of across() to deal with multiple columns:\n\n\n\ntbl %>%\n  mutate(v1 = 1, v2 = 2, v3 = 3) %>%\n  group_by(group) %>%\n  summarise(across(\n    where(is.numeric), \n    ~ mean(.x)\n    ))\n\n# A tibble: 3 × 5\n  group weight    v1    v2    v3\n  <fct>  <dbl> <dbl> <dbl> <dbl>\n1 ctrl    5.03     1     2     3\n2 trt1    4.66     1     2     3\n3 trt2    5.53     1     2     3\n\n\n\n\n\n\ntbl %>%\n  mutate(v1 = 1, v2 = 2, v3 = 3) %>%\n  group_by(group) %>%\n  summarise(across(\n    c(weight, v3),\n    list(\n    Min = ~ min(.x),\n    Max = ~ max(.x)\n    )\n  ))\n\n# A tibble: 3 × 5\n  group weight_Min weight_Max v3_Min v3_Max\n  <fct>      <dbl>      <dbl>  <dbl>  <dbl>\n1 ctrl        4.17       6.11      3      3\n2 trt1        3.59       6.03      3      3\n3 trt2        4.92       6.31      3      3\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOnce you used group_by() on a table, it stays grouped unless you use ungroup() on it afterwards. This was not relevant in the examples above, but you must be aware of this if you are using the grouped (summary) results for further steps, since this can lead to unexpected results. You can find an example and further resources on such unintended outcomes here.\n\n\n\n\n\n\n\n\nAdditional Resources (click to show)\n\n\n\n\n\n\n\n5.6 Grouped summaries with summarise() in R for data science (Wickham and Grolemund 2017)\n\nSummarise each group to fewer rows with summarise()\nGroup by one or more variables with group_by()"
  },
  {
    "objectID": "summaryarticles/compactletterdisplay.html#get-the-letters",
    "href": "summaryarticles/compactletterdisplay.html#get-the-letters",
    "title": "Compact Letter Display (CLD)",
    "section": "get the letters",
    "text": "get the letters\nYou will need to install the packages emmeans, multcomp and {multcompView}. The example given here is based on the PlantGrowth data, which is included in R.\n\nlibrary(emmeans)\nlibrary(multcomp)\nlibrary(multcompView)\n\n# set up model\nmodel <- lm(weight ~ group, data = PlantGrowth)\n\n# get (adjusted) weight means per group\nmodel_means <- emmeans(object = model,\n                       specs = \"group\")\n\n# add letters to each mean\nmodel_means_cld <- cld(object = model_means,\n                       adjust = \"Tukey\",\n                       Letters = letters,\n                       alpha = 0.05)\n# show output\nmodel_means_cld\n\n\n\n group emmean    SE df lower.CL upper.CL .group\n trt1    4.66 0.197 27     4.16     5.16  a    \n ctrl    5.03 0.197 27     4.53     5.53  ab   \n trt2    5.53 0.197 27     5.02     6.03   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 3 estimates \nP value adjustment: tukey method for comparing a family of 3 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping letter,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\nWe set up a model\n\nThis is a very simple example using lm(). You may use much more complex models and many other model classes.\n\n\n\nemmeans() estimates adjusted means per group.\n\nNote that when doing this for mixed models, one should use the Kenward-Roger method adjusting the denominator degrees of freedom. One may add the lmer.df = \"kenward-roger\" argument, yet this is the default in {emmeans} (Details here)! Also note that you cannot go wrong with this adjustment - even if there is nothing to adjust.\n\n\n\ncld() adds the letters in a new column named .group.\n\nThe alpha = argument lets you choose the significance level for the comparisons.\nIt allows for different multiplicity adjustments. Go to the “P-value adjustments” heading within the “summary.emmGrid” section in the emmeans documentation for more details on e.g. t-test, Tukey-test, Bonferroni adjustment etc.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you get the following note and are irritated by it,\n\n## Note: adjust = \"tukey\" was changed to \"sidak\" \n## because \"tukey\" is only appropriate for one set of pairwise comparisons\n## Conf-level adjustment: sidak method for 3 estimates.\n## P value adjustment: tukey method for comparing a family of 3 estimates\n\nhere is an answer explaining why this happens and that it is not a problem. It is not a problem in the sense that the p-values of the pairwise comparisons were indeed adjusted with the Tukey-method, while the Sidak adjustment was applied to the confidence intervals of the means (i.e. columns lower.CL and upper.CL)."
  },
  {
    "objectID": "summaryarticles/compactletterdisplay.html#interpret-the-letters",
    "href": "summaryarticles/compactletterdisplay.html#interpret-the-letters",
    "title": "Compact Letter Display (CLD)",
    "section": "interpret the letters",
    "text": "interpret the letters\nUntil August 2022, the note below the cld() outcome would read:\n\n## NOTE: Compact letter displays can be misleading\n##       because they show NON-findings rather than findings.\n##       Consider using 'pairs()', 'pwpp()', or 'pwpm()' instead.\n\nHowever, in CRAN version 1.8.0 it was changed to:\n\n## NOTE: If two or more means share the same grouping letter,\n##       then we cannot show them to be different.\n##       But we also did not show them to be the same.\n\nBoth notes are very much in line with the delicate matter of how the CLD must be understood. The author and maintainer of the emmeans package, Russell V. Lenth makes the argument that CLDs convey information in a way that may be misleading to the reader. This is because they “display non-findings rather than findings - they group together means based on NOT being able to show they are different” (personal communication). Furthermore, “[the CLD approach] works, but it is very black-and-white: with alpha = .05, P values slightly above or below .05 make a difference, but there’s no difference between a P value of .051 and one of .987, or between .049 and .00001” (posted here). He even wrote here that “Providing for CLDs at all remains one of my biggest regrets in developing this package”. Finally, the former note suggests using alternative plots, which are also created below.\nOn the other hand, it must be clear that the information conveyed by CLDs is not wrong as long as it is interpreted correctly. The documentation of the cld() function refers to Piepho (2004), but even more on point in this context is Piepho (2018):\n\nPiepho, Hans-Peter (2018) Letters in Mean Comparisons: What They Do and Don’t Mean, Agronomy Journal, 110(2), 431-434. DOI: 10.2134/agronj2017.10.0580 (ResearchGate)\nAbstract\n\nLetter displays allow efficient reporting of pairwise treatment comparisons.\nIt is important to correctly convey the meaning of letters in captions to tables and graphs displaying treatment means.\nThe meaning of a letter display can and should be stated in a single sentence without ambiguity.\n\nLetter displays are often used to report results of all pairwise comparisons among treatment means in comparative experiments. In captions to tables and charts using such letter displays, it is crucial to explain properly what the letters mean. In this paper I explain what the letters mean and how this meaning can be succinctly conveyed in a single sentence without ambiguity. This is contrasted to counter-examples commonly found in publications.\n\nThus, the article (= 4 pages long) is certainly worth a read if you are using CLDs."
  },
  {
    "objectID": "summaryarticles/compactletterdisplay.html#get-the-plots",
    "href": "summaryarticles/compactletterdisplay.html#get-the-plots",
    "title": "Compact Letter Display (CLD)",
    "section": "get the plots",
    "text": "get the plots\nHere I provide code for two ways of plotting the results via ggplot2. The first plot is the one I would use, while the second plot is one that is traditionally more common. Finally, I provide examples of other plots that I came across that are suggested as alternatives to CLD plots.\nplot 1: suggested\nI’ve been using and suggesting to use this type of plot for a while now. I know it contains a lot of information and may seem unfamiliar and overwhelming at first glance. However, I argue that if you take the time to understand what you are looking at, this plot is nice as it shows the raw data (black dots), descriptive statistics (black boxes), estimated means (red dots) and a measure of their precision (red error bars) as well as the compact letter display (red letters).\n\nClick to show/hide codelibrary(ggtext)    # automatic line breaks in caption\nlibrary(tidyverse) # ggplot & helper functions\nlibrary(scales)    # more helper functions\n\n# optional: sort factor levels of groups column according to highest mean\n# ...in means table\nmodel_means_cld <- model_means_cld %>% \n  mutate(group = fct_reorder(group, emmean))\n# ...in data table\nPlantGrowth <- PlantGrowth %>% \n  mutate(group = fct_relevel(group, levels(model_means_cld$group)))\n\n# plot\nggplot() +\n  # y-axis\n  scale_y_continuous(\n    name = \"Weight\",\n    limits = c(0, NA),\n    breaks = pretty_breaks(),\n    expand = expansion(mult = c(0,0.1))\n  ) +\n  # x-axis\n  scale_x_discrete(\n    name = \"Treatment\"\n  ) +\n  # general layout\n  theme_classic() +\n  theme(plot.caption = element_textbox_simple()) +\n  # black data points\n  geom_point(\n    data = PlantGrowth,\n    aes(y = weight, x = group),\n    shape = 16,\n    alpha = 0.5,\n    position = position_nudge(x = -0.2)\n  ) +\n  # black boxplot\n  geom_boxplot(\n    data = PlantGrowth,\n    aes(y = weight, x = group),\n    width = 0.05,\n    outlier.shape = NA,\n    position = position_nudge(x = -0.1)\n  ) +\n  # red mean value\n  geom_point(\n    data = model_means_cld,\n    aes(y = emmean, x = group),\n    size = 2,\n    color = \"red\"\n  ) +\n  # red mean errorbar\n  geom_errorbar(\n    data = model_means_cld,\n    aes(ymin = lower.CL, ymax = upper.CL, x = group),\n    width = 0.05,\n    color = \"red\"\n  ) +\n  # red letters\n  geom_text(\n    data = model_means_cld,\n    aes(\n      y = emmean,\n      x = group,\n      label = str_trim(.group)\n    ),\n    position = position_nudge(x = 0.1),\n    hjust = 0,\n    color = \"red\"\n  ) +\n  # caption\n  labs(\n    caption = \"Black dots represent raw data. Red dots and error bars represent (estimated marginal) means ± 95% confidence interval per group. Means not sharing any letter are significantly different by the Tukey-test at the 5% level of significance.\"\n  )\n\n\n\n\nplot 2: well-known\nTraditionally, bar plots with error bars are used a lot in this context. In my experience, there is at least one poster with one of them in every university building I. While they are not wrong per se, there is a decade-long discussion about why such “dynamite plots” are not optimal (see e.g. this nice blogpost).\n\nClick to show/hide codelibrary(ggtext)    # automatic line breaks in caption\nlibrary(tidyverse) # ggplot & helper functions\nlibrary(scales)    # more helper functions\n\n# optional: sort factor levels of groups column according to highest mean\n# ...in means table\nmodel_means_cld <- model_means_cld %>% \n  mutate(group = fct_reorder(group, emmean))\n# ...in data table\nPlantGrowth <- PlantGrowth %>% \n  mutate(group = fct_relevel(group, levels(model_means_cld$group)))\n\n# plot\nggplot() +\n  # y-axis\n  scale_y_continuous(\n    name = \"Weight\",\n    limits = c(0, NA),\n    breaks = pretty_breaks(),\n    expand = expansion(mult = c(0,0.1))\n  ) +\n  # x-axis\n  scale_x_discrete(\n    name = \"Treatment\"\n  ) +\n  # general layout\n  theme_classic() +\n  theme(plot.caption = element_textbox_simple()) +\n  # bars\n  geom_bar(data = model_means_cld,\n           aes(y = emmean, x = group),\n           stat = \"identity\") +\n  # errorbars\n  geom_errorbar(data = model_means_cld,\n                aes(\n                  ymin = emmean - SE,\n                  ymax = emmean + SE,\n                  x = group\n                ),\n                width = 0.1) +\n  # letters\n  geom_text(\n    data = model_means_cld,\n    aes(\n      y = emmean + SE,\n      x = group,\n      label = str_trim(.group)\n    ),\n    hjust = 0.5,\n    vjust = -0.5\n  ) +\n  # caption\n  labs(\n    caption = \"Bars with errorbars represent (estimated marginal) means ± standard error. Means not sharing any letter are significantly different by the Tukey-test at the 5% level of significance.\"\n  )"
  },
  {
    "objectID": "summaryarticles/compactletterdisplay.html#alternative-plots",
    "href": "summaryarticles/compactletterdisplay.html#alternative-plots",
    "title": "Compact Letter Display (CLD)",
    "section": "Alternative plots",
    "text": "Alternative plots\nNote that I simply collect alternative ways of plotting adjusted mean comparisons here - this does not mean I fully grasp their concept.\nalt 1: Pairwise P-value plot {emmeans}\nThis is the Pairwise P-value plot suggested in the former NOTE we received above as an alternative. The documentation reads: Factor levels (or combinations thereof) are plotted on the vertical scale, and P values are plotted on the horizontal scale. Each P value is plotted twice – at vertical positions corresponding to the levels being compared – and connected by a line segment. Thus, it is easy to visualize which P values are small and large, and which levels are compared.\n\nClick to show/hide codepwpp(model_means) + theme_bw()\n\n\n\n\nalt 2: Lighthouse plot {easystats}\nWithin the framework of the easystats packages, the lighthouse plots came up as a more recent idea. See this issue and this and this part of the documentation for more details.\n\nClick to show/hide codelibrary(modelbased)\nlibrary(see)\nplot(estimate_contrasts(model, adjust = \"tukey\"),\n     estimate_means(model)) +\n  theme_classic()\n\n\n\n\nalt 3: The {ggbetweenstats} plot\nFinally, the ggstatsplot package’s function ggbetweenstats() aims to create graphics with details from statistical tests included in the information-rich plots themselves and would compare our groups like this:\n\nClick to show/hide codelibrary(PMCMRplus)\nlibrary(rstantools)\nlibrary(ggstatsplot)\n# \"since the confidence intervals for the effect sizes are computed using\n# bootstrapping, important to set a seed for reproducibility\"\nset.seed(42)\nggstatsplot::ggbetweenstats(\n  data = PlantGrowth,\n  x = group,\n  y = weight,\n  pairwise.comparisons = TRUE,\n  pairwise.display = \"all\",\n  p.adjust.method = \"none\"\n)"
  },
  {
    "objectID": "summaryarticles/modeldiagnostics.html",
    "href": "summaryarticles/modeldiagnostics.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Most statistical methods and all statistical models make certain assumptions (about the data generating process), and (test) results will be meaningless or misleading if theses assumptions do not hold. Therefore, model diagnostics should be used to check how well the assumptions of any given model are met."
  },
  {
    "objectID": "summaryarticles/modeldiagnostics.html#independence",
    "href": "summaryarticles/modeldiagnostics.html#independence",
    "title": "Model Diagnostics",
    "section": "Independence",
    "text": "Independence\nAssumption: Individual observations are independent of each other (as opposed to dependent/correlated).\nModel diagnostics are actually not used to verify this specific assumption. Instead, this assumption is justified if proper randomization was applied as part of the experimental design. Keep in mind, that there are indeed non-classical scenarios like repeated measures over time or certain experimental designs where observations are not (assumed to be) independent, but these are not covered in this chapter."
  },
  {
    "objectID": "summaryarticles/modeldiagnostics.html#normality",
    "href": "summaryarticles/modeldiagnostics.html#normality",
    "title": "Model Diagnostics",
    "section": "Normality",
    "text": "Normality\nAssumption: The errors follow a normal distribution.\nA model’s errors cannot be directly observed, but are estimated by their residuals; these residuals can be used for checking normality.\n\n\n\n\n\n\nYes residuals - not data!\n\n\n\nUnfortunately, it is more common than it should be that people check whether their raw data (i.e. their response variable, e.g. yield) is normally distributed. This is not constructive. Instead, the model’s residuals should be checked for normality. Please see section “4 | ANSWERING QUESTION 1” in Kozak and Piepho (2018) for details.\n\n\nQQ plot\nApproximate normality can be assumed, if the dots in a QQ plot look close to a straight line. Kozak and Piepho (2018) point out that when it comes to checking normality, we need to be clear that we will never be able to check whether indeed the distribution is fully normal; instead, we should check whether it is approximately normal.\n\nmod %>% \n  check_normality() %>% \n  plot(type = \"qq\")\n\n\n\n\nTests\nAs explained further below, I do not embrace using statistical tests to check assumptions, but instead suggest using the diagnostic plot above. However, for completeness I still provide the following information:\nThere are multiple statistical tests for detecting a violation of the normality assumption. A p-value < 0.05 would suggest such a violation:\n\nols_test_normality(mod)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.9661         0.4379 \nKolmogorov-Smirnov        0.1101         0.8215 \nCramer-von Mises          3.6109         0.0000 \nAnderson-Darling          0.3582         0.4299 \n-----------------------------------------------"
  },
  {
    "objectID": "summaryarticles/modeldiagnostics.html#variance-homogeneity",
    "href": "summaryarticles/modeldiagnostics.html#variance-homogeneity",
    "title": "Model Diagnostics",
    "section": "Variance homogeneity",
    "text": "Variance homogeneity\nAssumption: The error variance is the same at any set of predictor values.\nAlso referred to as homoscedasticity (i.e. the opposite of heteroscedasticity)\nRes-Pred Plot\nWhile this plot does not seem to have an established name, it always has raw/standardized/studentized residuals on the y axis and fitted/predicted values on the x axis. Variance homogeneity can be assumed if the residuals form an approximate horizontal band around the 0 line indicating homogeneity of error variance.\n\nmod %>% \n  check_heteroscedasticity() %>% \n  plot()\n\n\n\n\nTests\nAs explained further below, I do not embrace using statistical tests to check assumptions, but instead suggest using the diagnostic plot above. However, for completeness I still provide the following information:\nThere are multiple statistical tests for detecting a violation of the variance homogeneity assumption. A p-value < 0.05 would suggest such a violation:\n\n\n\nols_test_breusch_pagan(mod)\n\n\n Breusch Pagan Test for Heteroskedasticity\n -----------------------------------------\n Ho: the variance is constant            \n Ha: the variance is not constant        \n\n               Data                \n ----------------------------------\n Response : weight \n Variables: fitted values of weight \n\n        Test Summary          \n -----------------------------\n DF            =    1 \n Chi2          =    3.000303 \n Prob > Chi2   =    0.08324896 \n\n\n\n\n\n\nols_test_bartlett(\n  data = PlantGrowth, \n  \"weight\", \n  group_var = \"group\")\n\n\n    Bartlett's Test of Homogenity of Variances    \n------------------------------------------------\nHo: Variances are equal across groups\nHa: Variances are unequal for atleast two groups\n\n        Test Summary         \n ----------------------------\n DF            =    2 \n Chi2          =    2.878592 \n Prob > Chi2   =    0.2370946"
  },
  {
    "objectID": "summaryarticles/modeldiagnostics.html#linearity",
    "href": "summaryarticles/modeldiagnostics.html#linearity",
    "title": "Model Diagnostics",
    "section": "Linearity",
    "text": "Linearity\nAssumption: The response can be written as a linear combination of the predictors.\nThis assumption can also be checked via the Res-Pred-plot from the Variance homogeneity section above. It can assumed to be met if the residuals spread randomly around the 0 line. In other words: At any fitted value, the mean of the residuals should be roughly 0. If this is the case, the linearity assumption is valid. I am not aware of any statistical tests for linearity."
  },
  {
    "objectID": "summaryarticles/modeldiagnostics.html#data-transformation",
    "href": "summaryarticles/modeldiagnostics.html#data-transformation",
    "title": "Model Diagnostics",
    "section": "Data transformation",
    "text": "Data transformation\nTransforming the response variable (e.g. yield) with a mathematical function (e.g. square root) can actually solve your problem. If the diagnostics of a linear model improve noticeably by replacing yield with sqrt(yield), you can simply use the latter to conduct an ANOVA and draw conclusions from it. Moreover, you can also compare means via post hoc tests. What follows is an example1 where this approach does as intended.\n\nClick to show/hide codepacman::p_load(agridat,\n               easystats,\n               emmeans,\n               multcomp,\n               multcompView,\n               tidyverse)\n\ndat <- agridat::bridges.cucumber %>%\n  filter(loc == \"Clemson\") %>%\n  mutate(colF = as.factor(col),\n         rowF = as.factor(row))\n\n\n\n\n\nmod1 <- lm(\n  yield ~ gen + rowF + colF, \n  data = dat)\n\nmod1 %>% \n  check_normality() %>% \n  plot(type = \"qq\")\n\n\n\n\n\n\n\n\nmod2 <- lm(\n  sqrt(yield) ~ gen + rowF + colF, \n  data = dat)\n\nmod2 %>% \n  check_normality() %>% \n  plot(type = \"qq\")\n\n\n\n\n\n\nYou can see that the QQ plot for mod2 does indeed look better than that for mod1. Thus, we can and should conduct the ANOVA for mod2. Its interpretation or rather the conclusions drawn from it are essentially the same - irrespective of whether yield or the square root of yield was modeled: There are significant differences between the genotypes.\n\nanova(mod2)\n\nAnalysis of Variance Table\n\nResponse: sqrt(yield)\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \ngen        3 10.5123  3.5041  8.8966 0.01256 *\nrowF       3  5.0283  1.6761  4.2555 0.06228 .\ncolF       3  4.2121  1.4040  3.5647 0.08670 .\nResiduals  6  2.3632  0.3939                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt is a little bit different when it comes to mean comparisons via post hoc tests. Not because of the conclusion part - this is the same here, too: e.g. two genotype means may be significantly different from each other according to the Tukey test. However, the mean itself can be irritating, since noone is used to dealing with square roots of yields. However, it is valid to present the means on the backtransformed (= original) scale, as long as it is made clear to the reader that the model fit and mean comparisons were made on a different (= square root) scale.\n\nmod2 %>% \n  emmeans(specs = ~ gen, type = \"response\") %>% \n  cld(Letters = letters)\n\n gen      response   SE df lower.CL upper.CL .group\n Poinsett     20.9 2.87  6     14.4     28.5  a    \n Sprint       25.1 3.14  6     18.0     33.3  a    \n Guardian     30.4 3.46  6     22.5     39.5  ab   \n Dasher       45.3 4.23  6     35.6     56.3   b   \n\nResults are averaged over the levels of: rowF, colF \nConfidence level used: 0.95 \nIntervals are back-transformed from the sqrt scale \nNote: contrasts are still on the sqrt scale \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping letter,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nNote that type = \"response\" can do the back-transformation for us automatically2 and specifically see the part of the message below the mean table that reads “Intervals are back-transformed from the sqrt scale. Note: contrasts are still on the sqrt scale”."
  },
  {
    "objectID": "summaryarticles/modeldiagnostics.html#generalized-linear-models",
    "href": "summaryarticles/modeldiagnostics.html#generalized-linear-models",
    "title": "Model Diagnostics",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nTO DO\n\n\n\n\n\n\nAdditional Resources (click to show)\n\n\n\n\n\nGeneral\n\nWhat’s normal anyway? Residual plots are more telling than significance tests when checking ANOVA assumptions (Kozak and Piepho 2018)\n\n\nChapter 13 Model Diagnostics in Applied Statistics with R (Dalpiaz, 2022)\n\nChapter 8 Model Diagnostics in Course Handouts for Bayesian Data Analysis (Lai, 2019)\n🇩🇪 Verbleibende Plots interpretieren, um Ihre Regression zu verbessern\n\n{olsrr}\n\nNormality\n\nR Tutorial: Normal Probability Plot of Residuals\nFor this specific purpose, QQ plots may also be called Normal probability plots\n\n{qqplotr}\n\nHomoscedasticity\n\nDocumentation on tests from {olsrr}\nWikipedia article on Homoscedasticity and heteroscedasticity\n\n\nTransformation\n\n🇩🇪 Chapter 3.3 in Dormann and Kühn (2011)"
  },
  {
    "objectID": "summaryarticles/usefulthings.html",
    "href": "summaryarticles/usefulthings.html",
    "title": "Useful things",
    "section": "",
    "text": "This chapter is a collection of things I wish I had known earlier in my years using R and that I hope can be of use to you. Sections are named after R packages or whatever applies and sorted alphabetically.\n{broom}\nIn R, results from statistical tests, models etc. are often formatted in a way that may not be optimal for further processing steps. Luckily, {broom} will format the results of the most common functions into tidy data structures.\n\n# Correlation Analysis for built-in example data \"mtcars\"\nmycor <- cor.test(mtcars$mpg, mtcars$disp)\nmycor\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$disp\nt = -8.7472, df = 30, p-value = 9.38e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9233594 -0.7081376\nsample estimates:\n       cor \n-0.8475514 \n\nlibrary(broom)\ntidy(mycor)\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method        alter…¹\n     <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>         <chr>  \n1   -0.848     -8.75 9.38e-10        30   -0.923    -0.708 Pearson's pr… two.si…\n# … with abbreviated variable name ¹​alternative\n\n\n{conflicted}\nSometimes, different packages have different functions with identical names. A famous example is the function filter(), which exists in {stats} and {dplyr}. If both of these packages are loaded, it is not clear which of the two functions should be used.\nTO DO\n{desplot}\n{desplot} makes it easy to plot experimental designs of field trials in agriculture. However, you do need two columns that provide the x and y coordinates of the individual plots on your field.\nTO DO\n{dlookr}\nWhen providing descriptive statistics tables, one may find the number of relevant measures become annoyingly large so that even with the {tidyverse}, several lines of code are necessary. Here are just five measures, and they are not even including the na.rm = TRUE argument, which is necessary for data with missing values.\n\nlibrary(tidyverse)\n\nPlantGrowth %>% \n  group_by(group) %>% \n  summarise(\n    mean = mean(weight),\n    stddev = sd(weight),\n    median = median(weight),\n    min = min(weight),\n    max = max(weight)\n  )\n\n# A tibble: 3 × 6\n  group  mean stddev median   min   max\n  <fct> <dbl>  <dbl>  <dbl> <dbl> <dbl>\n1 ctrl   5.03  0.583   5.15  4.17  6.11\n2 trt1   4.66  0.794   4.55  3.59  6.03\n3 trt2   5.53  0.443   5.44  4.92  6.31\n\n\nObviously, there are multiple packages who try to address just that. The one I’ve been using for some time now is {dlookr} with its describe() function. It actually provides more measures than I usually need1, but it has everything I want and I disregard the rest (via select()).\n\nPlantGrowth %>%\n  group_by(group) %>%\n  dlookr::describe(weight)\n\n# A tibble: 3 × 27\n  described_…¹ group     n    na  mean    sd se_mean   IQR skewn…² kurto…³   p00\n  <chr>        <fct> <int> <int> <dbl> <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>\n1 weight       ctrl     10     0  5.03 0.583   0.184 0.743   0.321  -0.229  4.17\n2 weight       trt1     10     0  4.66 0.794   0.251 0.662   0.659  -0.203  3.59\n3 weight       trt2     10     0  5.53 0.443   0.140 0.467   0.673  -0.324  4.92\n# … with 16 more variables: p01 <dbl>, p05 <dbl>, p10 <dbl>, p20 <dbl>,\n#   p25 <dbl>, p30 <dbl>, p40 <dbl>, p50 <dbl>, p60 <dbl>, p70 <dbl>,\n#   p75 <dbl>, p80 <dbl>, p90 <dbl>, p95 <dbl>, p99 <dbl>, p100 <dbl>, and\n#   abbreviated variable names ¹​described_variables, ²​skewness, ³​kurtosis\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is intentional that I did not actually load the {dlookr} package, but instead used its `describe()` function via the packagename::functionname() method. This is because of a minor bug in the {dlookr} package described here, which is only relevant if you are using the package with knitr/Rmarkdown/quarto. I am using quarto to generate this website and thus I avoid loading the package. This is fine for me, since I usually only need this one function one time during an analysis. It is also fine for you, since the code works the same way in a standard R script.\n\n\n{ggtext}\nAdding long text to plots created via {ggplot2} is problematic, since you have to insert line breaks yourself. However, {ggext}’s geom_textbox() for data labels and element_textbox_simple() for title, caption etc. will automatically add line breaks:\n\nlongtext <- \"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum.\"\n\n\n\n\nlibrary(ggplot2)\n\nggplot() +\n  \n  \n  aes(y = 1, x = 1, label = longtext) +\n  geom_label() +\n  labs(caption = longtext)\n\n\n\n\n\n\n\n\nlibrary(ggtext)\n\nggplot() +\n  theme(plot.caption =\n          element_textbox_simple()) +\n  aes(y = 1, x = 1, label = longtext) +\n  geom_textbox() +\n  labs(caption = longtext)\n\n\n\n\n\n\n{here}\nTO DO\n{insight}\nTO DO\n{janitor}\nTO DO\nKeyboard shortcuts\nHere are shortcuts I actually use regularly in RStudio:\n\n\n\n\n\n\nShortcut\nDescription\n\n\n\nCTRL+ENTER\nRun selected lines of code\n\n\nCTRL+C\nConvert all selected lines to comment\n\n\nCTRL+SHIFT+M\nInsert %>%\n\n\n\nCTRL+SHIFT+R\nInsert code section header\n\n\nCTRL+LEFT/RIGHT\nJump to Word\n\n\nCTRL+SHIFT+LEFT/RIGHT\nSelect Word\n\n\nALT+LEFT/RIGHT\nJump to Line Start/End\n\n\nALT+SHIFT+LEFT/RIGHT\nSelect to Line Start/End\n\n\nCTRL+A\nHighlight everything (to run the entire code)\n\n\nCTRL+Z\nUndo\n\n\n\nKeyboard shortcuts can be customized in RStudio as described here.\n{modelbased}\nTO DO\n{openxlsx}\nTO DO\n{pacman}\n\nYou now know how to install and load R packages the standard way. However, over the years I switched to using the function p_load() from the {pacman} package instead of library() and install.packages(). The reason is simple: Usually R-scripts start with multiple lines of library() statements that load the necessary packages. However, when this code is run on a different computer, the user may not have all these packages installed and will therefore get an error message. This can be avoided by using the p_load(), because it\n\nloads all packages that are installed and\ninstalls and loads all packages that are not installed.\n\nObviously, {pacman} itself must first be installed (the standard way). Moreover, you may now think that in order to use p_load() we do need a single library(pacman) first. However, we can avoid this by writing pacman::p_load() instead. Simply put, writing package_name::function_name() makes sure that this explicit function from this explicit package is being used. Additionally, R actually lets you use this function without loading the corresponding package. Thus, we now arrived at the way I handle packages at the beginning of all my R-scripts:\n\npacman::p_load(\n  package_name_1,\n  package_name_2,\n  package_name_3\n)\n\n{patchwork}\nTO DO\n{performance}\nTO DO\n{readxl}\nTO DO\n{reprex}\nTO DO\n{scales}\nTO DO\n\n%in% and %not_in%\n\nR has the built-in function %in% which checks whether something is present in a vector.\n\ntreatments <- c(\"Ctrl\", \"A\", \"B\")\n\nNot only can we checke which treatments are present in our treatment vector (left), but we can also easily keep only those that are (right).\n\n\n\nc(\"A\", \"D\") %in% treatments \n\n[1]  TRUE FALSE\n\n\n\n\n\n\nc(\"A\", \"D\") %>% .[. %in% treatments]\n\n[1] \"A\"\n\n\n\n\nNot built-in, for some reason, is the opposite of that function - checking whether something is not present. Yet, we can quickly built our own function that does exactly that:\n\n`%not_in%` <- Negate(`%in%`)\n\n\n\n\nc(\"A\", \"D\") %not_in% treatments \n\n[1] FALSE  TRUE\n\n\n\n\n\n\nc(\"A\", \"D\") %>% .[. %not_in% treatments]\n\n[1] \"D\"\n\n\n\n\nsystem('open \"file.png\"')\nTO DO\n\n\n\n\nFootnotes\n\nKeep in mind that p00 is the 0th percentile and thus the minimum. Analogously, p50 is the median and p100 the maximum.↩︎"
  },
  {
    "objectID": "summaryarticles/workshopprep.html",
    "href": "summaryarticles/workshopprep.html",
    "title": "Prepare for an upcoming workshop",
    "section": "",
    "text": "Hi there! If you are reading this, you will most likely soon participate in a workshop given by me. Thank you - I am already looking forward to it! Here are some tips to help you prepare."
  },
  {
    "objectID": "summaryarticles/workshopprep.html#r-packages",
    "href": "summaryarticles/workshopprep.html#r-packages",
    "title": "Prepare for an upcoming workshop",
    "section": "R-packages",
    "text": "R-packages\nDuring the workshop you will get to know and install many additional R-packages that are not automatically installed. In most cases, you can just install them the moment they are introduced. However, if you e.g. have a slow internet connection, it may be worthwhile to install them beforehand. You can install most of the packages we need by running the following code:\n\nif (!require(\"pacman\", quietly = TRUE))\n  install.packages(\"pacman\")\n\npacman::p_load(\n  agridat,\n  broom,\n  conflicted,\n  desplot,\n  dlookr,\n  emmeans,\n  ggtext,\n  glmmTMB,\n  here,\n  janitor,\n  lme4,\n  lmerTest,\n  modelbased,\n  multcomp,\n  multcompView,\n  naniar,\n  nlme,\n  openxlsx,\n  performance,\n  readxl,\n  scales,\n  tidyverse\n)"
  },
  {
    "objectID": "summaryarticles/workshopprep.html#having-two-screens-helps",
    "href": "summaryarticles/workshopprep.html#having-two-screens-helps",
    "title": "Prepare for an upcoming workshop",
    "section": "Having two screens helps!",
    "text": "Having two screens helps!\nDuring the workshop, I will constantly share my screen with you. Depending on whether you have multiple screens available or not, there are three main scenarios in my experience - ranked from worst to best:\n\nYou only have a single screen\nWhile this is the worst scenario, you obviously can still participate and will have no problem following me during the workshop. What may become difficult, however, is if you want to both look at my shared screen and also write your own R code simulatenously. If you only have only a single screen (that is not super wide), this means constantly switching between zoom and R. Nevertheless, there will be exercises during the workshop where everone has time to do an assignment on their own so that during those time you will definitely get some undivided R time.\nYou have a two screens, but they are not connected to the same deviceAn example for such a scenario would be having a computer with one screen and a laptop/tablet next to it. You could open R on one device and zoom on the other. This is certainly better than having only a single screen, since it is much easier to simultaneously read my and write your code. It is not optimal, however, since it does not allow for the advantages (i) and (ii) listed for scenario 3 below.\nYou have two screensHaving a computer/laptop with multiple screens connected is the optimal setup! You can easily look at both my and your R code and additionally (i) share your screen via zoom so that we can fix an issue you are having in R and (ii) copy-paste things from the zoom-chat into R."
  },
  {
    "objectID": "summaryarticles/workshopprep.html#do-i-need-to-install-zoom",
    "href": "summaryarticles/workshopprep.html#do-i-need-to-install-zoom",
    "title": "Prepare for an upcoming workshop",
    "section": "Do I need to install zoom?",
    "text": "Do I need to install zoom?\n(Obviously, this section is only relevant if the workshop will be held via zoom and not some other video communication software.)\nNo, not necessarily. It is not required to install the zoom software in order to participate in a zoom meeting, because you can also join from your browser (see details here and here). Basically, there will be a link that says “Join from your browser” and that’s it. However, be aware that there are some minor functions not available to you if you are joining from your browser."
  },
  {
    "objectID": "summaryarticles/workshopprep.html#what-else",
    "href": "summaryarticles/workshopprep.html#what-else",
    "title": "Prepare for an upcoming workshop",
    "section": "What else?",
    "text": "What else?\n\nCheck your microphoneMake sure we can hear you when asking a question.\nCheck your camera\nThis is optional of course, but I prefer participants turning their camera on as seeing your reactions improves the workshop.\n\nGet familiar with the options/functions\n\nKnow how to mute your microphone. It should be muted at all times except when you are actually talking to us.\nKnow how to use the zoom chat.\nIn zoom, know how to give a “thumbs up” - making a symbol appear on your screen."
  }
]