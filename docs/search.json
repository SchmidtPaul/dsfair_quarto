[
  {
    "objectID": "ch/exan/simple/alpha_johnwilliams1995.html",
    "href": "ch/exan/simple/alpha_johnwilliams1995.html",
    "title": "One-way alpha design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  agridat,\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  lme4,\n  lmerTest,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)\nconflicts_prefer(lmerTest::lmer)"
  },
  {
    "objectID": "ch/exan/simple/alpha_johnwilliams1995.html#import",
    "href": "ch/exan/simple/alpha_johnwilliams1995.html#import",
    "title": "One-way alpha design",
    "section": "Import",
    "text": "Import\nThe data is available as part of the {agridat} package and needs no further formatting:\n\ndat &lt;- as_tibble(agridat::john.alpha)\ndat\n\n# A tibble: 72 × 7\n    plot rep   block gen   yield   row   col\n   &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1     1 R1    B1    G11    4.12     1     1\n 2     2 R1    B1    G04    4.45     2     1\n 3     3 R1    B1    G05    5.88     3     1\n 4     4 R1    B1    G22    4.58     4     1\n 5     5 R1    B2    G21    4.65     5     1\n 6     6 R1    B2    G10    4.17     6     1\n 7     7 R1    B2    G20    4.01     7     1\n 8     8 R1    B2    G02    4.34     8     1\n 9     9 R1    B3    G23    4.23     9     1\n10    10 R1    B3    G14    4.76    10     1\n# ℹ 62 more rows"
  },
  {
    "objectID": "ch/exan/simple/alpha_johnwilliams1995.html#explore",
    "href": "ch/exan/simple/alpha_johnwilliams1995.html#explore",
    "title": "One-way alpha design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can summarize per block and per cultivar.\n\n\n\ndat %&gt;% \n  group_by(gen) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:n, mean, sd) %&gt;%\n  arrange(desc(n), desc(mean)) %&gt;% \n  print(n = Inf)\n\n# A tibble: 24 × 4\n   gen       n  mean     sd\n   &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 G01       3  5.16 0.534 \n 2 G05       3  5.06 0.841 \n 3 G12       3  4.91 0.641 \n 4 G15       3  4.89 0.207 \n 5 G19       3  4.87 0.398 \n 6 G13       3  4.83 0.619 \n 7 G21       3  4.82 0.503 \n 8 G17       3  4.73 0.379 \n 9 G16       3  4.73 0.502 \n10 G06       3  4.71 0.464 \n11 G22       3  4.64 0.432 \n12 G14       3  4.56 0.186 \n13 G02       3  4.51 0.574 \n14 G18       3  4.44 0.587 \n15 G04       3  4.40 0.0433\n16 G10       3  4.39 0.450 \n17 G11       3  4.38 0.641 \n18 G08       3  4.32 0.584 \n19 G24       3  4.14 0.726 \n20 G23       3  4.14 0.232 \n21 G07       3  4.13 0.510 \n22 G20       3  3.78 0.209 \n23 G09       3  3.61 0.606 \n24 G03       3  3.34 0.456 \n\n\n\n\n\n\ndat %&gt;% \n  group_by(rep, block) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:n, mean, sd) %&gt;%\n  arrange(desc(mean)) %&gt;% \n  print(n = Inf)\n\n# A tibble: 18 × 5\n   rep   block     n  mean     sd\n   &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 R2    B3        4  5.22 0.149 \n 2 R2    B5        4  5.21 0.185 \n 3 R2    B6        4  5.11 0.323 \n 4 R2    B4        4  5.01 0.587 \n 5 R1    B5        4  4.79 0.450 \n 6 R1    B1        4  4.75 0.772 \n 7 R1    B6        4  4.58 0.819 \n 8 R3    B1        4  4.38 0.324 \n 9 R1    B3        4  4.36 0.337 \n10 R1    B4        4  4.33 0.727 \n11 R3    B3        4  4.30 0.0710\n12 R1    B2        4  4.29 0.273 \n13 R2    B2        4  4.23 0.504 \n14 R3    B4        4  4.22 0.375 \n15 R3    B5        4  4.15 0.398 \n16 R2    B1        4  4.12 0.411 \n17 R3    B2        4  3.96 0.631 \n18 R3    B6        4  3.61 0.542 \n\n\n\n\nAdditionally, we can decide to plot our data:\n\nClick to show/hide code# sort genotypes by mean yield\ngen_order &lt;- dat %&gt;% \n  group_by(gen) %&gt;% \n  summarise(mean = mean(yield)) %&gt;% \n  arrange(mean) %&gt;% \n  pull(gen) %&gt;% \n  as.character()\n\nggplot(data = dat) +\n  aes(\n    y = yield,\n    x = gen,\n    shape = rep\n  ) +\n  geom_line(\n    aes(group = gen),\n    color = \"darkgrey\"\n  ) +\n  geom_point() +\n  scale_x_discrete(\n    name = \"Genotype\",\n    limits = gen_order\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_shape_discrete(\n    name = \"Block\"\n  ) +\n  guides(shape = guide_legend(nrow = 1)) +\n  theme_classic() +\n  theme(\n    legend.position = \"top\", \n    axis.text.x = element_text(angle = 90, vjust = 0.5)\n  )\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a non-resolvable augmented design) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}.\n\nClick to show/hide codedesplot(\n  data = dat, \n  flip = TRUE, # row 1 on top, not on bottom\n  form = gen ~ row + col | rep, # fill color per genotype, headers per replicate\n  out1 = block, # lines between incomplete blocks\n  out1.gpar = list(col = \"black\", lwd = 1, lty = \"dashed\"), # line type\n  main = \"Field layout\", # title\n  key.cex = 0.6,\n  layout = c(3, 1) # force all reps drawn in one row\n)"
  },
  {
    "objectID": "ch/exan/simple/augm_petersen1994.html",
    "href": "ch/exan/simple/augm_petersen1994.html",
    "title": "One-way augmented design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  lme4,\n  lmerTest,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)\nconflicts_prefer(lmerTest::lmer)"
  },
  {
    "objectID": "ch/exan/simple/augm_petersen1994.html#import",
    "href": "ch/exan/simple/augm_petersen1994.html#import",
    "title": "One-way augmented design",
    "section": "Import",
    "text": "Import\n\n\n# data is available online:\npath &lt;- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/Petersen1994.csv\"\n\n\n\ndat &lt;- read_csv(path) # use path from above\ndat\n\n# A tibble: 48 × 5\n   gen   yield block   row   col\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 st     2972 I         1     1\n 2 14     2405 I         2     1\n 3 26     2855 I         3     1\n 4 ci     2592 I         4     1\n 5 17     2572 I         5     1\n 6 wa     2608 I         6     1\n 7 22     2705 I         7     1\n 8 13     2391 I         8     1\n 9 st     3122 II        1     2\n10 ci     3023 II        2     2\n# ℹ 38 more rows"
  },
  {
    "objectID": "ch/exan/simple/augm_petersen1994.html#format",
    "href": "ch/exan/simple/augm_petersen1994.html#format",
    "title": "One-way augmented design",
    "section": "Format",
    "text": "Format\nBefore anything, the columns gen and block should be encoded as factors, since R by default encoded them as character.\n\ndat &lt;- dat %&gt;%\n  mutate(across(c(gen, block), ~ as.factor(.x)))"
  },
  {
    "objectID": "ch/exan/simple/augm_petersen1994.html#explore",
    "href": "ch/exan/simple/augm_petersen1994.html#explore",
    "title": "One-way augmented design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can summarize per block and per cultivar.\n\n\n\ndat %&gt;% \n  group_by(gen) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(n), desc(mean))\n\n# A tibble: 33 × 5\n   gen       n    na  mean    sd\n   &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 st        6     0 2759.  832.\n 2 ci        6     0 2726.  711.\n 3 wa        6     0 2678.  615.\n 4 19        1     0 3643    NA \n 5 11        1     0 3380    NA \n 6 07        1     0 3265    NA \n 7 03        1     0 3055    NA \n 8 04        1     0 3018    NA \n 9 01        1     0 3013    NA \n10 30        1     0 2955    NA \n# ℹ 23 more rows\n\n\n\n\n\n\ndat %&gt;% \n  group_by(block) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 6 × 5\n  block     n    na  mean    sd\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 VI        8     0 3205.  417.\n2 II        8     0 2864.  258.\n3 IV        8     0 2797.  445.\n4 I         8     0 2638.  202.\n5 III       8     0 2567.  440.\n6 V         8     0 1390.  207.\n\n\n\n\nAdditionally, we can decide to plot our data. Note that we here define custom colors for the genotypes, where all unreplicated entries get a shade of green and all replicated checks get a shade of red.\n\ngreens30 &lt;- colorRampPalette(c(\"#bce2cc\", \"#00923f\"))(30)\noranges3 &lt;- colorRampPalette(c(\"#e4572e\", \"#ad0000\"))(3)\ngen_cols &lt;- set_names(c(greens30, oranges3), nm = levels(dat$gen))\n\n\nClick to show/hide codeggplot(data = dat) +\n  aes(\n    y = yield,\n    x = gen,\n    shape = block\n  ) +\n  geom_point() +\n    scale_x_discrete(\n    name = \"Genotype\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_color_manual(\n    guide = \"none\",\n    values = gen_cols\n  ) +\n  scale_shape_discrete(\n    name = \"Block\"\n  ) +\n  guides(shape = guide_legend(nrow = 1)) +\n  theme_bw() +\n  theme(\n    legend.position = \"top\",\n    axis.text.x = element_text(size = 7)\n  )\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a non-resolvable augmented design) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}.\n\nClick to show/hide codedesplot(\n  data = dat,\n  flip = TRUE, # row 1 on top, not on bottom\n  form = gen ~ col + row, # fill color per cultivar  \n  col.regions = gen_cols, # custom fill colors\n  out1 = block, # line between blocks                     \n  text = gen, # cultivar names per plot\n  cex = 1, # cultviar names: font size\n  shorten = FALSE, # cultivar names: don't abbreviate\n  main = \"Field layout\", # plot title\n  show.key = FALSE # hide legend\n)"
  },
  {
    "objectID": "ch/exan/simple/crd_mead1993.html",
    "href": "ch/exan/simple/crd_mead1993.html",
    "title": "One-way completely randomized design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)"
  },
  {
    "objectID": "ch/exan/simple/crd_mead1993.html#import",
    "href": "ch/exan/simple/crd_mead1993.html#import",
    "title": "One-way completely randomized design",
    "section": "Import",
    "text": "Import\n\n\n# data is available online:\npath &lt;- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/Mead1993.csv\"\n\n\n\ndat &lt;- read_csv(path) # use path from above\ndat\n\n# A tibble: 24 × 4\n   variety yield   row   col\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 v1       25.1     4     2\n 2 v1       17.2     1     6\n 3 v1       26.4     4     1\n 4 v1       16.1     1     4\n 5 v1       22.2     1     2\n 6 v1       15.9     2     4\n 7 v2       40.2     4     4\n 8 v2       35.2     3     1\n 9 v2       32.0     4     6\n10 v2       36.5     2     1\n# ℹ 14 more rows"
  },
  {
    "objectID": "ch/exan/simple/crd_mead1993.html#format",
    "href": "ch/exan/simple/crd_mead1993.html#format",
    "title": "One-way completely randomized design",
    "section": "Format",
    "text": "Format\nBefore anything, the column variety should be encoded as a factor, since R by default encoded it as a character variable. There are multiple ways to do this - here are two:\n\n\n\ndat &lt;- dat %&gt;% \n  mutate(variety = as.factor(variety))\n\n\n\n\n\ndat &lt;- dat %&gt;% \n  mutate(across(variety, ~ as.factor(.x)))"
  },
  {
    "objectID": "ch/exan/simple/crd_mead1993.html#explore",
    "href": "ch/exan/simple/crd_mead1993.html#explore",
    "title": "One-way completely randomized design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can a summary per variety.\n\ndat %&gt;% \n  group_by(variety) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd, p00, p100) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 4 × 7\n  variety     n    na  mean    sd   p00  p100\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 v2          6     0  37.4  3.95  32.0  43.3\n2 v4          6     0  29.9  2.23  27.6  33.2\n3 v1          6     0  20.5  4.69  15.9  26.4\n4 v3          6     0  19.5  5.56  11.4  25.9\n\n\nAdditionally, we can decide to plot our data:\n\nClick to show/hide codeggplot(data = dat) +\n  aes(y = yield, x = variety) +\n  geom_point() +\n  scale_x_discrete(\n    name = \"Variety\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  theme_classic()\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a completely randomized design; CRD) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}:\n\nClick to show/hide codedesplot(\n  data = dat, \n  flip = TRUE, # row 1 on top, not on bottom\n  form = variety ~ col + row, # fill color per variety\n  text = variety, # variety names per plot\n  cex = 1, # variety names: font size\n  shorten = \"no\", # variety names: don't abbreviate\n  main = \"Field layout\", # plot title\n  show.key = FALSE # hide legend \n  )"
  },
  {
    "objectID": "ch/exan/simple/latsq_bridges1989.html",
    "href": "ch/exan/simple/latsq_bridges1989.html",
    "title": "One-way latin square design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  agridat,\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)"
  },
  {
    "objectID": "ch/exan/simple/latsq_bridges1989.html#import",
    "href": "ch/exan/simple/latsq_bridges1989.html#import",
    "title": "One-way latin square design",
    "section": "Import",
    "text": "Import\n\ndat &lt;- agridat::bridges.cucumber %&gt;% \n  as_tibble() %&gt;% \n  filter(loc == \"Clemson\") %&gt;% # filter data from only one location\n  select(-loc) # remove loc column which is now unnecessary\n\ndat\n\n# A tibble: 16 × 4\n   gen        row   col yield\n   &lt;fct&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1 Dasher       1     3  44.2\n 2 Dasher       2     4  54.1\n 3 Dasher       3     2  47.2\n 4 Dasher       4     1  36.7\n 5 Guardian     1     4  33  \n 6 Guardian     2     2  13.6\n 7 Guardian     3     1  44.1\n 8 Guardian     4     3  35.8\n 9 Poinsett     1     1  11.5\n10 Poinsett     2     3  22.4\n11 Poinsett     3     4  30.3\n12 Poinsett     4     2  21.5\n13 Sprint       1     2  15.1\n14 Sprint       2     1  20.3\n15 Sprint       3     3  41.3\n16 Sprint       4     4  27.1"
  },
  {
    "objectID": "ch/exan/simple/latsq_bridges1989.html#format",
    "href": "ch/exan/simple/latsq_bridges1989.html#format",
    "title": "One-way latin square design",
    "section": "Format",
    "text": "Format\nFor our analysis, gen, row and col should be encoded as factors. However, the desplot() function needs row and col as formatted as integers. Therefore we create copies of these columns encoded as factors and named rowF and colF. Below are two ways how to achieve this:\n\n\n\ndat &lt;- dat %&gt;%\n  mutate(\n    colF = as.factor(col),\n    rowF = as.factor(row)\n  )\n\n\n\n\n\ndat &lt;- dat %&gt;%\n  mutate(across(\n    .cols = c(row, col), \n    .fns = ~ as.factor(.x), \n    .names = (\"{.col}F\")\n  ))"
  },
  {
    "objectID": "ch/exan/simple/latsq_bridges1989.html#explore",
    "href": "ch/exan/simple/latsq_bridges1989.html#explore",
    "title": "One-way latin square design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can summarize per genotype, per row and per column.\n\ndat %&gt;% \n  group_by(gen) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 4 × 5\n  gen          n    na  mean    sd\n  &lt;fct&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Dasher       4     0  45.6  7.21\n2 Guardian     4     0  31.6 12.9 \n3 Sprint       4     0  26.0 11.4 \n4 Poinsett     4     0  21.4  7.71\n\n\n\n\n\ndat %&gt;% \n  group_by(rowF) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 4 × 5\n  rowF      n    na  mean    sd\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 3         4     0  40.7  7.36\n2 4         4     0  30.3  7.28\n3 2         4     0  27.6 18.1 \n4 1         4     0  26.0 15.4 \n\n\n\n\n\n\ndat %&gt;% \n  group_by(colF) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 4 × 5\n  colF      n    na  mean    sd\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 4         4     0  36.1 12.2 \n2 3         4     0  35.9  9.67\n3 1         4     0  28.2 14.9 \n4 2         4     0  24.4 15.6 \n\n\n\n\nAdditionally, we can decide to plot our data:\n\nClick to show/hide codeggplot(data = dat) +\n  aes(y = yield, x = gen, color = colF, shape = rowF) +\n  geom_point() +\n    scale_x_discrete(\n    name = \"Genotype\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_color_discrete(\n    name = \"Column\"\n  ) +\n  scale_shape_discrete(\n    name = \"Row\"\n  ) +\n  theme_classic()\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a Latin square design) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}. We can even create a second field plan that gives us a feeling for the yields per plot.\n\n\n\nClick to show/hide codedesplot(\n  data = dat,\n  flip = TRUE, # row 1 on top, not on bottom\n  form = gen ~ col + row, # fill color per genotype       \n  out1 = rowF, # line between rows\n  out2 = colF, # line between columns\n  out1.gpar = list(col = \"black\", lwd = 2), # out1 line style\n  out2.gpar = list(col = \"black\", lwd = 2), # out2 line style\n  text = gen, # gen names per plot\n  cex = 1, # gen names: font size\n  shorten = FALSE, # gen names: don't abbreviate\n  main = \"Field layout\", # plot title\n  show.key = FALSE # hide legend\n  ) \n\n\n\n\n\n\n\n\nClick to show/hide codedesplot(\n  data = dat,\n  flip = TRUE, # row 1 on top, not on bottom\n  form = yield ~ col + row, # fill color according to yield     \n  out1 = rowF, # line between rows\n  out2 = colF, # line between columns\n  out1.gpar = list(col = \"black\", lwd = 2), # out1 line style\n  out2.gpar = list(col = \"black\", lwd = 2), # out2 line style\n  text = gen, # gen names per plot\n  cex = 1, # gen names: font size\n  shorten = FALSE, # gen names: don't abbreviate\n  main = \"Yield per plot\", # plot title\n  show.key = FALSE # hide legend\n  ) \n\n\n\n\n\n\nThus, Dasher seems to be the most promising genotype in terms of yield. Moreover, it can be seen that yields were generally higher in column 4 and row 3."
  },
  {
    "objectID": "ch/exan/simple/rcbd_clewerscarisbrick2001.html",
    "href": "ch/exan/simple/rcbd_clewerscarisbrick2001.html",
    "title": "One-way randomized complete block design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)"
  },
  {
    "objectID": "ch/exan/simple/rcbd_clewerscarisbrick2001.html#import",
    "href": "ch/exan/simple/rcbd_clewerscarisbrick2001.html#import",
    "title": "One-way randomized complete block design",
    "section": "Import",
    "text": "Import\n\n\n# data is available online:\npath &lt;- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/Clewer&Scarisbrick2001.csv\"\n\n\n\ndat &lt;- read_csv(path) # use path from above\ndat\n\n# A tibble: 12 × 5\n   block cultivar yield   row   col\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 B1    C1         7.4     2     1\n 2 B1    C2         9.8     3     1\n 3 B1    C3         7.3     1     1\n 4 B1    C4         9.5     4     1\n 5 B2    C1         6.5     1     2\n 6 B2    C2         6.8     4     2\n 7 B2    C3         6.1     3     2\n 8 B2    C4         8       2     2\n 9 B3    C1         5.6     2     3\n10 B3    C2         6.2     1     3\n11 B3    C3         6.4     3     3\n12 B3    C4         7.4     4     3"
  },
  {
    "objectID": "ch/exan/simple/rcbd_clewerscarisbrick2001.html#format",
    "href": "ch/exan/simple/rcbd_clewerscarisbrick2001.html#format",
    "title": "One-way randomized complete block design",
    "section": "Format",
    "text": "Format\nBefore anything, the columns block and cultivar should be encoded as factors, since R by default encoded them as character.\n\ndat &lt;- dat %&gt;%\n  mutate(across(c(block, cultivar), ~ as.factor(.x)))"
  },
  {
    "objectID": "ch/exan/simple/rcbd_clewerscarisbrick2001.html#explore",
    "href": "ch/exan/simple/rcbd_clewerscarisbrick2001.html#explore",
    "title": "One-way randomized complete block design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can summarize per block and per cultivar.\n\n\n\ndat %&gt;% \n  group_by(cultivar) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 4 × 5\n  cultivar     n    na  mean    sd\n  &lt;fct&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 C4           3     0   8.3 1.08 \n2 C2           3     0   7.6 1.93 \n3 C3           3     0   6.6 0.624\n4 C1           3     0   6.5 0.9  \n\n\n\n\n\n\ndat %&gt;% \n  group_by(block) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 3 × 5\n  block     n    na  mean    sd\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 B1        4     0  8.5  1.33 \n2 B2        4     0  6.85 0.819\n3 B3        4     0  6.4  0.748\n\n\n\n\nAdditionally, we can decide to plot our data:\n\nClick to show/hide codeggplot(data = dat) +\n  aes(y = yield, x = cultivar, color = block) +\n  geom_point() +\n    scale_x_discrete(\n    name = \"Cultivar\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_color_discrete(\n    name = \"Block\"\n  ) +\n  theme_classic()\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a randomized complete block design; RCBD) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}. We can even create a second field plan that gives us a feeling for the yields per plot.\n\n\n\nClick to show/hide codedesplot(\n  data = dat,\n  flip = TRUE, # row 1 on top, not on bottom\n  form = cultivar ~ col + row, # fill color per cultivar       \n  out1 = block, # line between blocks                     \n  text = cultivar, # cultivar names per plot\n  cex = 1, # cultviar names: font size\n  shorten = FALSE, # cultivar names: don't abbreviate\n  main = \"Field layout\", # plot title\n  show.key = FALSE # hide legend\n  ) \n\n\n\n\n\n\n\n\nClick to show/hide codedesplot(\n  data = dat,\n  flip = TRUE, # row 1 on top, not on bottom\n  form = yield ~ col + row, # fill color according to yield      \n  out1 = block, # line between blocks                     \n  text = cultivar, # cultivar names per plot\n  cex = 1, # cultviar names: font size\n  shorten = FALSE, # cultivar names: don't abbreviate\n  main = \"Yield per plot\", # plot title\n  show.key = FALSE # hide legend\n  ) \n\n\n\n\n\n\nThus, C4 seems to be the most promising cultivar in terms of yield. Moreover, it can be seen that yields were generally higher in block B1 (left), compared to the other blocks."
  },
  {
    "objectID": "ch/exan/simple/rcbd_gomezgomez1984.html",
    "href": "ch/exan/simple/rcbd_gomezgomez1984.html",
    "title": "Two-way randomized complete block design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  MetBrewer,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)"
  },
  {
    "objectID": "ch/exan/simple/rcbd_gomezgomez1984.html#import",
    "href": "ch/exan/simple/rcbd_gomezgomez1984.html#import",
    "title": "Two-way randomized complete block design",
    "section": "Import",
    "text": "Import\n\n\n# data is available online:\npath &lt;- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/riceRCBD.csv\"\n\n\n\ndat &lt;- read_csv(path) # use path from above\ndat\n\n# A tibble: 72 × 6\n     row   col rep   N      G     yield\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n 1     2     6 rep1  Goomba Simba  4520\n 2     3     4 rep1  Koopa  Simba  5598\n 3     2     3 rep1  Toad   Simba  6192\n 4     1     1 rep1  Peach  Simba  8542\n 5     2     1 rep1  Diddy  Simba  5806\n 6     3     1 rep1  Yoshi  Simba  7470\n 7     4     5 rep1  Goomba Nala   4034\n 8     4     1 rep1  Koopa  Nala   6682\n 9     3     2 rep1  Toad   Nala   6869\n10     1     2 rep1  Peach  Nala   6318\n# ℹ 62 more rows"
  },
  {
    "objectID": "ch/exan/simple/rcbd_gomezgomez1984.html#format",
    "href": "ch/exan/simple/rcbd_gomezgomez1984.html#format",
    "title": "Two-way randomized complete block design",
    "section": "Format",
    "text": "Format\nBefore anything, the columns rep, N and G should be encoded as factors, since R by default encoded them as character.\n\ndat &lt;- dat %&gt;%\n  mutate(across(c(rep, N, G), ~ as.factor(.x)))"
  },
  {
    "objectID": "ch/exan/simple/rcbd_gomezgomez1984.html#explore",
    "href": "ch/exan/simple/rcbd_gomezgomez1984.html#explore",
    "title": "Two-way randomized complete block design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can summarize per nitrogen level, per genotype and also per nitrogen-genotype-combination.\n\n\n\ndat %&gt;% \n  group_by(N) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 6 × 5\n  N          n    na  mean    sd\n  &lt;fct&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Diddy     12     0 5866.  832.\n2 Toad      12     0 5864. 1434.\n3 Yoshi     12     0 5812  2349.\n4 Peach     12     0 5797. 2660.\n5 Koopa     12     0 5478.  657.\n6 Goomba    12     0 4054.  672.\n\ndat %&gt;% \n  group_by(G) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 4 × 5\n  G         n    na  mean    sd\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Simba    18     0 6554. 1475.\n2 Nala     18     0 6156. 1078.\n3 Timon    18     0 5563. 1269.\n4 Pumba    18     0 3642. 1434.\n\n\n\n\n\n\ndat %&gt;% \n  group_by(N, G) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean)) %&gt;% \n  print(n=Inf)\n\n# A tibble: 24 × 6\n   N      G         n    na  mean     sd\n   &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Peach  Simba     3     0 8701.  270. \n 2 Yoshi  Simba     3     0 7563.   86.9\n 3 Yoshi  Nala      3     0 6951.  808. \n 4 Toad   Nala      3     0 6895   166. \n 5 Toad   Simba     3     0 6733.  490. \n 6 Yoshi  Timon     3     0 6687.  496. \n 7 Peach  Nala      3     0 6540.  936. \n 8 Diddy  Simba     3     0 6400   523. \n 9 Diddy  Nala      3     0 6259   499. \n10 Peach  Timon     3     0 6065. 1097. \n11 Toad   Timon     3     0 6014   515. \n12 Diddy  Timon     3     0 5994   101. \n13 Koopa  Nala      3     0 5982   684. \n14 Koopa  Simba     3     0 5672   458. \n15 Koopa  Timon     3     0 5443.  589. \n16 Koopa  Pumba     3     0 4816   506. \n17 Diddy  Pumba     3     0 4812   963. \n18 Goomba Pumba     3     0 4481.  463. \n19 Goomba Nala      3     0 4306   646. \n20 Goomba Simba     3     0 4253.  248. \n21 Toad   Pumba     3     0 3816  1311. \n22 Goomba Timon     3     0 3177.  453. \n23 Yoshi  Pumba     3     0 2047.  703. \n24 Peach  Pumba     3     0 1881.  407. \n\n\n\n\nAdditionally, we can decide to plot our data. One way to deal with the combination of two factors would be to use panels/facets in ggplot2.\nNote that we here define a custom set of colors for the Nitrogen levels that will be used throughout this chapter.\n\nNcolors &lt;- met.brewer(\"VanGogh2\", 6) %&gt;% \n  as.vector() %&gt;% \n  set_names(levels(dat$N))\n\n\nClick to show/hide codeggplot(data = dat) +\n  aes(y = yield, x = N, color = N) +\n  facet_wrap(~G, labeller = label_both) +\n  stat_summary(\n    fun = mean,\n    colour = \"grey\",\n    geom = \"line\",\n    linetype = \"dotted\",\n    group = 1\n  ) +\n  geom_point() +\n  scale_x_discrete(\n    name = \"Nitrogen\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_color_manual(\n    values = Ncolors, \n    guide = \"none\"\n  ) +\n  theme_bw() +\n  theme(axis.text.x = element_text(\n    angle = 45,\n    hjust = 1,\n    vjust = 1\n  ))\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a randomized complete block design; RCBD) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}.\n\nClick to show/hide codedesplot(\n  data = dat,\n  form = rep ~ col + row | rep, # fill color per rep, headers per rep\n  col.regions = c(\"white\", \"grey95\", \"grey90\"),\n  text = G, # genotype names per plot\n  cex = 0.8, # genotype names: font size\n  shorten = \"abb\", # genotype names: abbreviate\n  col = N, # color of genotype names for each N-level\n  col.text = Ncolors, # use custom colors from above\n  out1 = col, out1.gpar = list(col = \"darkgrey\"), # lines between columns\n  out2 = row, out2.gpar = list(col = \"darkgrey\"), # lines between rows\n  main = \"Field layout\", # plot title\n  show.key = TRUE, # show legend\n  key.cex = 0.7 # legend font size\n  )\n\n\n\n\n\n\n\nClick to show/hide codedesplot(\n  data = dat,\n  form = yield ~ col + row | rep, # fill color per rep, headers per rep\n  text = G, # genotype names per plot\n  cex = 0.8, # genotype names: font size\n  shorten = \"abb\", # genotype names: abbreviate\n  col  = N, # color of genotype names for each N-level\n  col.text = Ncolors, # use custom colors from above\n  out1 = col, out1.gpar = list(col = \"darkgrey\"), # lines between columns\n  out2 = row, out2.gpar = list(col = \"darkgrey\"), # lines between rows\n  main = \"Yield per plot\", # plot title\n  show.key = FALSE, # show legend\n  key.cex = 0.7 # legend font size\n  )\n\n\n\n\n\n\n\n\nClick to show/hide coderepcolors &lt;- c(met.brewer(\"VanGogh3\", 1),\n               met.brewer(\"Hokusai2\", 1),\n               met.brewer(\"OKeeffe2\", 1)) %&gt;%\n  as.vector() %&gt;%\n  set_names(levels(dat$rep))\n\ndesplot(\n  data = dat,\n  form = rep ~ col + row | rep, # fill color per rep, headers per rep\n  col.regions = repcolors,\n  out1 = col, out1.gpar = list(col = \"darkgrey\"), # lines between columns\n  out2 = row, out2.gpar = list(col = \"darkgrey\"), # lines between rows\n  main = \"Experimental design focus\", # plot title\n  show.key = FALSE # don't show legend\n  )"
  },
  {
    "objectID": "ch/exan/simple/rowcol_kemptonfox1997.html",
    "href": "ch/exan/simple/rowcol_kemptonfox1997.html",
    "title": "One-way row column design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  lme4,\n  lmerTest,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)\nconflicts_prefer(lmerTest::lmer)"
  },
  {
    "objectID": "ch/exan/simple/rowcol_kemptonfox1997.html#import",
    "href": "ch/exan/simple/rowcol_kemptonfox1997.html#import",
    "title": "One-way row column design",
    "section": "Import",
    "text": "Import\nThe data is available as part of the {agridat} package:\n\ndat &lt;- as_tibble(agridat::kempton.rowcol)\ndat\n\n# A tibble: 68 × 5\n   rep     row   col gen   yield\n   &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt;\n 1 R1        1     1 G20    3.77\n 2 R1        1     2 G04    3.21\n 3 R1        1     3 G33    4.55\n 4 R1        1     4 G28    4.09\n 5 R1        1     5 G07    5.05\n 6 R1        1     6 G12    4.19\n 7 R1        1     7 G30    3.27\n 8 R1        2     1 G10    3.44\n 9 R1        2     2 G14    4.3 \n10 R1        2     4 G21    3.86\n# ℹ 58 more rows"
  },
  {
    "objectID": "ch/exan/simple/rowcol_kemptonfox1997.html#format",
    "href": "ch/exan/simple/rowcol_kemptonfox1997.html#format",
    "title": "One-way row column design",
    "section": "Format",
    "text": "Format\nFor our analysis, gen, row and col should be encoded as factors. However, the desplot() function needs row and col as formatted as integers. Therefore we create copies of these columns encoded as factors and named rowF and colF:\n\ndat &lt;- dat %&gt;%\n  mutate(\n    colF = as.factor(col),\n    rowF = as.factor(row)\n  )"
  },
  {
    "objectID": "ch/exan/simple/rowcol_kemptonfox1997.html#explore",
    "href": "ch/exan/simple/rowcol_kemptonfox1997.html#explore",
    "title": "One-way row column design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can summarize per block and per cultivar.\n\ndat %&gt;% \n  group_by(gen) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 35 × 5\n   gen       n    na  mean     sd\n   &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 G19       2     0  6.07 1.84  \n 2 G07       2     0  5.74 0.976 \n 3 G33       2     0  5.13 0.820 \n 4 G06       2     0  4.96 0.940 \n 5 G09       2     0  4.94 1.68  \n 6 G11       2     0  4.93 1.03  \n 7 G14       2     0  4.92 0.877 \n 8 G27       2     0  4.89 1.80  \n 9 G03       2     0  4.78 0.0424\n10 G25       2     0  4.78 0.361 \n# ℹ 25 more rows\n\n\nAdditionally, we can decide to plot our data.\n\nClick to show/hide code# sort genotypes by mean yield\ngen_order &lt;- dat %&gt;% \n  group_by(gen) %&gt;% \n  summarise(mean = mean(yield, na.rm = TRUE)) %&gt;% \n  arrange(mean) %&gt;% \n  pull(gen) %&gt;% \n  as.character()\n\nggplot(data = dat) +\n  aes(\n    y = yield,\n    x = gen,\n    shape = rep\n  ) +\n  geom_line(\n    aes(group = gen),\n    color = \"darkgrey\"\n  ) +\n  geom_point() +\n  scale_x_discrete(\n    name = \"Genotype\",\n    limits = gen_order\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_shape_discrete(\n    name = \"Replicate\"\n  ) +\n  guides(shape = guide_legend(nrow = 1)) +\n  theme_classic() +\n  theme(\n    legend.position = \"top\", \n    axis.text.x = element_text(angle = 90, vjust = 0.5)\n  )\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a resolvable row column design) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}. In this case it is worth noting that there is missing data, as yield values for two plots are not present in the data.\n\nClick to show/hide codedesplot(\n  data = dat,\n  form = gen ~ col + row | rep, # fill color per genotype, headers per replicate\n  text = gen, \n  cex = 0.7, \n  shorten = \"no\",\n  out1 = row, out1.gpar=list(col=\"black\"), # lines between rows\n  out2 = col, out2.gpar=list(col=\"black\"), # lines between columns\n  main = \"Field layout\", \n  show.key = FALSE\n)"
  },
  {
    "objectID": "ch/exan/simple/splitplot_gomezgomez1984.html",
    "href": "ch/exan/simple/splitplot_gomezgomez1984.html",
    "title": "Two-way split-plot design",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  conflicted,\n  desplot,\n  emmeans,\n  ggtext,\n  lme4,\n  lmerTest,\n  MetBrewer,\n  multcomp,\n  multcompView,\n  tidyverse)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)\nconflicts_prefer(lmerTest::lmer)"
  },
  {
    "objectID": "ch/exan/simple/splitplot_gomezgomez1984.html#import",
    "href": "ch/exan/simple/splitplot_gomezgomez1984.html#import",
    "title": "Two-way split-plot design",
    "section": "Import",
    "text": "Import\n\n\n# data is available online:\npath &lt;- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/Gomez&Gomez1984.csv\"\n\n\n\ndat &lt;- read_csv(path) # use path from above\ndat\n\n# A tibble: 72 × 7\n   yield   row   col rep   mainplot G     N     \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; \n 1  4520     4     1 rep1  mp01     Simba Goomba\n 2  5598     2     2 rep1  mp02     Simba Koopa \n 3  6192     1     3 rep1  mp03     Simba Toad  \n 4  8542     2     4 rep1  mp04     Simba Peach \n 5  5806     2     5 rep1  mp05     Simba Diddy \n 6  7470     1     6 rep1  mp06     Simba Yoshi \n 7  4034     2     1 rep1  mp01     Nala  Goomba\n 8  6682     4     2 rep1  mp02     Nala  Koopa \n 9  6869     3     3 rep1  mp03     Nala  Toad  \n10  6318     4     4 rep1  mp04     Nala  Peach \n# ℹ 62 more rows"
  },
  {
    "objectID": "ch/exan/simple/splitplot_gomezgomez1984.html#format",
    "href": "ch/exan/simple/splitplot_gomezgomez1984.html#format",
    "title": "Two-way split-plot design",
    "section": "Format",
    "text": "Format\nBefore anything, the columns rep, N and G should be encoded as factors, since R by default encoded them as character.\n\ndat &lt;- dat %&gt;%\n  mutate(across(c(rep:N), ~ as.factor(.x)))"
  },
  {
    "objectID": "ch/exan/simple/splitplot_gomezgomez1984.html#explore",
    "href": "ch/exan/simple/splitplot_gomezgomez1984.html#explore",
    "title": "Two-way split-plot design",
    "section": "Explore",
    "text": "Explore\nWe make use of dlookr::describe() to conveniently obtain descriptive summary tables. Here, we get can summarize per nitrogen level, per genotype and also per nitrogen-genotype-combination.\n\n\n\ndat %&gt;% \n  group_by(N) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 6 × 5\n  N          n    na  mean    sd\n  &lt;fct&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Diddy     12     0 5866.  832.\n2 Toad      12     0 5864. 1434.\n3 Yoshi     12     0 5812  2349.\n4 Peach     12     0 5797. 2660.\n5 Koopa     12     0 5478.  657.\n6 Goomba    12     0 4054.  672.\n\ndat %&gt;% \n  group_by(G) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean))\n\n# A tibble: 4 × 5\n  G         n    na  mean    sd\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Simba    18     0 6554. 1475.\n2 Nala     18     0 6156. 1078.\n3 Timon    18     0 5563. 1269.\n4 Pumba    18     0 3642. 1434.\n\n\n\n\n\n\ndat %&gt;% \n  group_by(N, G) %&gt;% \n  dlookr::describe(yield) %&gt;% \n  select(2:sd) %&gt;%\n  arrange(desc(mean)) %&gt;% \n  print(n=Inf)\n\n# A tibble: 24 × 6\n   N      G         n    na  mean     sd\n   &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Peach  Simba     3     0 8701.  270. \n 2 Yoshi  Simba     3     0 7563.   86.9\n 3 Yoshi  Nala      3     0 6951.  808. \n 4 Toad   Nala      3     0 6895   166. \n 5 Toad   Simba     3     0 6733.  490. \n 6 Yoshi  Timon     3     0 6687.  496. \n 7 Peach  Nala      3     0 6540.  936. \n 8 Diddy  Simba     3     0 6400   523. \n 9 Diddy  Nala      3     0 6259   499. \n10 Peach  Timon     3     0 6065. 1097. \n11 Toad   Timon     3     0 6014   515. \n12 Diddy  Timon     3     0 5994   101. \n13 Koopa  Nala      3     0 5982   684. \n14 Koopa  Simba     3     0 5672   458. \n15 Koopa  Timon     3     0 5443.  589. \n16 Koopa  Pumba     3     0 4816   506. \n17 Diddy  Pumba     3     0 4812   963. \n18 Goomba Pumba     3     0 4481.  463. \n19 Goomba Nala      3     0 4306   646. \n20 Goomba Simba     3     0 4253.  248. \n21 Toad   Pumba     3     0 3816  1311. \n22 Goomba Timon     3     0 3177.  453. \n23 Yoshi  Pumba     3     0 2047.  703. \n24 Peach  Pumba     3     0 1881.  407. \n\n\n\n\nAdditionally, we can decide to plot our data. One way to deal with the combination of two factors would be to use panels/facets in ggplot2.\nNote that we here define a custom set of colors for the Nitrogen levels that will be used throughout this chapter.\n\nNcolors &lt;- met.brewer(\"VanGogh2\", 6) %&gt;% \n  as.vector() %&gt;% \n  set_names(levels(dat$N))\n\n\nClick to show/hide codeggplot(data = dat) +\n  aes(y = yield, x = N, color = N) +\n  facet_wrap(~G, labeller = label_both) +\n  stat_summary(\n    fun = mean,\n    colour = \"grey\",\n    geom = \"line\",\n    linetype = \"dotted\",\n    group = 1\n  ) +\n  geom_point() +\n  scale_x_discrete(\n    name = \"Nitrogen\"\n  ) +\n  scale_y_continuous(\n    name = \"Yield\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_color_manual(\n    values = Ncolors, \n    guide = \"none\"\n  ) +\n  theme_bw() +\n  theme(axis.text.x = element_text(\n    angle = 45,\n    hjust = 1,\n    vjust = 1\n  ))\n\n\n\n\nFinally, since this is an experiment that was laid with a certain experimental design (= a split-plot design) - it makes sense to also get a field plan. This can be done via desplot() from {desplot}.\n\nClick to show/hide codedesplot(\n  data = dat,\n  form = rep ~ col + row | rep, # fill color per rep, headers per rep\n  col.regions = c(\"white\", \"grey95\", \"grey90\"),\n  text = G, # genotype names per plot\n  cex = 0.8, # genotype names: font size\n  shorten = \"abb\", # genotype names: abbreviate\n  col = N, # color of genotype names for each N-level\n  col.text = Ncolors, # use custom colors from above\n  out1 = col, out1.gpar = list(col = \"darkgrey\"), # lines between columns\n  out2 = row, out2.gpar = list(col = \"darkgrey\"), # lines between rows\n  main = \"Field layout\", # plot title\n  show.key = TRUE, # show legend\n  key.cex = 0.7 # legend font size\n  )\n\n\n\n\n\n\n\nClick to show/hide codedesplot(\n  data = dat,\n  form = yield ~ col + row | rep, # fill color per rep, headers per rep\n  text = G, # genotype names per plot\n  cex = 0.8, # genotype names: font size\n  shorten = \"abb\", # genotype names: abbreviate\n  col  = N, # color of genotype names for each N-level\n  col.text = Ncolors, # use custom colors from above\n  out1 = col, out1.gpar = list(col = \"darkgrey\"), # lines between columns\n  out2 = row, out2.gpar = list(col = \"darkgrey\"), # lines between rows\n  main = \"Yield per plot\", # plot title\n  show.key = FALSE # show legend\n  )\n\n\n\n\n\n\n\n\nClick to show/hide codemainplotcolors &lt;- c(met.brewer(\"VanGogh3\", 6),\n                    met.brewer(\"Hokusai2\", 6),\n                    met.brewer(\"OKeeffe2\", 6)) %&gt;%\n  as.vector() %&gt;%\n  set_names(levels(dat$mainplot))\n\ndesplot(\n  data = dat,\n  form = mainplot ~ col + row | rep, # fill color per rep, headers per rep\n  col.regions = mainplotcolors,\n  out1 = col, out1.gpar = list(col = \"darkgrey\"), # lines between columns\n  out2 = row, out2.gpar = list(col = \"darkgrey\"), # lines between rows\n  main = \"Experimental design focus\", # plot title\n  show.key = TRUE, # don't show legend\n  key.cex = 0.6\n  )"
  },
  {
    "objectID": "ch/misc/furtherresources.html",
    "href": "ch/misc/furtherresources.html",
    "title": "Further resources",
    "section": "",
    "text": "This is a collection of resources I would recommend for topics that are not covered (well enough) on this website.\nIn general, you should browse through the Big Book of R, which is curated collection of over 300 R programming books on different topics.\n\nGeneralized Linear Models\n\nThe online version of “Beyond Multiple Linear Regression - Applied Generalized Linear Models and Multilevel Models in R” by Roback and Legler (2021)\nThe open access PDF version “Generalized Linear Mixed Models with Applications in Agriculture and Biology” by Ruı́z et al. (2023)\n\n\n\nMaps\n\nChapter 6: Maps in Wickham (2016)\nDrawing beautiful maps programmatically with R, sf and ggplot2 Part 1, Part 2 and Part 3\n#30DayMapChallenge - check out the posts of previous years, e.g. all the contributions of user bydata and the R code that created them.\n\n\n\nMeta analyses\n\nThe online version of “Doing Meta-Analysis with R: A Hands-On Guide” by Harrer et al. (2021)\n\n\n\n\n\n\n\n\nReferences\n\nHarrer, Mathias, Pim Cuijpers, Toshi A. Furukawa, and David D. Ebert. 2021. Doing Meta-Analysis with r. Chapman; Hall/CRC. https://doi.org/10.1201/9781003107347.\n\n\nRoback, Paul, and Julie Legler. 2021. Beyond Multiple Linear Regression. Chapman; Hall/CRC. https://doi.org/10.1201/9780429066665.\n\n\nRuı́z, Josafhat Salinas, Osval Antonio Montesinos López, Gabriela Hernández Ramı́rez, and Jose Crossa Hiriart. 2023. Generalized Linear Mixed Models with Applications in Agriculture and Biology. Springer International Publishing. https://doi.org/10.1007/978-3-031-32800-8.\n\n\nWickham, Hadley. 2016. Ggplot2 Elegant Graphics for Data Analysis. Use r! Springer International Publishing. https://doi.org/10.1007/978-3-319-24277-4.\n\nCitationBibTeX citation:@online{schmidt2023,\n  author = {Paul Schmidt},\n  title = {Further Resources},\n  date = {2023-10-01},\n  url = {https://schmidtpaul.github.io/dsfair_quarto//ch/misc/furtherresources.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPaul Schmidt. 2023. “Further Resources.” October 1, 2023.\nhttps://schmidtpaul.github.io/dsfair_quarto//ch/misc/furtherresources.html."
  },
  {
    "objectID": "ch/misc/usefulthings.html",
    "href": "ch/misc/usefulthings.html",
    "title": "Useful things",
    "section": "",
    "text": "This chapter is a collection of things I wish I had known earlier in my years using R and that I hope can be of use to you. Sections are named after R packages or whatever applies and sorted alphabetically."
  },
  {
    "objectID": "ch/misc/usefulthings.html#footnotes",
    "href": "ch/misc/usefulthings.html#footnotes",
    "title": "Useful things",
    "section": "Footnotes",
    "text": "Footnotes\n\nKeep in mind that p00 is the 0th percentile and thus the minimum. Analogously, p50 is the median and p100 the maximum.↩︎"
  },
  {
    "objectID": "ch/misc/workshopprep.html",
    "href": "ch/misc/workshopprep.html",
    "title": "Prepare for an upcoming workshop",
    "section": "",
    "text": "Hi there! If you are reading this, you will most likely soon participate in a workshop given by me. Thank you - I am already looking forward to it! Here are some tips to help you prepare."
  },
  {
    "objectID": "ch/misc/workshopprep.html#r-packages",
    "href": "ch/misc/workshopprep.html#r-packages",
    "title": "Prepare for an upcoming workshop",
    "section": "R-packages",
    "text": "R-packages\nDuring the workshop you will get to know and install many additional R-packages that are not automatically installed. In most cases, you can just install them the moment they are introduced. However, if you e.g. have a slow internet connection, it may be worthwhile to install them beforehand. You can install most of the packages we need by running the following code:\n\nif (!require(\"pacman\", quietly = TRUE))\n  install.packages(\"pacman\")\n\npacman::p_load(\n  agridat,\n  broom,\n  conflicted,\n  desplot,\n  dlookr,\n  emmeans,\n  ggtext,\n  glmmTMB,\n  here,\n  janitor,\n  lme4,\n  lmerTest,\n  modelbased,\n  multcomp,\n  multcompView,\n  naniar,\n  nlme,\n  openxlsx,\n  performance,\n  readxl,\n  scales,\n  tidyverse\n)"
  },
  {
    "objectID": "ch/misc/workshopprep.html#having-two-screens-helps",
    "href": "ch/misc/workshopprep.html#having-two-screens-helps",
    "title": "Prepare for an upcoming workshop",
    "section": "Having two screens helps!",
    "text": "Having two screens helps!\nDuring the workshop, I will constantly share my screen with you. Depending on whether you have multiple screens available or not, there are three main scenarios in my experience - ranked from worst to best:\n\nYou only have a single screen\nWhile this is the worst scenario, you obviously can still participate and will have no problem following me during the workshop. What may become difficult, however, is if you want to both look at my shared screen and also write your own R code simulatenously. If you only have only a single screen (that is not super wide), this means constantly switching between zoom and R. Nevertheless, there will be exercises during the workshop where everone has time to do an assignment on their own so that during those time you will definitely get some undivided R time.\nYou have a two screens, but they are not connected to the same deviceAn example for such a scenario would be having a computer with one screen and a laptop/tablet next to it. You could open R on one device and zoom on the other. This is certainly better than having only a single screen, since it is much easier to simultaneously read my and write your code. It is not optimal, however, since it does not allow for the advantages (i) and (ii) listed for scenario 3 below.\nYou have two screensHaving a computer/laptop with multiple screens connected is the optimal setup! You can easily look at both my and your R code and additionally (i) share your screen via zoom so that we can fix an issue you are having in R and (ii) copy-paste things from the zoom-chat into R."
  },
  {
    "objectID": "ch/misc/workshopprep.html#do-i-need-to-install-zoom",
    "href": "ch/misc/workshopprep.html#do-i-need-to-install-zoom",
    "title": "Prepare for an upcoming workshop",
    "section": "Do I need to install zoom?",
    "text": "Do I need to install zoom?\n(Obviously, this section is only relevant if the workshop will be held via zoom and not some other video communication software.)\nNo, not necessarily. It is not required to install the zoom software in order to participate in a zoom meeting, because you can also join from your browser (see details here and here). Basically, there will be a link that says “Join from your browser” and that’s it. However, be aware that there are some minor functions not available to you if you are joining from your browser."
  },
  {
    "objectID": "ch/misc/workshopprep.html#what-else",
    "href": "ch/misc/workshopprep.html#what-else",
    "title": "Prepare for an upcoming workshop",
    "section": "What else?",
    "text": "What else?\n\nCheck your microphoneMake sure we can hear you when asking a question.\nCheck your camera\nThis is optional of course, but I prefer participants turning their camera on as seeing your reactions improves the workshop.\nGet familiar with the options/functionsKnow how to mute your microphone. It should be muted at all times except when you are actually talking to us.\nKnow how to use the zoom chat.\nKnow how to use non-verbal feedback/meeting reactions in zoom (i.e. thumbs up etc.)."
  },
  {
    "objectID": "ch/rbasics/baddata.html",
    "href": "ch/rbasics/baddata.html",
    "title": "Bad data & Outliers",
    "section": "",
    "text": "# (install &) load packages\npacman::p_load(\n  broom,\n  conflicted,\n  here,\n  janitor,\n  naniar,\n  readxl,\n  tidyverse\n)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)\nThere are two download links:"
  },
  {
    "objectID": "ch/rbasics/baddata.html#import",
    "href": "ch/rbasics/baddata.html#import",
    "title": "Bad data & Outliers",
    "section": "Import",
    "text": "Import\nAssuming you are working in a R-project, save the formatted file somewhere within the project directory. I have saved it within a sub folder called data so that the relative path to my file is data/vision_fixed.xls.\n\npath &lt;- here(\"data\", \"vision_fixed.xls\")\ndat &lt;- read_excel(path)\n\ndat\n\n# A tibble: 29 × 9\n   Person   Ages Gender `Civil state` Height Profession Vision Distance PercDist\n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andrés     25 M      S                180 Student        10      1.5     15  \n 2 Anja       29 F      S                168 Professio…     10      4.5     45  \n 3 Armando    31 M      S                169 Professio…      9      4.5     50  \n 4 Carlos     25 M      M                185 Professio…      8      6       75  \n 5 Cristi…    23 F      &lt;NA&gt;             170 Student        10      3       30  \n 6 Delfa      39 F      M                158 Professio…      6      4.5     75  \n 7 Eduardo    28 M      S                166 Professio…      8      4.5     56.2\n 8 Enrique    NA &lt;NA&gt;   &lt;NA&gt;              NA Professio…     NA      6       NA  \n 9 Fanny      25 F      M                164 Student         9      3       33.3\n10 Franci…    46 M      M                168 Professio…      8      4.5     56.2\n# ℹ 19 more rows\n\n\nThis is optional, but we could argue that our column names are not in a desirable format. To deal with this, we can use the clean_names() functions of {janitor}. This package has several more handy functions for cleaning data that are worth checking out.\n\ndat &lt;- dat %&gt;% clean_names()\ndat\n\n# A tibble: 29 × 9\n   person    ages gender civil_state height profession vision distance perc_dist\n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Andrés      25 M      S              180 Student        10      1.5      15  \n 2 Anja        29 F      S              168 Professio…     10      4.5      45  \n 3 Armando     31 M      S              169 Professio…      9      4.5      50  \n 4 Carlos      25 M      M              185 Professio…      8      6        75  \n 5 Cristina    23 F      &lt;NA&gt;           170 Student        10      3        30  \n 6 Delfa       39 F      M              158 Professio…      6      4.5      75  \n 7 Eduardo     28 M      S              166 Professio…      8      4.5      56.2\n 8 Enrique     NA &lt;NA&gt;   &lt;NA&gt;            NA Professio…     NA      6        NA  \n 9 Fanny       25 F      M              164 Student         9      3        33.3\n10 Francis…    46 M      M              168 Professio…      8      4.5      56.2\n# ℹ 19 more rows"
  },
  {
    "objectID": "ch/rbasics/baddata.html#goal",
    "href": "ch/rbasics/baddata.html#goal",
    "title": "Bad data & Outliers",
    "section": "Goal",
    "text": "Goal\nVery much like in the previous chapter, our goal is to look at the relationship of two numeric variables: ages and vision. What is new about this data is, that it (i) has missing values and (ii) has a potential outlier."
  },
  {
    "objectID": "ch/rbasics/baddata.html#exploring",
    "href": "ch/rbasics/baddata.html#exploring",
    "title": "Bad data & Outliers",
    "section": "Exploring",
    "text": "Exploring\nTo quickly get a first feeling for this dataset, we can use summary() and draw a plot via plot() or ggplot().\n\nsummary(dat)\n\n    person               ages          gender          civil_state       \n Length:29          Min.   :22.00   Length:29          Length:29         \n Class :character   1st Qu.:25.00   Class :character   Class :character  \n Mode  :character   Median :26.00   Mode  :character   Mode  :character  \n                    Mean   :30.61                                        \n                    3rd Qu.:29.50                                        \n                    Max.   :55.00                                        \n                    NA's   :1                                            \n     height       profession            vision          distance    \n Min.   :145.0   Length:29          Min.   : 3.000   Min.   :1.500  \n 1st Qu.:164.8   Class :character   1st Qu.: 7.000   1st Qu.:1.500  \n Median :168.0   Mode  :character   Median : 9.000   Median :3.000  \n Mean   :168.2                      Mean   : 8.357   Mean   :3.466  \n 3rd Qu.:172.8                      3rd Qu.:10.000   3rd Qu.:4.500  \n Max.   :190.0                      Max.   :10.000   Max.   :6.000  \n NA's   :1                          NA's   :1                       \n   perc_dist     \n Min.   : 15.00  \n 1st Qu.: 20.24  \n Median : 40.18  \n Mean   : 45.45  \n 3rd Qu.: 57.19  \n Max.   :150.00  \n NA's   :1       \n\n\n\n\n\nClick to show/hide codeplot(y = dat$vision, x = dat$ages)\n\n\n\n\n\n\n\n\nClick to show/hide codeggplot(data = dat) +\n  aes(x = ages, y = vision) +\n  geom_point(size = 2) +\n  scale_x_continuous(\n    name = \"Person's age\",\n    limits = c(20, 60),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_y_continuous(\n    name = \"Person's vision\",\n    limits = c(0, NA),\n    breaks = seq(0, 10, 2),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n    theme_bw()\n\n\n\n\n\n\nApparently, most people are in their 20s and can see quite well, however some people are older and they tend to have a vision that’s a little worse."
  },
  {
    "objectID": "ch/rbasics/baddata.html#step-1-investigate",
    "href": "ch/rbasics/baddata.html#step-1-investigate",
    "title": "Bad data & Outliers",
    "section": "Step 1: Investigate",
    "text": "Step 1: Investigate\nIn such a scenario, the first thing you should do is find out more about this suspicious data point. In our case, we would start by finding out the person’s name. One way of doing this is by simply filtering the data:\n\ndat %&gt;% \n  filter(vision == 3)\n\n# A tibble: 1 × 9\n  person   ages gender civil_state height profession   vision distance perc_dist\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Rolando    26 M      M              180 Professional      3      4.5       150\n\n\nWe find that it was 26 year old Rolando who supposedly had a vision score of only 3."
  },
  {
    "objectID": "ch/rbasics/baddata.html#step-2-act",
    "href": "ch/rbasics/baddata.html#step-2-act",
    "title": "Bad data & Outliers",
    "section": "Step 2: Act",
    "text": "Step 2: Act\nSince we pretend it is you who collected the data, you should now\n\nthink back if you can actually remember Rolando and if he had poor vision and/or\nfind other documents such as your handwritten sheets to verify this number and make sure you did not make any typos transferring the data to your computer.\n\nThis may reaffirm or correct the suspicious data point and thus end the discussion on whether it is an outlier that should be removed from the data. However, you may also decide to delete this value. Yet, it must be realized, that deleting one or multiple values from a dataset almost always affects the results from subsequent statistical analyses - especially if the values stick out from the rest."
  },
  {
    "objectID": "ch/rbasics/baddata.html#r²---coeff.-of-det.",
    "href": "ch/rbasics/baddata.html#r²---coeff.-of-det.",
    "title": "Bad data & Outliers",
    "section": "R² - Coeff. of det.",
    "text": "R² - Coeff. of det.\nNevertheless, it is clear that the red line has a much better fit to the remaining data points, than the green line has - simply because Rolando’s data point sticks out so much. One way of measuring how well a regression fits the data is by calculating the coefficient of determination \\(R^2\\), which measures the proportion of total variation in the data explained by the model and can thus range from 0 (=bad) to 1 (=good). It can easily obtained via glance(), which is another function from {broom}:\n\n\n\nglance(reg)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.247         0.218  1.54      8.54 0.00709     1  -50.9  108.  112.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\n\nglance(reg_noRo)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.485         0.464  1.04      23.5 0.0000548     1  -38.4  82.7  86.6\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nFinally, we find that removing Rolando from the dataset increased the \\(R^2\\) for the simple linear regression from 25% to 49%."
  },
  {
    "objectID": "ch/rbasics/correlation_regression.html",
    "href": "ch/rbasics/correlation_regression.html",
    "title": "Correlation & Regression",
    "section": "",
    "text": "This chapter is trying to give you a feeling for what correlation and (simple linear) regression is. I am aware that the example data doesn’t have anything to do with agriculture or related fields, but I decided to keep it because it allows for an intuitive conclusion at the end.\n# (install &) load packages\npacman::p_load(\n  broom,\n  conflicted,\n  modelbased,\n  tidyverse)\n\n# handle function conflicts\nconflicts_prefer(dplyr::filter) \nconflicts_prefer(dplyr::select)"
  },
  {
    "objectID": "ch/rbasics/correlation_regression.html#import",
    "href": "ch/rbasics/correlation_regression.html#import",
    "title": "Correlation & Regression",
    "section": "Import",
    "text": "Import\n\n\n# data is available online:\npath &lt;- \"https://raw.githubusercontent.com/SchmidtPaul/dsfair_quarto/master/data/DrinksPeterMax.csv\"\n\n\n\ndat &lt;- read_csv(path) # use path from above\ndat\n\n# A tibble: 20 × 3\n   Person drinks blood_alc\n   &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Max         1       0.2\n 2 Max         2       0.3\n 3 Max         3       0.5\n 4 Max         3       0.6\n 5 Max         4       0.6\n 6 Max         4       0.5\n 7 Max         4       0.7\n 8 Max         5       0.6\n 9 Max         7       0.8\n10 Max         8       1  \n11 Peter       1       0.1\n12 Peter       1       0.1\n13 Peter       1       0.2\n14 Peter       1       0.2\n15 Peter       1       0.1\n16 Peter       3       0.3\n17 Peter       5       0.5\n18 Peter       6       0.8\n19 Peter       8       0.9\n20 Peter       9       1.3"
  },
  {
    "objectID": "ch/rbasics/correlation_regression.html#goal",
    "href": "ch/rbasics/correlation_regression.html#goal",
    "title": "Correlation & Regression",
    "section": "Goal",
    "text": "Goal\nThe goal of this analysis is to answer the question how the number of drinks relates to the blood alcohol level. Note that we can ignore the column Person, since we do not care whether data came from Peter or Max. Thus, we only focus on the two numeric columns drinks and blood_alc. For them, we will do a correlation and a regression analysis."
  },
  {
    "objectID": "ch/rbasics/correlation_regression.html#exploring",
    "href": "ch/rbasics/correlation_regression.html#exploring",
    "title": "Correlation & Regression",
    "section": "Exploring",
    "text": "Exploring\nTo quickly get a first feeling for this dataset, we can use summary() and draw a plot via plot() or ggplot().\n\nsummary(dat)\n\n    Person              drinks       blood_alc    \n Length:20          Min.   :1.00   Min.   :0.100  \n Class :character   1st Qu.:1.00   1st Qu.:0.200  \n Mode  :character   Median :3.50   Median :0.500  \n                    Mean   :3.85   Mean   :0.515  \n                    3rd Qu.:5.25   3rd Qu.:0.725  \n                    Max.   :9.00   Max.   :1.300  \n\n\n\n\n\nClick to show/hide codeplot(y = dat$blood_alc, x = dat$drinks)\n\n\n\n\n\n\n\n\nClick to show/hide codeggplot(data = dat) +\n  aes(x = drinks, y = blood_alc) +\n  geom_point(size = 2) +\n  scale_x_continuous(\n    name = \"Number of drinks\",\n    limits = c(0, 9),\n    breaks = seq(0, 9),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_y_continuous(\n    name = \"Blood alcohol content\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n    theme_classic()\n\n\n\n\n\n\nApparently, the number of drinks ranges from 1 to 9 with a mean of 3.85, while the measured blood alcohol levels range from 0.1 to 1.3 with a mean of 0.515. The plots show a clear trend of increasing blood alcohol levels with a higher number of drinks - which is what we would expect."
  },
  {
    "objectID": "ch/rbasics/correlation_regression.html#get-it",
    "href": "ch/rbasics/correlation_regression.html#get-it",
    "title": "Correlation & Regression",
    "section": "Get it",
    "text": "Get it\nIf you only want to get the actual correlation estimate, you can use the function cor() and provide the two numeric variables (as vectors):\n\ncor(dat$drinks, dat$blood_alc)\n\n[1] 0.9559151\n\n\nSo the correlation between number of drinks and blood alcohol content in our sample is ca. 0.96 and thus very strong, since it is almost 1."
  },
  {
    "objectID": "ch/rbasics/correlation_regression.html#test-it",
    "href": "ch/rbasics/correlation_regression.html#test-it",
    "title": "Correlation & Regression",
    "section": "Test it",
    "text": "Test it\nIf you would like additional information, such as a confidence interval and a test resulting in a p-value, you can use cor.test() instead of cor(). We may also use the {broom} package to get the results in a more convenient format.\n\n\n\nmycor &lt;- cor.test(dat$drinks, \n                  dat$blood_alc)\nmycor\n\n\n    Pearson's product-moment correlation\n\ndata:  dat$drinks and dat$blood_alc\nt = 13.811, df = 18, p-value = 5.089e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8897837 0.9827293\nsample estimates:\n      cor \n0.9559151 \n\n\n\n\n\n\ntidy(mycor)\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1    0.956      13.8 5.09e-11        18    0.890     0.983 Pearson'… two.sided  \n\n\n\n\nLooking at this longer output, you can see the sample estimate at the bottom, a confidence interval above it and a p-value with the corresponding test hypothesis above that. Run ?cor.test() and look at the “Details” section for more info. Here, our correlation estimate of 0.96 is significantly different from 0, since the p-value is 0.0000000000509 and therefore \\(&lt; 0.05\\). Furthermore, the confidence interval is 0.890 - 0.983 meaning that we are 95% sure that the true correlation is somewhere in that range.\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\n{correlation}\n{corrr}\n{ggcorrplot}"
  },
  {
    "objectID": "ch/rbasics/correlation_regression.html#get-it-1",
    "href": "ch/rbasics/correlation_regression.html#get-it-1",
    "title": "Correlation & Regression",
    "section": "Get it",
    "text": "Get it\nIn R, we can use the lm() function for fitting linear models so that it fits the simple linear regression equation shown above easily:\n\nreg &lt;- lm(formula = blood_alc ~ drinks,\n          data = dat)\n\nAs you can see, we refer to our data object dat in the data = argument so that in the formula = argument we only need to write the names of the respective columns in dat. Furthermore, we store the results in the reg object. When looking at this object, we get the following results:\n\nreg\n\n\nCall:\nlm(formula = blood_alc ~ drinks, data = dat)\n\nCoefficients:\n(Intercept)       drinks  \n    0.04896      0.12105  \n\n\nFirst, our command is repeated and then the “Coefficients” are shown, which are indeed the estimates for \\(a\\) and \\(b\\). So the best straight line is:\n\\[ bloodalc = 0.049 + 0.121 * drinks \\]\nwhich looks like this:\n\nClick to show/hide codeggplot(data = dat) +\n  aes(x = drinks, y = blood_alc) +\n  geom_point(size = 2) +\n  geom_smooth(\n    method = lm,\n    formula = y ~ x,\n    se = FALSE,\n    fullrange = TRUE,\n    color = \"#00923f\"\n  ) +\n  scale_x_continuous(\n    name = \"Number of drinks\",\n    limits = c(0, 9),\n    breaks = seq(0, 9),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_y_continuous(\n    name = \"Blood alcohol content\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\nHere is a little more info why formula = blood_alc ~ drinks leads to R estimating the \\(a\\) and \\(b\\) we want: What makes sense is that blood_alc is \\(y\\), drinks is \\(x\\) and ~ would therefore be the \\(=\\) in our equation. However, why is it we never had to write anything about \\(a\\) or \\(b\\)? The answer is that (i) when fitting a linear model, there is usually always an intercept (=\\(a\\)) by default and (ii) when writing a numeric variable (=drinks) as on the right side of the equation, it will automatically be assumed to have a slope (=\\(b\\)) multiplied with it. Accordingly, blood_alc ~ drinks automatically translates to blood_alc = a + b*drinks so to speak."
  },
  {
    "objectID": "ch/rbasics/correlation_regression.html#is-this-right",
    "href": "ch/rbasics/correlation_regression.html#is-this-right",
    "title": "Correlation & Regression",
    "section": "Is this right?",
    "text": "Is this right?\nAfter fitting a model, you may use it to make predictions. Here is one way of obtaining the expected blood alcohol content for having 0 to 9 drinks according to our simple linear regression via {modelbased}:\n\npreddat &lt;- tibble(drinks = seq(0, 9)) %&gt;% \n  estimate_expectation(model = reg) %&gt;% \n  as_tibble()\n\npreddat\n\n# A tibble: 10 × 5\n   drinks Predicted     SE  CI_low CI_high\n    &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1      0    0.0490 0.0406 -0.0363   0.134\n 2      1    0.170  0.0337  0.0993   0.241\n 3      2    0.291  0.0278  0.233    0.349\n 4      3    0.412  0.0238  0.362    0.462\n 5      4    0.533  0.0226  0.486    0.581\n 6      5    0.654  0.0247  0.602    0.706\n 7      6    0.775  0.0294  0.713    0.837\n 8      7    0.896  0.0357  0.821    0.971\n 9      8    1.02   0.0428  0.927    1.11 \n10      9    1.14   0.0505  1.03     1.24 \n\n\nYou may notice that according to our model, the expected alcohol content in your blood when having 0 drinks is actually 0.049 and thus larger than 0. This is obviously not true in real life. Instead, the true intercept should actually be exactly 0, so what went wrong?\nFirst of all, data will never be perfect in the sense that the when a parameter really is e.g. 42, its estimate based on measured data is also exactly 42.000000… . Instead, there are e.g. measurement errors: Peter and Max may have forgotten a drink or two or their device to measure the alcohol content is not precise enough. In fact, this would most likely be the underlying reason here - but remember that I made the data up.\nSo I would like you to think about the issue from two other angles:\n\nAre the results really saying the intercept is &gt; 0?\nDid we even ask the right question or should we have fitted a different model?\n\nAre the results really saying the intercept is &gt; 0?\nNo, they are not. Yes, the sample estimate for the intercept is 0.049, but when looking at more detailed information via e.g. summary(). We may also use the {broom} package to get the results in a more convenient format.\n\ntidy(reg, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.0490   0.0406       1.21 2.43e- 1  -0.0363     0.134\n2 drinks        0.121    0.00876     13.8  5.09e-11   0.103      0.139\n\n\nyou can see that the p-value for the intercept is 0.243, which is larger than 0.05 and thus saying that we could not find the intercept to be significantly different from 0. A second indication can be found when looking at the confidence interval of the expected value for having 0 drinks in the table above: [-0.0363, 0.1340]. This interval actually includes 0 which suggests that the true expected blood alcohol content for having 0 drinks may indeed be 0.\nShould we have fitted a different model?\nWe certainly could have and we will actually do it now. It must be clear that statistically speaking there was nothing wrong with our analysis. However, from a biological standpoint or in other words - because of our background knowledge and expertise as scientists - we could have indeed actively decided for a regression analysis that does not have an intercept and is thus forced to start 0 in terms of blood alcohol content. After all, statistics is just a tool to help us make conclusions. It is a powerful tool, but it will always be our responsibility to “ask the right questions” i.e. apply expedient methods.\nA simple linear regression without an intercept is strictly speaking no longer “simple”, since it no longer has the typical equation, but instead this one:\n\\[ y = \\beta x\\]\nTo tell lm() that it should not estimate the default intercept, we simply add 0 + right after the ~. As expected, we only get one estimate for the slope:\n\nreg_noint &lt;- lm(formula = blood_alc ~ 0 + drinks, data = dat)\nreg_noint\n\n\nCall:\nlm(formula = blood_alc ~ 0 + drinks, data = dat)\n\nCoefficients:\ndrinks  \n0.1298  \n\n\nmeaning that this regression with no intercept is estimated as\n\\[ bloodalc = 0.1298 * drinks \\]\nand must definitely predict 0 blood_alc when having 0 drinks. As a final result, we can compare both regression lines visually in a ggplot:\n\nClick to show/hide codeggplot(data = dat) + \n  aes(x = drinks, y = blood_alc) +\n  geom_point(size = 2) +\n  geom_smooth(\n    method = lm,\n    formula = y ~ x,\n    se = FALSE,\n    fullrange = TRUE,\n    color = \"#00923f\"\n  ) +\n    geom_smooth(\n    method = lm,\n    formula = y ~ 0 + x,\n    se = FALSE,\n    fullrange = TRUE,\n    color = \"#e4572e\"\n  ) +\n  scale_x_continuous(\n    name = \"Number of drinks\",\n    limits = c(0, 9),\n    breaks = seq(0, 9),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  scale_y_continuous(\n    name = \"Blood alcohol content\",\n    limits = c(0, NA),\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  theme_classic()"
  },
  {
    "objectID": "ch/rbasics/firststeps.html",
    "href": "ch/rbasics/firststeps.html",
    "title": "First steps in R",
    "section": "",
    "text": "This chapter is mostly aimed at people who are very new to R. However, people who do know R may still find useful insights from the sections where I emphasize how I use R.\nFurthermore, this tutorial teaches R the way I use it, which means you can do (and may have done) almost everything I do here with other code/functions/approaches. “The way I use it” mostly refers to me using the {tidyverse}, which “is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures” and has become quite popular over the last years. Here is a direct comparison of how to do things in R with base R and via the tidyverse."
  },
  {
    "objectID": "ch/rbasics/firststeps.html#base-r",
    "href": "ch/rbasics/firststeps.html#base-r",
    "title": "First steps in R",
    "section": "base R",
    "text": "base R\nAfter installing R there are many functions etc. you can use right away - which is what we did above. For example, when running ?mean, the help page will tell you that this function is part of the {base} package. As the name suggests, this package is built-in and its functions are ready to use the moment you have installed R. You can verify this by going to the “Packages” tab in RStudio - you will find the base package and it will have a checked box next to it."
  },
  {
    "objectID": "ch/rbasics/firststeps.html#loading-packages",
    "href": "ch/rbasics/firststeps.html#loading-packages",
    "title": "First steps in R",
    "section": "loading packages",
    "text": "loading packages\nWhen looking at the “Packages” tab in RStudio you may notice that some packages are listed, but do not have a check mark in the box next to them. These are packages that are installed, but not loaded. When a package is not loaded, its functions cannot be used. In order to load a package, the default command is library(package_name). This command must be run once every time you open a new R session."
  },
  {
    "objectID": "ch/rbasics/firststeps.html#installing-additional-packages",
    "href": "ch/rbasics/firststeps.html#installing-additional-packages",
    "title": "First steps in R",
    "section": "installing additional packages",
    "text": "installing additional packages\nR really shines because of the ability to install additional packages from external sources. Basically, anyone can create a function, put it in a package and make it available online. Some packages are very sophisticated and popular - e.g. the package ggplot2, which is not built-in, has been downloaded 75 million times. In order to install a package, the default command is install.packages(package_name). Alternatively, you can also click on the “Install” button in the top left of the “Packages” tab and type in the package_name there.\n\n\n\n\n\n\nNote\n\n\n\nA package only needs to be installed once, but\nA package must be loaded every time you open a new R session!\n\n\nHere is a curated list of R packages and tools for different areas."
  },
  {
    "objectID": "ch/rbasics/firststeps.html#footnotes",
    "href": "ch/rbasics/firststeps.html#footnotes",
    "title": "First steps in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nAs long as you are dealing with numbers and calculations. Spaces do matter when we are talking about strings (i.e. text).↩︎"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html",
    "href": "ch/rbasics/tidyverse.html",
    "title": "The tidyverse",
    "section": "",
    "text": "When using R, you will sooner or later hear about the {tidyverse}. The tidyverse is a collection of R packages that “share an underlying design philosophy, grammar, and data structures” of tidy data. The individual tidyverse packages comprise some of the most downloaded R packages.\nInstall the complete tidyverse with:\ninstall.packages(\"tidyverse\")\n# or\npacman::p_load(\"tidyverse\")\nI did not use the tidyverse packages in my first years using R, but I wish I did. While you can often reach your goal with or without using the tidyverse packages, I personally prefer using them. Thus, they are used extensively throughout the chapters of this website.\nDuring the next sections I will try to explain how to use some of these packages and sometimes compare them to the Base R (= non-tidyverse) alternative."
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#data.frame",
    "href": "ch/rbasics/tidyverse.html#data.frame",
    "title": "The tidyverse",
    "section": "data.frame",
    "text": "data.frame\nBase R has a standard format for data tables called data.frame. Here is an example table that is an R built-in, just like pi is - it is called PlantGrowth:\n\nPlantGrowth\n\n   weight group\n1    4.17  ctrl\n2    5.58  ctrl\n3    5.18  ctrl\n4    6.11  ctrl\n5    4.50  ctrl\n6    4.61  ctrl\n7    5.17  ctrl\n8    4.53  ctrl\n9    5.33  ctrl\n10   5.14  ctrl\n11   4.81  trt1\n12   4.17  trt1\n13   4.41  trt1\n14   3.59  trt1\n15   5.87  trt1\n16   3.83  trt1\n17   6.03  trt1\n18   4.89  trt1\n19   4.32  trt1\n20   4.69  trt1\n21   6.31  trt2\n22   5.12  trt2\n23   5.54  trt2\n24   5.50  trt2\n25   5.37  trt2\n26   5.29  trt2\n27   4.92  trt2\n28   6.15  trt2\n29   5.80  trt2\n30   5.26  trt2\n\n\nLet us create a copy of this table called df (dataframe) and then use some helpful functions to get a first impression of this data:\n\ndf &lt;- PlantGrowth\nstr(df)\n\n'data.frame':   30 obs. of  2 variables:\n $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(df)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\n\nWe can see that this dataset has 30 observations (=rows) and 2 variables (=columns) and is of the type “data.frame”. Furthermore, the first variable is called weight and contains numeric values for which we get some measures of central tendency like the minimum, maximum, mean and median. The second variable is called group and is of the type factor containing a total of three different levels, which each appear 10 times.\nIf you want to extract/use values of only one column of such a data.frame, you write the name of the data.frame, then a $ and finally the name of the respective column. It returns the values of that column as a vector:\n\ndf$weight\n\n [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n[16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\n\ndf$group\n\n [1] ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl trt1 trt1 trt1 trt1 trt1\n[16] trt1 trt1 trt1 trt1 trt1 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2\nLevels: ctrl trt1 trt2"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#tibble",
    "href": "ch/rbasics/tidyverse.html#tibble",
    "title": "The tidyverse",
    "section": "tibble",
    "text": "tibble\nOne major aspect of the tidyverse is formatting tables as tibble instead of data.frame. A tibble “is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not.” It is super simple to convert a data.frame into a tibble, but you must have the tidyverse R package {tibble} installed and loaded - which it is if you are loading the entire {tidyverse}. Let us convert our df into a tibble and call it tbl:\n\npacman::p_load(tidyverse)\ntbl &lt;- as_tibble(df)\ntbl\n\n# A tibble: 30 × 2\n   weight group\n    &lt;dbl&gt; &lt;fct&gt;\n 1   4.17 ctrl \n 2   5.58 ctrl \n 3   5.18 ctrl \n 4   6.11 ctrl \n 5   4.5  ctrl \n 6   4.61 ctrl \n 7   5.17 ctrl \n 8   4.53 ctrl \n 9   5.33 ctrl \n10   5.14 ctrl \n# ℹ 20 more rows\n\n\nOf course, the data itself does not change - only its format and the way it is displayed to us in R. If you compare the output we get from printing tbl here to that of printing df above, I would like to point out some things I find extremely convenient for tibbles:\n\nThere is an extra first line telling us about the number of rows and columns.\nThere is an extra line below the column names telling us about the data type of each column.\nOnly the first ten rows of data are printed and a “… with 20 more rows” is added below.\nIt can’t be seen here, but this would analogously happen if there were too many columns.\nIt can’t be seen here, but missing values NA and negative numbers are printed in red.\n\nFinally, note that in its heart, a tibble is still a data.frame and in most cases you can do everything with a tibble that you can do with a data.frame:\n\n\n\nclass(tbl)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nstr(tbl)\n\ntibble [30 × 2] (S3: tbl_df/tbl/data.frame)\n $ weight: num [1:30] 4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(tbl)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\ntbl$weight\n\n [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n[16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\n\ntbl$group\n\n [1] ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl trt1 trt1 trt1 trt1 trt1\n[16] trt1 trt1 trt1 trt1 trt1 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2\nLevels: ctrl trt1 trt2\n\n\n\n\n\n\nclass(df)\n\n[1] \"data.frame\"\n\nstr(df)\n\n'data.frame':   30 obs. of  2 variables:\n $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(df)\n\n     weight       group   \n Min.   :3.590   ctrl:10  \n 1st Qu.:4.550   trt1:10  \n Median :5.155   trt2:10  \n Mean   :5.073            \n 3rd Qu.:5.530            \n Max.   :6.310            \n\ndf$weight\n\n [1] 4.17 5.58 5.18 6.11 4.50 4.61 5.17 4.53 5.33 5.14 4.81 4.17 4.41 3.59 5.87\n[16] 3.83 6.03 4.89 4.32 4.69 6.31 5.12 5.54 5.50 5.37 5.29 4.92 6.15 5.80 5.26\n\ndf$group\n\n [1] ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl ctrl trt1 trt1 trt1 trt1 trt1\n[16] trt1 trt1 trt1 trt1 trt1 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2 trt2\nLevels: ctrl trt1 trt2\n\n\n\n\nTherefore, I almost always format my datasets as tibbles."
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#no-pipe---intermediate-steps",
    "href": "ch/rbasics/tidyverse.html#no-pipe---intermediate-steps",
    "title": "The tidyverse",
    "section": "No pipe - intermediate steps",
    "text": "No pipe - intermediate steps\nUsing one function at a time and saving the output in the variables a - d, we can do this:\n\na &lt;- filter(PlantGrowth, group == \"ctrl\")\nb &lt;- pull(a, weight) # same as: b &lt;- a$weight\nc &lt;- sqrt(b)\nd &lt;- round(c, digits = 1)\nsort(d)\n\n [1] 2.0 2.1 2.1 2.1 2.3 2.3 2.3 2.3 2.4 2.5"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#no-pipe---nesting-functions",
    "href": "ch/rbasics/tidyverse.html#no-pipe---nesting-functions",
    "title": "The tidyverse",
    "section": "No pipe - nesting functions",
    "text": "No pipe - nesting functions\nJust like in MS Excel, it is possible to write functions inside of functions so that we can do this:\n\nsort(round(sqrt(pull(filter(PlantGrowth, group == \"ctrl\"), weight)), digits = 1))\n\n [1] 2.0 2.1 2.1 2.1 2.3 2.3 2.3 2.3 2.4 2.5"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#pipe",
    "href": "ch/rbasics/tidyverse.html#pipe",
    "title": "The tidyverse",
    "section": "Pipe!",
    "text": "Pipe!\nThis approach (i) allows you to write functions from left to right / top to bottom and thus in the order they are executed and the way you think about them and (ii) does not create extra variables for intermediate steps:\n\nPlantGrowth %&gt;% \n  filter(group == \"ctrl\") %&gt;% \n  pull(weight) %&gt;% \n  sqrt() %&gt;% \n  round(digits = 1) %&gt;% \n  sort()\n\n [1] 2.0 2.1 2.1 2.1 2.3 2.3 2.3 2.3 2.4 2.5\n\n\nYou can think about it like this: Something (in this case the PlantGrowth data.frame) goes into the pipe and is directed to the next function filter(). By default, this function takes what came out of the previous pipe and puts it as its first argument. This happens with every pipe. You’ll notice that all the functions who required two arguments above, now only need one argument, i.e. the additional argument, because the main argument stating which data is to be used is by default simply what came out of the previous pipe. Accordingly, the functions sqrt() and sort() appear empty here, because they only need one piece of information and that is which data they should work with. Finally note that you can easily highlight only some of the lines up until one of the pipes to see the intermediate results.\n\n\n\n\n\n\nNote\n\n\n\nThe keyboard shortcut for writing %&gt;% in RStudio is CTRL+SHIFT+M. Keyboard shortcuts can be customized in RStudio as described here."
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#mutate",
    "href": "ch/rbasics/tidyverse.html#mutate",
    "title": "The tidyverse",
    "section": "mutate()",
    "text": "mutate()\nThis function is useful whenever you want to change existing columns or add new columns to your table. To keep the following examples short and simple, let’s create tbl2 as only the first six rows of tbl via the head() function:\n\ntbl2 &lt;- head(tbl)\ntbl2\n\n# A tibble: 6 × 2\n  weight group\n   &lt;dbl&gt; &lt;fct&gt;\n1   4.17 ctrl \n2   5.58 ctrl \n3   5.18 ctrl \n4   6.11 ctrl \n5   4.5  ctrl \n6   4.61 ctrl \n\n\nLet’s start by adding 2 to the weight in our data. Below, we do this two different ways: by adding a column named new to the dataset (left) and by replacing/overwriting the original weight column (right):\n\n\n\ntbl2 %&gt;% \n  mutate(new = weight + 2)\n\n# A tibble: 6 × 3\n  weight group   new\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n1   4.17 ctrl   6.17\n2   5.58 ctrl   7.58\n3   5.18 ctrl   7.18\n4   6.11 ctrl   8.11\n5   4.5  ctrl   6.5 \n6   4.61 ctrl   6.61\n\n\n\n\n\n\ntbl2 %&gt;% \n  mutate(weight = weight + 2)\n\n# A tibble: 6 × 2\n  weight group\n   &lt;dbl&gt; &lt;fct&gt;\n1   6.17 ctrl \n2   7.58 ctrl \n3   7.18 ctrl \n4   8.11 ctrl \n5   6.5  ctrl \n6   6.61 ctrl \n\n\n\n\nWe can also create multiple columns at once (left) and make the values of the new column dynamically depend on the other columns via case_when() (right):\n\n\n\ntbl2 %&gt;%\n  mutate(\n    `Name with Space` = \"Hello!\",\n    number10 = 10\n  )\n\n# A tibble: 6 × 4\n  weight group `Name with Space` number10\n   &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;                &lt;dbl&gt;\n1   4.17 ctrl  Hello!                  10\n2   5.58 ctrl  Hello!                  10\n3   5.18 ctrl  Hello!                  10\n4   6.11 ctrl  Hello!                  10\n5   4.5  ctrl  Hello!                  10\n6   4.61 ctrl  Hello!                  10\n\n\n\n\n\n\ntbl2 %&gt;% \n  mutate(size = case_when(\n    weight &gt; 5.5 ~ \"large\",\n    weight &lt; 4.5 ~ \"small\",\n    TRUE ~ \"normal\" # everything else\n  ))\n\n# A tibble: 6 × 3\n  weight group size  \n   &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; \n1   4.17 ctrl  small \n2   5.58 ctrl  large \n3   5.18 ctrl  normal\n4   6.11 ctrl  large \n5   4.5  ctrl  normal\n6   4.61 ctrl  normal\n\n\n\n\nFinally, we can efficiently apply the same function to multiple columns at once via across(). We can select the columns e.g. manually via their names in a vector (left) or via a function such as is.numeric (right):\n\n\n\ntbl2 %&gt;%\n  mutate(v1 = 1, v2 = 2, v3 = 3) %&gt;%\n  mutate(\n    across(c(v1, v2), ~ .x + 20)\n    )\n\n# A tibble: 6 × 5\n  weight group    v1    v2    v3\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   4.17 ctrl     21    22     3\n2   5.58 ctrl     21    22     3\n3   5.18 ctrl     21    22     3\n4   6.11 ctrl     21    22     3\n5   4.5  ctrl     21    22     3\n6   4.61 ctrl     21    22     3\n\n\n\n\n\n\ntbl2 %&gt;%\n  mutate(v1 = 1, v2 = 2, v3 = 3) %&gt;%\n  mutate(\n    across(where(is.numeric), ~ .x + 20)\n    )\n\n# A tibble: 6 × 5\n  weight group    v1    v2    v3\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   24.2 ctrl     21    22    23\n2   25.6 ctrl     21    22    23\n3   25.2 ctrl     21    22    23\n4   26.1 ctrl     21    22    23\n5   24.5 ctrl     21    22    23\n6   24.6 ctrl     21    22    23\n\n\n\n\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\n\n5.5 Add new variables with mutate() in R for data science (Wickham and Grolemund 2017)\n\nCreate, modify, and delete columns with mutate()\nA general vectorised if with case_when()\nApply a function (or functions) across multiple columns with across()"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#select",
    "href": "ch/rbasics/tidyverse.html#select",
    "title": "The tidyverse",
    "section": "select()",
    "text": "select()\nThis function is useful whenever you want to select a subset of columns or change the order of columns. To provide better examples, let’s first create a table tbl3 with a few more columns:\n\ntbl3 &lt;- tbl2 %&gt;% \n  mutate(var1 = 1, var2 = 2, var3 = \"text\", var4 = \"word\")\n\ntbl3\n\n# A tibble: 6 × 6\n  weight group  var1  var2 var3  var4 \n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   4.17 ctrl      1     2 text  word \n2   5.58 ctrl      1     2 text  word \n3   5.18 ctrl      1     2 text  word \n4   6.11 ctrl      1     2 text  word \n5   4.5  ctrl      1     2 text  word \n6   4.61 ctrl      1     2 text  word \n\n\nWe can now select individual columns manually by giving all names (left) and even select all columns from:to by writing a colon between them (right):\n\n\n\ntbl3 %&gt;% \n  select(group, var1, var4)\n\n# A tibble: 6 × 3\n  group  var1 var4 \n  &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt;\n1 ctrl      1 word \n2 ctrl      1 word \n3 ctrl      1 word \n4 ctrl      1 word \n5 ctrl      1 word \n6 ctrl      1 word \n\n\n\n\n\n\ntbl3 %&gt;% \n  select(group, var1:var4)\n\n# A tibble: 6 × 5\n  group  var1  var2 var3  var4 \n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1 ctrl      1     2 text  word \n2 ctrl      1     2 text  word \n3 ctrl      1     2 text  word \n4 ctrl      1     2 text  word \n5 ctrl      1     2 text  word \n6 ctrl      1     2 text  word \n\n\n\n\nWe can also delete specific columns by putting a - in fornt of their name or use functions like starts_with(), ends_with(), contains(), matches() and num_range() to select all columns based on (parts of) their name:\n\n\n\ntbl3 %&gt;% \n  select(-group)\n\n# A tibble: 6 × 5\n  weight  var1  var2 var3  var4 \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   4.17     1     2 text  word \n2   5.58     1     2 text  word \n3   5.18     1     2 text  word \n4   6.11     1     2 text  word \n5   4.5      1     2 text  word \n6   4.61     1     2 text  word \n\n\n\n\n\n\ntbl3 %&gt;% \n  select(contains(\"r\"))\n\n# A tibble: 6 × 5\n  group  var1  var2 var3  var4 \n  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1 ctrl      1     2 text  word \n2 ctrl      1     2 text  word \n3 ctrl      1     2 text  word \n4 ctrl      1     2 text  word \n5 ctrl      1     2 text  word \n6 ctrl      1     2 text  word \n\n\n\n\nFinally, we can select based on a function like is.numeric via where() (left) or simply rearrange while keeping all columns by using everything() (right)\n\n\n\ntbl3 %&gt;% \n  select(where(is.numeric))\n\n# A tibble: 6 × 3\n  weight  var1  var2\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   4.17     1     2\n2   5.58     1     2\n3   5.18     1     2\n4   6.11     1     2\n5   4.5      1     2\n6   4.61     1     2\n\n\n\n\n\n\ntbl3 %&gt;% \n  select(var1, everything())\n\n# A tibble: 6 × 6\n   var1 weight group  var2 var3  var4 \n  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1   4.17 ctrl      2 text  word \n2     1   5.58 ctrl      2 text  word \n3     1   5.18 ctrl      2 text  word \n4     1   6.11 ctrl      2 text  word \n5     1   4.5  ctrl      2 text  word \n6     1   4.61 ctrl      2 text  word \n\n\n\n\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\n\n5.4 Select columns with select() in R for data science (Wickham and Grolemund 2017)\n\nSubset columns using their names and types with select()\nSelect variables that match a pattern with starts_with() etc.\nSelect variables with a function with where()"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#filter",
    "href": "ch/rbasics/tidyverse.html#filter",
    "title": "The tidyverse",
    "section": "filter()",
    "text": "filter()\nThis function is useful whenever you want to subset rows based on their values. Note that for the examples here, we use the original tbl with 30 observations.\nLet’s immediately filter for two conditions: Observations that belong to group trt2 and (&) are larger than 6 (left); Observations that are larger than 6 or (|) smaller than 4 (right):\n\n\n\ntbl %&gt;% \n  filter(weight &gt; 6 & group == \"trt2\")\n\n# A tibble: 2 × 2\n  weight group\n   &lt;dbl&gt; &lt;fct&gt;\n1   6.31 trt2 \n2   6.15 trt2 \n\n\n\n\n\n\ntbl %&gt;% \n  filter(weight &gt; 6 | weight &lt; 4)\n\n# A tibble: 6 × 2\n  weight group\n   &lt;dbl&gt; &lt;fct&gt;\n1   6.11 ctrl \n2   3.59 trt1 \n3   3.83 trt1 \n4   6.03 trt1 \n5   6.31 trt2 \n6   6.15 trt2 \n\n\n\n\nInstead of writing a lot of conditions separated by |, it is often more efficient to use %in%:\n\n\n\ntbl %&gt;% \n  filter(group == \"trt1\" | group == \"trt2\")\n\n# A tibble: 20 × 2\n   weight group\n    &lt;dbl&gt; &lt;fct&gt;\n 1   4.81 trt1 \n 2   4.17 trt1 \n 3   4.41 trt1 \n 4   3.59 trt1 \n 5   5.87 trt1 \n 6   3.83 trt1 \n 7   6.03 trt1 \n 8   4.89 trt1 \n 9   4.32 trt1 \n10   4.69 trt1 \n11   6.31 trt2 \n12   5.12 trt2 \n13   5.54 trt2 \n14   5.5  trt2 \n15   5.37 trt2 \n16   5.29 trt2 \n17   4.92 trt2 \n18   6.15 trt2 \n19   5.8  trt2 \n20   5.26 trt2 \n\n\n\n\n\n\ntbl %&gt;% \n  filter(group %in% c(\"trt1\", \"trt2\"))\n\n# A tibble: 20 × 2\n   weight group\n    &lt;dbl&gt; &lt;fct&gt;\n 1   4.81 trt1 \n 2   4.17 trt1 \n 3   4.41 trt1 \n 4   3.59 trt1 \n 5   5.87 trt1 \n 6   3.83 trt1 \n 7   6.03 trt1 \n 8   4.89 trt1 \n 9   4.32 trt1 \n10   4.69 trt1 \n11   6.31 trt2 \n12   5.12 trt2 \n13   5.54 trt2 \n14   5.5  trt2 \n15   5.37 trt2 \n16   5.29 trt2 \n17   4.92 trt2 \n18   6.15 trt2 \n19   5.8  trt2 \n20   5.26 trt2 \n\n\n\n\nWe can also filter for values that are not of the ctrl group (left) or that are larger than the mean weight (right):\n\n\n\ntbl %&gt;% \n  filter(group != \"ctrl\")\n\n# A tibble: 20 × 2\n   weight group\n    &lt;dbl&gt; &lt;fct&gt;\n 1   4.81 trt1 \n 2   4.17 trt1 \n 3   4.41 trt1 \n 4   3.59 trt1 \n 5   5.87 trt1 \n 6   3.83 trt1 \n 7   6.03 trt1 \n 8   4.89 trt1 \n 9   4.32 trt1 \n10   4.69 trt1 \n11   6.31 trt2 \n12   5.12 trt2 \n13   5.54 trt2 \n14   5.5  trt2 \n15   5.37 trt2 \n16   5.29 trt2 \n17   4.92 trt2 \n18   6.15 trt2 \n19   5.8  trt2 \n20   5.26 trt2 \n\n\n\n\n\n\ntbl %&gt;% \n  filter(weight &gt; mean(weight))\n\n# A tibble: 17 × 2\n   weight group\n    &lt;dbl&gt; &lt;fct&gt;\n 1   5.58 ctrl \n 2   5.18 ctrl \n 3   6.11 ctrl \n 4   5.17 ctrl \n 5   5.33 ctrl \n 6   5.14 ctrl \n 7   5.87 trt1 \n 8   6.03 trt1 \n 9   6.31 trt2 \n10   5.12 trt2 \n11   5.54 trt2 \n12   5.5  trt2 \n13   5.37 trt2 \n14   5.29 trt2 \n15   6.15 trt2 \n16   5.8  trt2 \n17   5.26 trt2 \n\n\n\n\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\n\n5.2 Filter rows with filter() in R for data science (Wickham and Grolemund 2017)\n\nSubset rows using column values with filter()"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#arrange",
    "href": "ch/rbasics/tidyverse.html#arrange",
    "title": "The tidyverse",
    "section": "arrange()",
    "text": "arrange()\nThis function is useful whenever you want to sort rows based on their values. We’ll once more create a new version of our original dataset to best show what this function can do:\n\ntbl4 &lt;- tbl %&gt;%\n  slice(1:3, 11:13, 21:23) \n# this keeps only rows 1,2,3,11,12,13,21,22,23\n\nWe can arrange rows via writing the column name (or column index/number). Note that by default values are sorted in ascending order and strings are sorted alphabetically, but this can be reversed by using desc():\n\n\n\ntbl4 %&gt;% \n  arrange(weight)\n\n# A tibble: 9 × 2\n  weight group\n   &lt;dbl&gt; &lt;fct&gt;\n1   4.17 ctrl \n2   4.17 trt1 \n3   4.41 trt1 \n4   4.81 trt1 \n5   5.12 trt2 \n6   5.18 ctrl \n7   5.54 trt2 \n8   5.58 ctrl \n9   6.31 trt2 \n\n\n\n\n\n\ntbl4 %&gt;% \n  arrange(desc(weight))\n\n# A tibble: 9 × 2\n  weight group\n   &lt;dbl&gt; &lt;fct&gt;\n1   6.31 trt2 \n2   5.58 ctrl \n3   5.54 trt2 \n4   5.18 ctrl \n5   5.12 trt2 \n6   4.81 trt1 \n7   4.41 trt1 \n8   4.17 ctrl \n9   4.17 trt1 \n\n\n\n\nYou can also sort via multiple columns and you can provide a custom sorting order in a vector:\n\n\n\ntbl4 %&gt;% \n  arrange(group, weight)\n\n# A tibble: 9 × 2\n  weight group\n   &lt;dbl&gt; &lt;fct&gt;\n1   4.17 ctrl \n2   5.18 ctrl \n3   5.58 ctrl \n4   4.17 trt1 \n5   4.41 trt1 \n6   4.81 trt1 \n7   5.12 trt2 \n8   5.54 trt2 \n9   6.31 trt2 \n\n\n\n\n\n\nmyorder &lt;- c(\"trt1\", \"ctrl\", \"trt2\")\n\ntbl4 %&gt;% \n  arrange(\n    match(group, myorder), \n    weight\n  )\n\n# A tibble: 9 × 2\n  weight group\n   &lt;dbl&gt; &lt;fct&gt;\n1   4.17 trt1 \n2   4.41 trt1 \n3   4.81 trt1 \n4   4.17 ctrl \n5   5.18 ctrl \n6   5.58 ctrl \n7   5.12 trt2 \n8   5.54 trt2 \n9   6.31 trt2 \n\n\n\n\nNote that NA (= missing values) are always sorted to the end3, even when using desc().\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\n\n5.3 Arrange rows with arrange() in R for data science (Wickham and Grolemund 2017)\n\nArrange rows by column values with arrange()\nHow to have NA’s displayed first using arrange()"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#summarise",
    "href": "ch/rbasics/tidyverse.html#summarise",
    "title": "The tidyverse",
    "section": "summarise()",
    "text": "summarise()\nThis function can be useful whenever you want to summarise data. Yet, it is not very useful (left) unless it is paired with group_by() (right).\n\n\n\ntbl %&gt;% \n  # no group_by \n  summarise(my_mean = mean(weight))\n\n# A tibble: 1 × 1\n  my_mean\n    &lt;dbl&gt;\n1    5.07\n\n\n\n\n\n\ntbl %&gt;% \n  group_by(group) %&gt;% \n  summarise(my_mean = mean(weight))\n\n# A tibble: 3 × 2\n  group my_mean\n  &lt;fct&gt;   &lt;dbl&gt;\n1 ctrl     5.03\n2 trt1     4.66\n3 trt2     5.53\n\n\n\n\nYou can create multiple summary output columns (left) and have multiple grouping columns (right):\n\n\n\ntbl %&gt;% \n  group_by(group) %&gt;% \n  summarise(\n    Mean = mean(weight),\n    StdDev = sd(weight),\n    Min = min(weight),\n    Median = median(weight),\n    Max = max(weight),\n    n_Obs = n(),\n  )\n\n# A tibble: 3 × 7\n  group  Mean StdDev   Min Median   Max n_Obs\n  &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 ctrl   5.03  0.583  4.17   5.15  6.11    10\n2 trt1   4.66  0.794  3.59   4.55  6.03    10\n3 trt2   5.53  0.443  4.92   5.44  6.31    10\n\n\n\n\n\n\ntbl %&gt;% \n  mutate(larger5 = case_when(\n    weight &gt; 5 ~ \"yes\",\n    weight &lt; 5 ~ \"no\"\n  )) %&gt;% \n  group_by(group, larger5) %&gt;% \n  summarise(\n    n_Obs = n(),\n    Mean = mean(weight)\n  )\n\n# A tibble: 6 × 4\n# Groups:   group [3]\n  group larger5 n_Obs  Mean\n  &lt;fct&gt; &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 ctrl  no          4  4.45\n2 ctrl  yes         6  5.42\n3 trt1  no          8  4.34\n4 trt1  yes         2  5.95\n5 trt2  no          1  4.92\n6 trt2  yes         9  5.59\n\n\n\n\nJust like with mutate(), we can make use of across() to deal with multiple columns:\n\n\n\ntbl %&gt;%\n  mutate(v1 = 1, v2 = 2, v3 = 3) %&gt;%\n  group_by(group) %&gt;%\n  summarise(across(\n    where(is.numeric), \n    ~ mean(.x)\n    ))\n\n# A tibble: 3 × 5\n  group weight    v1    v2    v3\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ctrl    5.03     1     2     3\n2 trt1    4.66     1     2     3\n3 trt2    5.53     1     2     3\n\n\n\n\n\n\ntbl %&gt;%\n  mutate(v1 = 1, v2 = 2, v3 = 3) %&gt;%\n  group_by(group) %&gt;%\n  summarise(across(\n    c(weight, v3),\n    list(\n    Min = ~ min(.x),\n    Max = ~ max(.x)\n    )\n  ))\n\n# A tibble: 3 × 5\n  group weight_Min weight_Max v3_Min v3_Max\n  &lt;fct&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 ctrl        4.17       6.11      3      3\n2 trt1        3.59       6.03      3      3\n3 trt2        4.92       6.31      3      3\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOnce you used group_by() on a table, it stays grouped unless you use ungroup() on it afterwards. This was not relevant in the examples above, but you must be aware of this if you are using the grouped (summary) results for further steps, since this can lead to unexpected results. You can find an example and further resources on such unintended outcomes here.\n\n\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\n\n5.6 Grouped summaries with summarise() in R for data science (Wickham and Grolemund 2017)\n\nSummarise each group to fewer rows with summarise()\nGroup by one or more variables with group_by()"
  },
  {
    "objectID": "ch/rbasics/tidyverse.html#footnotes",
    "href": "ch/rbasics/tidyverse.html#footnotes",
    "title": "The tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\nBut it was not the first package to use it. This blog post has a nice summary of the history of the pipe operator in R.↩︎\nNote that there are some differences between %&gt;% and |&gt; - find more about it e.g. here, here or here.↩︎\nSee the additional resources below if you want it differently.↩︎\nIt does not make a difference here, whether we put x = chr or x = fct in the ggplot statement.↩︎\nYes, the mean in this example is not really a mean, since there is only one number per group.↩︎\nNote that while I create two vectors in this example, this will work just as well with columns of a table via `table %&gt;% mutate(new = stringrfunction(old))`↩︎"
  },
  {
    "objectID": "ch/summaryarticles/compactletterdisplay.html",
    "href": "ch/summaryarticles/compactletterdisplay.html",
    "title": "Compact Letter Display (CLD)",
    "section": "",
    "text": "Compact letter displays are often used to report results of all pairwise comparisons among treatment means in comparative experiments. See Piepho (2004) and Piepho (2018) for more details and find a coding example below.\n\n\n\n\n\n\n\n\n\n*Means not sharing any letter are significantly different by the Tukey-test at the 5% level of significance.\n\nGroup\nMean weight*\n\n\n\nTrt1\n4.7\\(^{a}\\)\n\n\n\nCtrl\n5.0\\(^{ab}\\)\n\n\n\nTrt2\n5.5\\(^{b}\\)"
  },
  {
    "objectID": "ch/summaryarticles/compactletterdisplay.html#get-the-letters",
    "href": "ch/summaryarticles/compactletterdisplay.html#get-the-letters",
    "title": "Compact Letter Display (CLD)",
    "section": "get the letters",
    "text": "get the letters\nYou will need to install the packages emmeans, multcomp and {multcompView}. The example given here is based on the PlantGrowth data, which is included in R.\n\nlibrary(emmeans)\nlibrary(multcomp)\nlibrary(multcompView)\n\n# set up model\nmodel &lt;- lm(weight ~ group, data = PlantGrowth)\n\n# get (adjusted) weight means per group\nmodel_means &lt;- emmeans(object = model,\n                       specs = \"group\")\n\n# add letters to each mean\nmodel_means_cld &lt;- cld(object = model_means,\n                       adjust = \"Tukey\",\n                       Letters = letters,\n                       alpha = 0.05)\n# show output\nmodel_means_cld\n\n\n\n group emmean    SE df lower.CL upper.CL .group\n trt1    4.66 0.197 27     4.16     5.16  a    \n ctrl    5.03 0.197 27     4.53     5.53  ab   \n trt2    5.53 0.197 27     5.02     6.03   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 3 estimates \nP value adjustment: tukey method for comparing a family of 3 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\n\n\nWe set up a model\n\nThis is a very simple example using lm(). You may use much more complex models and many other model classes.\n\n\n\nemmeans() estimates adjusted means per group.\n\n\nspecs = lets you define for which factor levels you want the means\nNote that when doing this for mixed models, one should use the Kenward-Roger method adjusting the denominator degrees of freedom. One may add the lmer.df = \"kenward-roger\" argument, yet this is the default in {emmeans} (Details here)! Also note that you cannot go wrong with this adjustment - even if there is nothing to adjust.\n\n\n\ncld() adds the letters in a new column named .group.\n\n\nadjust = lets you choose the p-value adjustment method. It allows for different multiplicity adjustments. Go to the “P-value adjustments” heading within the “summary.emmGrid” section in the emmeans documentation for more details on e.g. Fisher’s LSD test, Tukey-test, Bonferroni adjustment etc.\n\nLetters = letters is needed because for some reason the default are numbers and not letters… even though the function is called Compact Letter Display?!\n\nalpha = lets you choose the significance level for the comparisons.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you get the following note and are irritated by it,\n\n## Note: adjust = \"tukey\" was changed to \"sidak\" \n## because \"tukey\" is only appropriate for one set of pairwise comparisons\n## Conf-level adjustment: sidak method for 3 estimates.\n## P value adjustment: tukey method for comparing a family of 3 estimates\n\nhere is an answer explaining why this happens and that it is not a problem. It is not a problem in the sense that the p-values of the pairwise comparisons were indeed adjusted with the Tukey-method, while the Sidak adjustment was applied to the confidence intervals of the means (i.e. columns lower.CL and upper.CL)."
  },
  {
    "objectID": "ch/summaryarticles/compactletterdisplay.html#interpret-the-letters",
    "href": "ch/summaryarticles/compactletterdisplay.html#interpret-the-letters",
    "title": "Compact Letter Display (CLD)",
    "section": "interpret the letters",
    "text": "interpret the letters\nUntil August 2022, the note below the cld() outcome would read:\n\n## NOTE: Compact letter displays can be misleading\n##       because they show NON-findings rather than findings.\n##       Consider using 'pairs()', 'pwpp()', or 'pwpm()' instead.\n\nHowever, in CRAN version 1.8.0 it was changed to:\n\n## NOTE: If two or more means share the same grouping letter,\n##       then we cannot show them to be different.\n##       But we also did not show them to be the same.\n\nBoth notes are very much in line with the delicate matter of how the CLD must be understood. The author and maintainer of the emmeans package, Russell V. Lenth makes the argument that CLDs convey information in a way that may be misleading to the reader. This is because they “display non-findings rather than findings - they group together means based on NOT being able to show they are different” (personal communication). Furthermore, “[the CLD approach] works, but it is very black-and-white: with alpha = .05, P values slightly above or below .05 make a difference, but there’s no difference between a P value of .051 and one of .987, or between .049 and .00001” (posted here). He even wrote here that “Providing for CLDs at all remains one of my biggest regrets in developing this package”. Finally, the former note suggests using alternative plots, which are also created below.\nOn the other hand, it must be clear that the information conveyed by CLDs is not wrong as long as it is interpreted correctly. The documentation of the cld() function refers to Piepho (2004), but even more on point in this context is Piepho (2018):\n\nPiepho, Hans-Peter (2018) Letters in Mean Comparisons: What They Do and Don’t Mean, Agronomy Journal, 110(2), 431-434. DOI: 10.2134/agronj2017.10.0580 (ResearchGate)\nAbstract\n\nLetter displays allow efficient reporting of pairwise treatment comparisons.\nIt is important to correctly convey the meaning of letters in captions to tables and graphs displaying treatment means.\nThe meaning of a letter display can and should be stated in a single sentence without ambiguity.\n\nLetter displays are often used to report results of all pairwise comparisons among treatment means in comparative experiments. In captions to tables and charts using such letter displays, it is crucial to explain properly what the letters mean. In this paper I explain what the letters mean and how this meaning can be succinctly conveyed in a single sentence without ambiguity. This is contrasted to counter-examples commonly found in publications.\n\nThus, the article (= 4 pages long) is certainly worth a read if you are using CLDs."
  },
  {
    "objectID": "ch/summaryarticles/compactletterdisplay.html#get-the-plots",
    "href": "ch/summaryarticles/compactletterdisplay.html#get-the-plots",
    "title": "Compact Letter Display (CLD)",
    "section": "get the plots",
    "text": "get the plots\nHere I provide code for two ways of plotting the results via ggplot2. The first plot is the one I would use, while the second plot is one that is traditionally more common. Finally, I provide examples of other plots that I came across that are suggested as alternatives to CLD plots.\nplot 1: suggested\nI’ve been using and suggesting to use this type of plot for a while now. I know it contains a lot of information and may seem unfamiliar and overwhelming at first glance. However, I argue that if you take the time to understand what you are looking at, this plot is nice as it shows the raw data (black dots), descriptive statistics (black boxes), estimated means (red dots) and a measure of their precision (red error bars) as well as the compact letter display (red letters).\n\nClick to show/hide codelibrary(ggtext)    # automatic line breaks in caption\nlibrary(tidyverse) # ggplot & helper functions\nlibrary(scales)    # more helper functions\n\n# optional: sort factor levels of groups column according to highest mean\n# ...in means table\nmodel_means_cld &lt;- model_means_cld %&gt;% \n  mutate(group = fct_reorder(group, emmean))\n# ...in data table\nPlantGrowth &lt;- PlantGrowth %&gt;% \n  mutate(group = fct_relevel(group, levels(model_means_cld$group)))\n\n# plot\nggplot() +\n  # y-axis\n  scale_y_continuous(\n    name = \"Weight\",\n    limits = c(0, NA),\n    breaks = pretty_breaks(),\n    expand = expansion(mult = c(0,0.1))\n  ) +\n  # x-axis\n  scale_x_discrete(\n    name = \"Treatment\"\n  ) +\n  # general layout\n  theme_classic() +\n  theme(plot.caption = element_textbox_simple()) +\n  # black data points\n  geom_point(\n    data = PlantGrowth,\n    aes(y = weight, x = group),\n    shape = 16,\n    alpha = 0.5,\n    position = position_nudge(x = -0.2)\n  ) +\n  # black boxplot\n  geom_boxplot(\n    data = PlantGrowth,\n    aes(y = weight, x = group),\n    width = 0.05,\n    outlier.shape = NA,\n    position = position_nudge(x = -0.1)\n  ) +\n  # red mean value\n  geom_point(\n    data = model_means_cld,\n    aes(y = emmean, x = group),\n    size = 2,\n    color = \"red\"\n  ) +\n  # red mean errorbar\n  geom_errorbar(\n    data = model_means_cld,\n    aes(ymin = lower.CL, ymax = upper.CL, x = group),\n    width = 0.05,\n    color = \"red\"\n  ) +\n  # red letters\n  geom_text(\n    data = model_means_cld,\n    aes(\n      y = emmean,\n      x = group,\n      label = str_trim(.group)\n    ),\n    position = position_nudge(x = 0.1),\n    hjust = 0,\n    color = \"red\"\n  ) +\n  # caption\n  labs(\n    caption = \"Black dots represent raw data. Red dots and error bars represent (estimated marginal) means ± 95% confidence interval per group. Means not sharing any letter are significantly different by the Tukey-test at the 5% level of significance.\"\n  )\n\n\n\n\nplot 2: well-known\nTraditionally, bar plots with error bars are used a lot in this context. In my experience, there is at least one poster with one of them in every university building I. While they are not wrong per se, there is a decade-long discussion about why such “dynamite plots” are not optimal (see e.g. this nice blogpost).\n\nClick to show/hide codelibrary(ggtext)    # automatic line breaks in caption\nlibrary(tidyverse) # ggplot & helper functions\nlibrary(scales)    # more helper functions\n\n# optional: sort factor levels of groups column according to highest mean\n# ...in means table\nmodel_means_cld &lt;- model_means_cld %&gt;% \n  mutate(group = fct_reorder(group, emmean))\n# ...in data table\nPlantGrowth &lt;- PlantGrowth %&gt;% \n  mutate(group = fct_relevel(group, levels(model_means_cld$group)))\n\n# plot\nggplot() +\n  # y-axis\n  scale_y_continuous(\n    name = \"Weight\",\n    limits = c(0, NA),\n    breaks = pretty_breaks(),\n    expand = expansion(mult = c(0,0.1))\n  ) +\n  # x-axis\n  scale_x_discrete(\n    name = \"Treatment\"\n  ) +\n  # general layout\n  theme_classic() +\n  theme(plot.caption = element_textbox_simple()) +\n  # bars\n  geom_bar(data = model_means_cld,\n           aes(y = emmean, x = group),\n           stat = \"identity\") +\n  # errorbars\n  geom_errorbar(data = model_means_cld,\n                aes(\n                  ymin = emmean - SE,\n                  ymax = emmean + SE,\n                  x = group\n                ),\n                width = 0.1) +\n  # letters\n  geom_text(\n    data = model_means_cld,\n    aes(\n      y = emmean + SE,\n      x = group,\n      label = str_trim(.group)\n    ),\n    hjust = 0.5,\n    vjust = -0.5\n  ) +\n  # caption\n  labs(\n    caption = \"Bars with errorbars represent (estimated marginal) means ± standard error. Means not sharing any letter are significantly different by the Tukey-test at the 5% level of significance.\"\n  )\n\n\n\n\nalternative plots\nNote that I simply collect alternative ways of plotting adjusted mean comparisons here - this does not mean I fully grasp their concept.\nalt 1: Pairwise P-value plot {emmeans}\nThis is the Pairwise P-value plot suggested in the former NOTE we received above as an alternative. The documentation reads: Factor levels (or combinations thereof) are plotted on the vertical scale, and P values are plotted on the horizontal scale. Each P value is plotted twice – at vertical positions corresponding to the levels being compared – and connected by a line segment. Thus, it is easy to visualize which P values are small and large, and which levels are compared.\n\nClick to show/hide codepwpp(model_means) + theme_bw()\n\n\n\n\nalt 2: Lighthouse plot {easystats}\nWithin the framework of the easystats packages, the lighthouse plots came up as a more recent idea. See this issue and this and this part of the documentation for more details.\n\nClick to show/hide codelibrary(modelbased)\nlibrary(see)\nplot(estimate_contrasts(model, adjust = \"tukey\"),\n     estimate_means(model)) +\n  theme_classic()\n\n\n\n\nalt 3: The {ggbetweenstats} plot\nFinally, the ggstatsplot package’s function ggbetweenstats() aims to create graphics with details from statistical tests included in the information-rich plots themselves and would compare our groups like this:\n\nClick to show/hide codelibrary(PMCMRplus)\nlibrary(rstantools)\nlibrary(ggstatsplot)\n# \"since the confidence intervals for the effect sizes are computed using\n# bootstrapping, important to set a seed for reproducibility\"\nset.seed(42)\nggstatsplot::ggbetweenstats(\n  data = PlantGrowth,\n  x = group,\n  y = weight,\n  pairwise.comparisons = TRUE,\n  pairwise.display = \"all\",\n  p.adjust.method = \"none\"\n)"
  },
  {
    "objectID": "ch/summaryarticles/compactletterdisplay.html#see-the-comparisons",
    "href": "ch/summaryarticles/compactletterdisplay.html#see-the-comparisons",
    "title": "Compact Letter Display (CLD)",
    "section": "see the comparisons",
    "text": "see the comparisons\nThe compact letter display allows us to understand the statistical significances forall pairwise comparisons without ever having to look at the individual comparisons a.k.a. contrasts. Yet, if you do want to look at them, here are two options that both start with the model_means object we created at the beginning of this document.\nWe can simply use details = TRUE in the cld() function. This adds a second table called comparisons to the output.\n\nmodel_means %&gt;% \n  cld(adjust = \"none\",\n      Letters = letters,\n      details = TRUE)\n\n$emmeans\n group emmean    SE df lower.CL upper.CL .group\n trt1    4.66 0.197 27     4.26     5.07  a    \n ctrl    5.03 0.197 27     4.63     5.44  ab   \n trt2    5.53 0.197 27     5.12     5.93   b   \n\nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n$comparisons\n contrast    estimate    SE df t.ratio p.value\n ctrl - trt1    0.371 0.279 27   1.331  0.1944\n trt2 - trt1    0.865 0.279 27   3.103  0.0045\n trt2 - ctrl    0.494 0.279 27   1.772  0.0877\n\n\nHowever, we do not actually nee multcomp::cld() function if we only want to investigate the individual pairwise comparisons. Moreover, when using emmeans::pairs() on the means object that we created via emmeans::emmeans(), we can set infer = c(TRUE, TRUE) to obtain both the confidence interval1 and p-values for each difference.\n\nmodel_means %&gt;% \n  pairs(adjust = \"none\",\n        infer = c(TRUE, TRUE))\n\n contrast    estimate    SE df lower.CL upper.CL t.ratio p.value\n ctrl - trt1    0.371 0.279 27   -0.201    0.943   1.331  0.1944\n ctrl - trt2   -0.494 0.279 27   -1.066    0.078  -1.772  0.0877\n trt1 - trt2   -0.865 0.279 27   -1.437   -0.293  -3.103  0.0045\n\nConfidence level used: 0.95"
  },
  {
    "objectID": "ch/summaryarticles/compactletterdisplay.html#get-the-lsdhsd",
    "href": "ch/summaryarticles/compactletterdisplay.html#get-the-lsdhsd",
    "title": "Compact Letter Display (CLD)",
    "section": "get the LSD/HSD",
    "text": "get the LSD/HSD\nThe least significant difference (LSD) represents the smallest value a difference between two means would need to be in order to be statistically significant according to the Fisher’s LSD test (i.e. have a p-value &lt; 0.05). In a simple setting (i.e. balanced data and standard confidence interval), the LSD is simply the half-width of the confidence interval and for our example 0.572:\n\nmodel_means %&gt;% \n  pairs(adjust = \"none\",\n        infer = c(TRUE, TRUE)) %&gt;% \n  as_tibble() %&gt;% \n  mutate(LSD = upper.CL - estimate)\n\n# A tibble: 3 × 9\n  contrast    estimate    SE    df lower.CL upper.CL t.ratio p.value   LSD\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 ctrl - trt1    0.371 0.279    27   -0.201   0.943     1.33 0.194   0.572\n2 ctrl - trt2   -0.494 0.279    27   -1.07    0.0780   -1.77 0.0877  0.572\n3 trt1 - trt2   -0.865 0.279    27   -1.44   -0.293    -3.10 0.00446 0.572\n\n\nThe honestly significant difference (HSD) is basically the Tukey-version of the LSD and for our example 0.691:\n\nmodel_means %&gt;% \n  pairs(adjust = \"Tukey\",\n        infer = c(TRUE, TRUE)) %&gt;% \n  as_tibble() %&gt;% \n  mutate(HSD = upper.CL - estimate)\n\n# A tibble: 3 × 9\n  contrast    estimate    SE    df lower.CL upper.CL t.ratio p.value   HSD\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 ctrl - trt1    0.371 0.279    27   -0.320    1.06     1.33  0.391  0.691\n2 ctrl - trt2   -0.494 0.279    27   -1.19     0.197   -1.77  0.198  0.691\n3 trt1 - trt2   -0.865 0.279    27   -1.56    -0.174   -3.10  0.0120 0.691\n\n\nNote that the LSD or HSD is not necessarily a single value for all comparisons as e.g. shown here. This was actually the core point of (all publications in) my PhD (full text on ResearchGate): Modern experiments do not always lead to balanced/orthogonal data and simple variance structures. However, only if both of these conditions are given, the LSD/HSD will be a single constant value across all comparisons."
  },
  {
    "objectID": "ch/summaryarticles/compactletterdisplay.html#get-the-mean-standard-error-of-differences",
    "href": "ch/summaryarticles/compactletterdisplay.html#get-the-mean-standard-error-of-differences",
    "title": "Compact Letter Display (CLD)",
    "section": "get the mean standard error of differences",
    "text": "get the mean standard error of differences\nIt is sometimes useful to extract the average/mean standard error of differences (s.e.d.)2. Here is an efficient way of obtaining it:\n\nmodel_means %&gt;%\n  pairs(infer = c(FALSE, FALSE)) %&gt;% # skip p-value/CI calculation\n  as_tibble() %&gt;% \n  summarise(meanSED = mean(SE))\n\n# A tibble: 1 × 1\n  meanSED\n    &lt;dbl&gt;\n1   0.279"
  },
  {
    "objectID": "ch/summaryarticles/compactletterdisplay.html#deal-with-significant-interactions",
    "href": "ch/summaryarticles/compactletterdisplay.html#deal-with-significant-interactions",
    "title": "Compact Letter Display (CLD)",
    "section": "deal with significant interactions",
    "text": "deal with significant interactions\nFor now, please check out my answer here on stackoverflow."
  },
  {
    "objectID": "ch/summaryarticles/compactletterdisplay.html#footnotes",
    "href": "ch/summaryarticles/compactletterdisplay.html#footnotes",
    "title": "Compact Letter Display (CLD)",
    "section": "Footnotes",
    "text": "Footnotes\n\nAlternatively pairs() %&gt;% confint() also works as discussed here.↩︎\nThis does not directly relate to the compact letter display, but I’ve kept it in this chapter anyway.↩︎"
  },
  {
    "objectID": "ch/summaryarticles/designingexperiments.html",
    "href": "ch/summaryarticles/designingexperiments.html",
    "title": "Designing Experiments",
    "section": "",
    "text": "This article is very much under construction, but check out\n\nthe old version of this chapter here\nthe packages {FielDHub} and {agricolae}\nthese publications:\n\nCasler (2015)\nH. P. Piepho, Büchse, and Emrich (2003)\nH. P. Piepho, Büchse, and Richter (2004)\nHans-Peter Piepho et al. (2022)\n\n\n\n\n\n\n\n\nReferences\n\nCasler, Michael D. 2015. “Fundamentals of Experimental Design: Guidelines for Designing Successful Experiments.” Agronomy Journal 107 (2): 692–705. https://doi.org/10.2134/agronj2013.0114.\n\n\nPiepho, H. P., A. Büchse, and K. Emrich. 2003. “A Hitchhiker’s Guide to Mixed Models for Randomized Experiments.” Journal of Agronomy and Crop Science 189 (5): 310–22. https://doi.org/10.1046/j.1439-037X.2003.00049.x.\n\n\nPiepho, H. P., A. Büchse, and C. Richter. 2004. “A Mixed Modelling Approach for Randomized Experiments with Repeated Measures.” Journal of Agronomy and Crop Science 190 (4): 230–47. https://doi.org/10.1111/j.1439-037X.2004.00097.x.\n\n\nPiepho, Hans-Peter, Doreen Gabriel, Jens Hartung, Andreas Büchse, Meike Grosse, Sabine Kurz, Friedrich Laidig, et al. 2022. “One, Two, Three: Portable Sample Size in Agricultural Research.” The Journal of Agricultural Science 160 (6): 459–82. https://doi.org/10.1017/s0021859622000466.\n\nCitationBibTeX citation:@online{schmidt2023,\n  author = {Paul Schmidt},\n  title = {Designing {Experiments}},\n  date = {2023-06-17},\n  url = {https://schmidtpaul.github.io/dsfair_quarto//ch/summaryarticles/designingexperiments.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPaul Schmidt. 2023. “Designing Experiments.” June 17, 2023.\nhttps://schmidtpaul.github.io/dsfair_quarto//ch/summaryarticles/designingexperiments.html."
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html",
    "href": "ch/summaryarticles/ggplot2intro.html",
    "title": "How I use ggplot2",
    "section": "",
    "text": "This tutorial serves as an introductory guide to ggplot2, tailored specifically for beginners with no prior exposure to ggplot2. However, it’s worth mentioning that as we delve deeper into the subject, we’ll employ both BaseR and tidyverse code for some data preparations.\nIn addition, this ggplot2 guide reflects my personal approach and application of visualization techniques, focusing on the disciplines that align with the theme of this website - from agricultural sciences to experimental data from biology or life sciences at large. This tutorial, therefore, may not encompass all facets of ggplot2, but rather those elements that I frequently utilize in these specific domains.\n\nHere are some other ggplot2 tutorials and resources that I like:\n\n\nChapter 3: Data Visualisation in (Wickham and Grolemund 2017)\n\nCédric Scherer’s (2022a) A ggplot2 tutorial for beautiful plotting in R\n\nCédric Scherer’s (2022b) Graphic Design with ggplot2\n\nAndrew Heiss’ (2023) Data visualization with R\n\nClaus Wilke’s (2019) Fundamentals of Data Visualization\n\n\nWe are using the p_load() function of the {pacman} package to install and load all necessary packages for this tutorial.\n\npacman::p_load(\n  ggplot2,\n  ggrepel,\n  ggtext\n  )\n\n\nHere are some beautiful ggplots"
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html#who-this-is-for",
    "href": "ch/summaryarticles/ggplot2intro.html#who-this-is-for",
    "title": "How I use ggplot2",
    "section": "",
    "text": "This tutorial serves as an introductory guide to ggplot2, tailored specifically for beginners with no prior exposure to ggplot2. However, it’s worth mentioning that as we delve deeper into the subject, we’ll employ both BaseR and tidyverse code for some data preparations.\nIn addition, this ggplot2 guide reflects my personal approach and application of visualization techniques, focusing on the disciplines that align with the theme of this website - from agricultural sciences to experimental data from biology or life sciences at large. This tutorial, therefore, may not encompass all facets of ggplot2, but rather those elements that I frequently utilize in these specific domains."
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html#other-resources",
    "href": "ch/summaryarticles/ggplot2intro.html#other-resources",
    "title": "How I use ggplot2",
    "section": "",
    "text": "Here are some other ggplot2 tutorials and resources that I like:\n\n\nChapter 3: Data Visualisation in (Wickham and Grolemund 2017)\n\nCédric Scherer’s (2022a) A ggplot2 tutorial for beautiful plotting in R\n\nCédric Scherer’s (2022b) Graphic Design with ggplot2\n\nAndrew Heiss’ (2023) Data visualization with R\n\nClaus Wilke’s (2019) Fundamentals of Data Visualization"
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html#packages-to-install-load",
    "href": "ch/summaryarticles/ggplot2intro.html#packages-to-install-load",
    "title": "How I use ggplot2",
    "section": "",
    "text": "We are using the p_load() function of the {pacman} package to install and load all necessary packages for this tutorial.\n\npacman::p_load(\n  ggplot2,\n  ggrepel,\n  ggtext\n  )"
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html#showcase",
    "href": "ch/summaryarticles/ggplot2intro.html#showcase",
    "title": "How I use ggplot2",
    "section": "",
    "text": "Here are some beautiful ggplots"
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html#name",
    "href": "ch/summaryarticles/ggplot2intro.html#name",
    "title": "How I use ggplot2",
    "section": "Name",
    "text": "Name\nThe name = allows you to change the axis titles.\n\nmyplot +\n  scale_y_continuous(name = \"Weight (g)\") +\n  scale_x_discrete(name = \"Treatment Group\")"
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html#limits",
    "href": "ch/summaryarticles/ggplot2intro.html#limits",
    "title": "How I use ggplot2",
    "section": "Limits",
    "text": "Limits\nThe limits = argument in the scale_*_* functions allows you to specify the range of values displayed on the axis. This can be particularly useful when you want to focus on a specific part of your data. Let’s see how this works in practice with our scatter plot example.\n\n\n\nmyplot +\n  scale_y_continuous(\n    name = \"Weight (g)\",\n    limits = c(0, 7)\n  ) +\n  scale_x_discrete(\n    name = \"Treatment Group\",\n    limits = c(\"ctrl\", \"trt2\")\n  )\n\n\n\n\n\n\n\n\nmyplot +\n  scale_y_continuous(\n    name = \"Weight (g)\",\n    limits = c(0, NA)\n  ) +\n  scale_x_discrete(\n    name = \"Treatment Group\",\n    limits = c(\"trt1\", \"ctrl\", \"trt2\")\n  )\n\n\n\n\n\n\nIn the left plot, we use the limits = argument in scale_y_continuous() to set the y-axis to range from 0 to 7. This works as expected, showing all weights from 0 to 7. However, including only “ctrl” and “trt2” (i.e. the first and last level) in the limits = argument of in scale_x_discrete(), results in only these two groups being displayed on the x-axis. The key point here is that for a discrete scale, the limits = argument needs to include all the levels you want to display.\nIn the right plot, we again use the limits = argument in scale_y_continuous(), but this time we only specify the lower limit (0) and use NA for the upper limit. This tells ggplot2 to start the y-axis at 0 and end it at the maximum value in the data, which is the default behavior. For the x-axis, we provide all three levels (“trt1”, “ctrl”, “trt2”) in the limits = argument of scale_x_discrete(). This not only ensures that all groups are displayed, but also allows us to control the order in which they appear.\nThis demonstrates how the limits = argument can be used differently in scale_*_continuous() and scale_*_discrete(). In a continuous scale, it defines the range of values, while in a discrete scale, it specifies which levels to include and their order.\nIn the end, it’s important to note that setting the limits can exclude data outside the specified range from the plot. This means that the excluded data will not be considered when calculating statistics or generating geoms. In other words, while setting limits can help focus your plot on specific aspects of your data, it can also exclude important information. Always consider the implications of setting limits on your data visualization.\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nIf you are wondering why I wanted the y-axis to start at 0, read this, this and this\n\nList of all scales functions"
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html#breaks",
    "href": "ch/summaryarticles/ggplot2intro.html#breaks",
    "title": "How I use ggplot2",
    "section": "Breaks",
    "text": "Breaks\nThe breaks = argument in these functions allows you to specify the locations of the tick marks on the axis.\n\n\n\nmyplot +\n  scale_y_continuous(\n    name = \"Weight (g)\",\n    limits = c(0, NA),\n    breaks = c(0, 6)\n  ) +\n  scale_x_discrete(\n    name = \"Treatment Group\"\n  )\n\n\n\n\n\n\n\n\nmyplot +\n  scale_y_continuous(\n    name = \"Weight (g)\",\n    limits = c(0, NA),\n    breaks = seq(0, 6)\n  ) +\n  scale_x_discrete(\n    name = \"Treatment Group\"\n  )\n\n\n\n\n\n\nIn the left plot, we use the breaks = argument in scale_y_continuous() to set the y-axis tick marks at 0 and 6. This really results in only two tick marks being displayed on the y-axis. While this is not typically useful for data representation, it serves to illustrate that the breaks = argument can be used to place tick marks at any specified values.\nIn the right plot, we use the seq() function in the breaks = argument to set the y-axis tick marks at every integer value from 0 to 6. This provides a more informative view of the data, as it allows us to see the weight values at regular intervals.\n\n\n\n\n\n\nTip\n\n\n\nInstead of having to manually write “6” in breaks = seq(0, 6) you can instead do this:\n\n\nbreaks = seq(0, max(PlantGrowth$weight)) automatically finds the maximum value in the data\n\nbreaks = scales::breaks_width(1) makes use of the breaks_width() function in the {scales} package to simply define the width of the breaks"
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html#labels",
    "href": "ch/summaryarticles/ggplot2intro.html#labels",
    "title": "How I use ggplot2",
    "section": "Labels",
    "text": "Labels\nThe labels = argument allows you to specify the text that is displayed for each tick mark on the axis. This can be particularly useful when the values in your data are not self-explanatory or when you want to use more descriptive labels.\n\n\n\nmyplot +\n  scale_y_continuous(\n    name = \"Weight (g)\",\n    limits = c(0, NA),\n    breaks = seq(0, 6)\n  ) +\n  scale_x_discrete(\n    name = \"Treatment Group\",\n    labels = c(\"Control\", \"Treatment 1\", \"Treatment 2\")\n  )\n\n\n\n\n\n\n\n\nmyplot +\n  scale_y_continuous(\n    name = \"Weight (g)\",\n    limits = c(0, NA),\n    breaks = seq(0, 6)\n  ) +\n  scale_x_discrete(\n    name = \"Treatment Group\",\n    labels = c(\n      ctrl = \"Control\", \n      trt1 = \"Treatment 1\", \n      trt2 = \"Treatment 2\"\n    )\n  )\n\n\n\n\n\n\nIn the first example, we simply provide a vector of labels. This works fine as long as the levels on the x-axis are in the same order as the labels in the vector. However, if the levels are not in the expected order, the labels will be associated with the wrong levels.\nIn the second example, we provide a named vector of labels. This ensures that the labels are correctly associated with their corresponding levels, regardless of the order of the levels. This is why using a named vector is often the safer option.\n\n\n\n\n\n\nTip\n\n\n\nYou can also use labels = on continuous axes, if you make use of the label_*() functions in the {scales} package. Here are some examples:\n\n\nlabels = label_number() displays numbers on your axis any way you want. E.g. decimal.mark = \".\" displays axis label 3.14 as 3,14 etc.\n\nlabels = label_percent() displays axis labels 0.05, 0.4 as 5%, 40% etc.\n\nlabels = label_log() displays axis labels 10, 100, 1000 as \\(10^1\\), \\(10^2\\), \\(10^3\\) etc."
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html#expand",
    "href": "ch/summaryarticles/ggplot2intro.html#expand",
    "title": "How I use ggplot2",
    "section": "Expand",
    "text": "Expand\nThe expand = argument in the scale_*_* functions allows you to control the expansion of the scale. This is particularly useful when you want to adjust the space between the plotted data and the axes.\nBy default, ggplot2 adds a small amount of space around the data to ensure that the data doesn’t overlap with the axes. However, there might be situations where you want to adjust this space. For instance, you might want to remove the space below the 0 on the y-axis.\n\n\n\nmyplot +\n  scale_y_continuous(\n    name = \"Weight (g)\",\n    limits = c(0, NA),\n    breaks = seq(0, 6),\n    expand = c(0, 0)\n  ) +\n  scale_x_discrete(\n    name = \"Treatment Group\",\n    labels = c(\n      ctrl = \"Control\", \n      trt1 = \"Treatment 1\", \n      trt2 = \"Treatment 2\"\n    )\n  )\n\n\n\n\n\n\n\n\nmyplot +\n  scale_y_continuous(\n    name = \"Weight (g)\",\n    limits = c(0, NA),\n    breaks = seq(0, 6),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_x_discrete(\n    name = \"Treatment Group\",\n    labels = c(\n      ctrl = \"Control\", \n      trt1 = \"Treatment 1\", \n      trt2 = \"Treatment 2\"\n    )\n  )\n\n\n\n\n\n\nIn the left plot, we use the expand = c(0, 0) argument in scale_y_continuous() to simply set the expansion to 0 on both sides of the scale. This removes all extra space around the data. However, this also results in the plot being cut off right at the maximum observation, which might not be desirable.\nIn the right plot, we use the expansion() function in the expand = argument. This function allows us to set different expansion multipliers for the lower and upper limits of the scale. Here, we set the lower multiplier to 0 to remove the space below 0, and the upper multiplier to 0.05 to add a small amount (= 5%) of space above the maximum observation.\n\n\n\n\n\n\nTip\n\n\n\nFor a better understanding of how this expansion-thing works, I found this cheat sheet to be insightful.\n\n\nWe update our myplot according to what we just learned. To get a better overview, we recreate it from scratch:\n\nmyplot &lt;- ggplot(data = PlantGrowth) +\n  aes(y = weight, x = group) +\n  geom_point() +\n  scale_y_continuous(\n    name = \"Weight (g)\",\n    limits = c(0, NA),\n    breaks = seq(0, 6),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_x_discrete(\n    name = \"Treatment Group\",\n    labels = c(\n      ctrl = \"Control\", \n      trt1 = \"Treatment 1\", \n      trt2 = \"Treatment 2\"\n    )\n  )"
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html#general-aesthetic-modifications",
    "href": "ch/summaryarticles/ggplot2intro.html#general-aesthetic-modifications",
    "title": "How I use ggplot2",
    "section": "General Aesthetic Modifications",
    "text": "General Aesthetic Modifications\nTo show you what is possible with geom_point(), let’s create two new plots. In the first plot, we will set the color of the points to orange. In the second plot, we will modify several aspects of the points, including color, shape, size, and transparency.\n\n\n\nmyplot +\n  geom_point(color = \"orange\")\n\n\n\n\n\n\n\n\nmyplot +\n  geom_point(\n    color = \"purple\", \n    shape = 18, \n    size = 3, \n    alpha = 0.5\n  )\n\n\n\n\n\n\nIn the first plot, we use color = “orange” inside the geom_point() function. This sets the color of all points to orange.\nIn the second plot, we modify several aspects of the points:\n\ncolor = “purple” sets the color of the points to purple. shape = 18 changes the shape of the points. ggplot2 includes several different shapes that you can use. The number 18 corresponds to a filled diamond shape.\nsize = 3 increases the size of the points. The size is measured in mm.\nalpha = 0.5 sets the transparency of the points. The alpha value ranges from 0 (completely transparent) to 1 (completely opaque).\n\nThese modifications allow us to customize the appearance of the points to suit our preferences and the needs of our data."
  },
  {
    "objectID": "ch/summaryarticles/ggplot2intro.html#aesthetic-mapping-with-aes",
    "href": "ch/summaryarticles/ggplot2intro.html#aesthetic-mapping-with-aes",
    "title": "How I use ggplot2",
    "section": "Aesthetic Mapping with aes()",
    "text": "Aesthetic Mapping with aes()"
  },
  {
    "objectID": "ch/summaryarticles/modeldiagnostics.html",
    "href": "ch/summaryarticles/modeldiagnostics.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Most statistical methods and all statistical models make certain assumptions (about the data generating process), and (test) results will be meaningless or misleading if theses assumptions do not hold. Therefore, model diagnostics should be used to check how well the assumptions of any given model are met."
  },
  {
    "objectID": "ch/summaryarticles/modeldiagnostics.html#independence",
    "href": "ch/summaryarticles/modeldiagnostics.html#independence",
    "title": "Model Diagnostics",
    "section": "Independence",
    "text": "Independence\nAssumption: Individual observations are independent of each other (as opposed to dependent/correlated).\nModel diagnostics are actually not used to verify this specific assumption. Instead, this assumption is justified if proper randomization was applied as part of the experimental design. Keep in mind, that there are indeed non-classical scenarios like repeated measures over time or certain experimental designs where observations are not (assumed to be) independent, but these are not covered in this chapter."
  },
  {
    "objectID": "ch/summaryarticles/modeldiagnostics.html#normality",
    "href": "ch/summaryarticles/modeldiagnostics.html#normality",
    "title": "Model Diagnostics",
    "section": "Normality",
    "text": "Normality\nAssumption: The errors follow a normal distribution.\nA model’s errors cannot be directly observed, but are estimated by their residuals; these residuals can be used for checking normality.\n\n\n\n\n\n\nYes residuals - not data!\n\n\n\nUnfortunately, it is more common than it should be that people check whether their raw data (i.e. their response variable, e.g. yield) is normally distributed. This is not constructive. Instead, the model’s residuals should be checked for normality. Please see section “4 | ANSWERING QUESTION 1” in Kozak and Piepho (2018) for details.\n\n\nQQ plot\nApproximate normality can be assumed, if the dots in a QQ plot look close to a straight line. Kozak and Piepho (2018) point out that when it comes to checking normality, we need to be clear that we will never be able to check whether indeed the distribution is fully normal; instead, we should check whether it is approximately normal.\n\nmod %&gt;% \n  check_normality() %&gt;% \n  plot(type = \"qq\")\n\n\n\n\nTests\nAs explained further below, I do not embrace using statistical tests to check assumptions, but instead suggest using the diagnostic plot above. However, for completeness I still provide the following information:\nThere are multiple statistical tests for detecting a violation of the normality assumption. A p-value &lt; 0.05 would suggest such a violation:\n\nols_test_normality(mod)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.9661         0.4379 \nKolmogorov-Smirnov        0.1101         0.8215 \nCramer-von Mises          3.6109         0.0000 \nAnderson-Darling          0.3582         0.4299 \n-----------------------------------------------"
  },
  {
    "objectID": "ch/summaryarticles/modeldiagnostics.html#variance-homogeneity",
    "href": "ch/summaryarticles/modeldiagnostics.html#variance-homogeneity",
    "title": "Model Diagnostics",
    "section": "Variance homogeneity",
    "text": "Variance homogeneity\nAssumption: The error variance is the same at any set of predictor values.\nAlso referred to as homoscedasticity (i.e. the opposite of heteroscedasticity)\nRes-Pred Plot\nWhile this plot does not seem to have an established name, it always has raw/standardized/studentized residuals on the y axis and fitted/predicted values on the x axis. Variance homogeneity can be assumed if the residuals form an approximate horizontal band around the 0 line indicating homogeneity of error variance.\n\nmod %&gt;% \n  check_heteroscedasticity() %&gt;% \n  plot()\n\n\n\n\nTests\nAs explained further below, I do not embrace using statistical tests to check assumptions, but instead suggest using the diagnostic plot above. However, for completeness I still provide the following information:\nThere are multiple statistical tests for detecting a violation of the variance homogeneity assumption. A p-value &lt; 0.05 would suggest such a violation:\n\n\n\nols_test_breusch_pagan(mod)\n\n\n Breusch Pagan Test for Heteroskedasticity\n -----------------------------------------\n Ho: the variance is constant            \n Ha: the variance is not constant        \n\n               Data                \n ----------------------------------\n Response : weight \n Variables: fitted values of weight \n\n        Test Summary          \n -----------------------------\n DF            =    1 \n Chi2          =    3.000303 \n Prob &gt; Chi2   =    0.08324896 \n\n\n\n\n\n\nols_test_bartlett(\n  data = PlantGrowth, \n  \"weight\", \n  group_var = \"group\")\n\n\n    Bartlett's Test of Homogenity of Variances    \n------------------------------------------------\nHo: Variances are equal across groups\nHa: Variances are unequal for atleast two groups\n\n        Test Summary         \n ----------------------------\n DF            =    2 \n Chi2          =    2.878592 \n Prob &gt; Chi2   =    0.2370946"
  },
  {
    "objectID": "ch/summaryarticles/modeldiagnostics.html#linearity",
    "href": "ch/summaryarticles/modeldiagnostics.html#linearity",
    "title": "Model Diagnostics",
    "section": "Linearity",
    "text": "Linearity\nAssumption: The response can be written as a linear combination of the predictors.\nThis assumption can also be checked via the Res-Pred-plot from the Variance homogeneity section above. It can assumed to be met if the residuals spread randomly around the 0 line. In other words: At any fitted value, the mean of the residuals should be roughly 0. If this is the case, the linearity assumption is valid. I am not aware of any statistical tests for linearity."
  },
  {
    "objectID": "ch/summaryarticles/modeldiagnostics.html#data-transformation",
    "href": "ch/summaryarticles/modeldiagnostics.html#data-transformation",
    "title": "Model Diagnostics",
    "section": "Data transformation",
    "text": "Data transformation\nTransforming the response variable (e.g. yield) with a mathematical function (e.g. square root) can actually solve your problem. If the diagnostics of a linear model improve noticeably by replacing yield with sqrt(yield), you can simply use the latter to conduct an ANOVA and draw conclusions from it. Moreover, you can also compare means via post hoc tests. What follows is an example1 where this approach does as intended.\n\nClick to show/hide codepacman::p_load(agridat,\n               easystats,\n               emmeans,\n               multcomp,\n               multcompView,\n               tidyverse)\n\ndat &lt;- agridat::bridges.cucumber %&gt;%\n  filter(loc == \"Clemson\") %&gt;%\n  mutate(colF = as.factor(col),\n         rowF = as.factor(row))\n\n\n\n\n\nmod1 &lt;- lm(\n  yield ~ gen + rowF + colF, \n  data = dat)\n\nmod1 %&gt;% \n  check_normality() %&gt;% \n  plot(type = \"qq\")\n\n\n\n\n\n\n\n\nmod2 &lt;- lm(\n  sqrt(yield) ~ gen + rowF + colF, \n  data = dat)\n\nmod2 %&gt;% \n  check_normality() %&gt;% \n  plot(type = \"qq\")\n\n\n\n\n\n\nYou can see that the QQ plot for mod2 does indeed look better than that for mod1. Thus, we can and should conduct the ANOVA for mod2. Its interpretation or rather the conclusions drawn from it are essentially the same - irrespective of whether yield or the square root of yield was modeled: There are significant differences between the genotypes.\n\nanova(mod2)\n\nAnalysis of Variance Table\n\nResponse: sqrt(yield)\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ngen        3 10.5123  3.5041  8.8966 0.01256 *\nrowF       3  5.0283  1.6761  4.2555 0.06228 .\ncolF       3  4.2121  1.4040  3.5647 0.08670 .\nResiduals  6  2.3632  0.3939                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt is a little bit different when it comes to mean comparisons via post hoc tests. Not because of the conclusion part - this is the same here, too: e.g. two genotype means may be significantly different from each other according to the Tukey test. However, the mean itself can be irritating, since noone is used to dealing with square roots of yields. However, it is valid to present the means on the backtransformed (= original) scale, as long as it is made clear to the reader that the model fit and mean comparisons were made on a different (= square root) scale.\n\nmod2 %&gt;% \n  emmeans(specs = ~ gen, type = \"response\") %&gt;% \n  cld(adjust = \"Tukey\", Letters = letters)\n\n gen      response   SE df lower.CL upper.CL .group\n Poinsett     20.9 2.87  6     12.0     32.1  a    \n Sprint       25.1 3.14  6     15.3     37.3  a    \n Guardian     30.4 3.46  6     19.5     43.7  ab   \n Dasher       45.3 4.23  6     31.7     61.4   b   \n\nResults are averaged over the levels of: rowF, colF \nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 4 estimates \nIntervals are back-transformed from the sqrt scale \nNote: contrasts are still on the sqrt scale \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nNote that type = \"response\" can do the back-transformation for us automatically2 and specifically see the part of the message below the mean table that reads “Intervals are back-transformed from the sqrt scale. Note: contrasts are still on the sqrt scale”."
  },
  {
    "objectID": "ch/summaryarticles/modeldiagnostics.html#generalized-linear-models",
    "href": "ch/summaryarticles/modeldiagnostics.html#generalized-linear-models",
    "title": "Model Diagnostics",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nTO DO\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\nGeneral\n\nWhat’s normal anyway? Residual plots are more telling than significance tests when checking ANOVA assumptions (Kozak and Piepho 2018)\n\n\nChapter 13 Model Diagnostics in Applied Statistics with R (Dalpiaz, 2022)\n\nChapter 8 Model Diagnostics in Course Handouts for Bayesian Data Analysis (Lai, 2019)\n🇩🇪 Verbleibende Plots interpretieren, um Ihre Regression zu verbessern\n\n{olsrr}\n\nNormality\n\nR Tutorial: Normal Probability Plot of Residuals\nFor this specific purpose, QQ plots may also be called Normal probability plots\n\n{qqplotr}\n\nHomoscedasticity\n\nDocumentation on tests from {olsrr}\nWikipedia article on Homoscedasticity and heteroscedasticity\n\n\nTransformation\n\n🇩🇪 Chapter 3.3 in Dormann and Kühn (2011)"
  },
  {
    "objectID": "ch/summaryarticles/modeldiagnostics.html#footnotes",
    "href": "ch/summaryarticles/modeldiagnostics.html#footnotes",
    "title": "Model Diagnostics",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis is the same data as in the latin square chapter.↩︎\nThis works only if you did the transformation as part of the model as we did here in lm(). It cannot work if you did the transformation by creating a column in the dataset before fitting the model.↩︎"
  },
  {
    "objectID": "ch/summaryarticles/whyseequal.html",
    "href": "ch/summaryarticles/whyseequal.html",
    "title": "Why are the StdErr all the same?",
    "section": "",
    "text": "I am often asked something along the lines of:"
  },
  {
    "objectID": "ch/summaryarticles/whyseequal.html#footnotes",
    "href": "ch/summaryarticles/whyseequal.html#footnotes",
    "title": "Why are the StdErr all the same?",
    "section": "Footnotes",
    "text": "Footnotes\n\na.k.a. adjusted means, estimated marginal means (emmeans), least-squares means (lsmeans), modelbased means↩︎\n\\(SE = SD/\\sqrt(n)\\) see e.g. Wikipedia↩︎\na.k.a. adjusted means, estimated marginal means (emmeans), least-squares means (lsmeans), modelbased means↩︎\na.k.a. Homoscedasticity, Homogeneity of Variance, Assumption of Equal Variance↩︎\na balanced design has an equal number of observations for all possible level combinations; read more e.g. here↩︎"
  },
  {
    "objectID": "footer.html",
    "href": "footer.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\nThis is my footer.\n\n\n\n\n\nCitationBibTeX citation:@online{schmidt2023,\n  author = {Paul Schmidt},\n  date = {2023-10-01},\n  url = {https://schmidtpaul.github.io/dsfair_quarto//footer.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPaul Schmidt. 2023. October 1, 2023. https://schmidtpaul.github.io/dsfair_quarto//footer.html."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Agriculture in R",
    "section": "",
    "text": "DSFAIR - Data Science for Agriculture in R starts out as a step-by-step introduction to R while its later chapters are more of a cookbook with statistical analyses of typical examples in life sciences with focus on experimental agriculture, biology, ecology and other related fields."
  },
  {
    "objectID": "index.html#workshops",
    "href": "index.html#workshops",
    "title": "Data Science for Agriculture in R",
    "section": "Workshops",
    "text": "Workshops\nMoreover, the chapters published here serve as the basis for any of my R‑Workshops. Check out the “Prepare for an upcoming workshop” chapter and the list of previous workshops."
  }
]